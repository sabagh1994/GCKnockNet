{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import load_data\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "#import params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameter values\n",
    "#CMD=\"python3.6 load_data.py --data_dir $DATADIR --out_label_count 50 --data_serialized \\\n",
    "#    --training --data_mode all --data_batch_size 6 --train_max_steps 1 &> $LOGDIR/$TESTCT.log\"\n",
    "\n",
    "# parameter setup\n",
    "param_dict= {}\n",
    "param_dict[\"data_batch_size\"]= 256\n",
    "param_dict[\"data_dir\"]= './data/train/'\n",
    "param_dict['data_mode'] = 'all'\n",
    "param_dict['data_batch_norm']= True\n",
    "param_dict[\"data_serialized\"]= True\n",
    "param_dict[\"data_sparse\"]= False\n",
    "param_dict[\"adj_sparse\"]= False #later on change it to True\n",
    "param_dict[\"train_max_steps\"]= 1000\n",
    "param_dict[\"out_label_count\"]= 50\n",
    "param_dict[\"is_training\"]= True #stop shuffling the data for code check!:) for training you should shuffle the data\n",
    "# but for the validation set the shuffle off to go through each example once\n",
    "\n",
    "# gene-gene network parameters\n",
    "param_dict[\"network_file_name\"]= '9606.STRING_experimental.edge'\n",
    "param_dict[\"net_type\"]= 'experimental'\n",
    "param_dict[\"network_dir\"]= './data/network-probe/'\n",
    "\n",
    "# Graph Convolutional Layer params\n",
    "#param_dict['GCL_dropout']= 1.\n",
    "param_dict['num_features_nonzero'] = 0 #(tf.int32) in placeholder\n",
    "param_dict[\"l2_weight_decay\"]= 0 #5e-4 # 'Weight for L2 loss on embedding matrix.'\n",
    "param_dict['GCL_batch_norm']= False\n",
    "param_dict['CL_batch_norm']= False\n",
    "param_dict['FC_batch_norm']= False\n",
    "\n",
    "param_dict['Arch'] = [['GC', 15]\n",
    "                      ['FC', 950],\n",
    "                      ['FC', 950],\n",
    "                      ['FC', 950],\n",
    "                      ['FC', 50]]\n",
    "        \n",
    "param_dict[\"learning_rate\"]= 0.00001 #initial learning rate\n",
    "param_dict[\"dropout\"]= 0.2 # dropout rate for tarin data \n",
    "\n",
    "param_dict['probe_length']= 841\n",
    "\n",
    "param_dict['GraphSage_final_norm']= False\n",
    "param_dict['residual_mode']= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics.py module\n",
    "def masked_softmax_cross_entropy(preds, labels): #, mask):\n",
    "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    #mask = tf.cast(mask, dtype=tf.float32)\n",
    "    #mask /= tf.reduce_mean(mask)\n",
    "    #loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def masked_accuracy(preds, labels): #, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    \n",
    "    return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "# inits.py module\n",
    "def uniform(shape, scale=0.05, name=None):\n",
    "    \"\"\"Uniform init.\"\"\"\n",
    "    initial = tf.random_uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32)\n",
    "    #return tf.get_variable(name, initializer= initial, trainable= True)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = tf.sqrt(6/(shape[0]+shape[1]))\n",
    "    init_range= tf.cast(init_range, dtype= tf.float32)\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    #return tf.get_variable(name, initializer= initial, trainable= True)\n",
    "    return tf.Variable(initial, name=name, validate_shape=False)\n",
    "\n",
    "\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    #return tf.get_variable(name, initializer= initial, trainable= True)\n",
    "    return tf.Variable(initial, name=name, validate_shape=False)\n",
    "\n",
    "\n",
    "def ones(shape, name=None):\n",
    "    \"\"\"All ones.\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float32)\n",
    "    #return tf.get_variable(name, initializer= initial, trainable= True)\n",
    "    return tf.Variable(initial, name=name, validate_shape=False)\n",
    "\n",
    "#preprocessing functions (later put them in preprocess.py module)\n",
    "def dense_tensor_to_sparse(dense_tensor):\n",
    "    a_t = dense_tensor\n",
    "    # Find indices where the tensor is not zero\n",
    "    idx = tf.where(tf.not_equal(a_t, 0))\n",
    "    # Make the sparse tensor\n",
    "    # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape()\n",
    "    # if tensor shape is dynamic\n",
    "    sparse_tensor = tf.SparseTensor(idx, tf.gather_nd(a_t, idx), tf.shape(a_t, out_type=tf.int64))\n",
    "    # Make a dense tensor back from the sparse one, only to check result is correct\n",
    "    #dense = tf.sparse_tensor_to_dense(sparse_tensor)\n",
    "    return sparse_tensor\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj= adj+tf.eye(tf.shape(adj)[0])\n",
    "    rowsum= tf.reduce_sum(adj, 1)\n",
    "    d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "    d_inv_sqrt= tf.reshape(d_inv_sqrt, [1, tf.size(d_inv_sqrt)])\n",
    "    zero_out= tf.cast(~tf.is_inf(d_inv_sqrt), tf.float32)\n",
    "    d_inv_sqrt= d_inv_sqrt*zero_out\n",
    "    d_mat_inv_sqrt = tf.diag(tf.reshape(d_inv_sqrt, [tf.size(d_inv_sqrt)]))\n",
    "    term1= tf.transpose(tf.matmul(adj, d_mat_inv_sqrt))\n",
    "    return tf.matmul(term1, d_mat_inv_sqrt)\n",
    "    \n",
    "def preprocess_adj(adj,issparse):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    if issparse:\n",
    "        adj_normalized= dense_tensor_to_sparse(adj_normalized)\n",
    "    return adj_normalized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### graph sage adjacency normalization \n",
    "def normalize_adj_sage(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    rowsum= tf.reduce_sum(adj, 1)\n",
    "    d_inv= tf.pow(rowsum, -1)\n",
    "    d_inv= tf.reshape(d_inv, [1, tf.size(d_inv)])\n",
    "    zero_out= tf.cast(~tf.is_inf(d_inv), tf.float32)\n",
    "    d_inv= d_inv*zero_out\n",
    "    d_mat_inv = tf.diag(tf.reshape(d_inv, [tf.size(d_inv)]))\n",
    "    return tf.matmul(d_mat_inv, adj)\n",
    "\n",
    "def preprocess_adj_sage(adj,issparse):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj_sage(adj)\n",
    "    if issparse:\n",
    "        adj_normalized= dense_tensor_to_sparse(adj_normalized)\n",
    "    return adj_normalized\n",
    "###### end of graph sage adj normalization \n",
    "\n",
    "def build_adj(network_dict, issparse):\n",
    "    graph= nx.from_dict_of_lists(network_dict)\n",
    "    adj = nx.adjacency_matrix(graph,nodelist= probe_gene_order)\n",
    "    # convert from scipy sparse to numpy\n",
    "    adj= adj.toarray()\n",
    "    adj= tf.convert_to_tensor(adj, dtype= tf.float32)\n",
    "    adj_new= preprocess_adj_sage(adj,issparse)\n",
    "    return adj_new, adj\n",
    "\n",
    "\n",
    "def preprocess_features(features, probe_gene_order, main_gene_order, issparse):\n",
    "    mask=[]\n",
    "    for i, ind in enumerate(main_gene_order):\n",
    "        if ind in probe_gene_order:\n",
    "            mask.append(1)\n",
    "        else:\n",
    "            mask.append(0)\n",
    "    mask= np.array(mask).reshape(1,len(mask))\n",
    "    mask= np.concatenate((mask,mask), axis=1)\n",
    "    \n",
    "    mask= tf.convert_to_tensor(mask, dtype= tf.float32)\n",
    "    features= features*mask \n",
    "    \n",
    "    x= features\n",
    "    intermediate_tensor = tf.reduce_sum(tf.abs(x), 0)\n",
    "    zero_vector = tf.zeros(shape=(1,1), dtype=tf.float32)\n",
    "    bool_mask = tf.not_equal(intermediate_tensor, zero_vector)\n",
    "    #bool_mask should have only one dimension\n",
    "    bool_mask= tf.reshape(bool_mask, [tf.size(bool_mask)])\n",
    "    features = tf.boolean_mask(x, bool_mask, axis=1)\n",
    "    # normalize the features if needed later\n",
    "    \n",
    "    # convert the N*(ctrl+pert= 2*G features) tensor to a 3D tensor (N,G,2)\n",
    "    dim= tf.cast(tf.shape(features)[1]/2, tf.int32)\n",
    "    features= tf.reshape(features, [tf.shape(features)[0],2,dim])\n",
    "    features= tf.transpose(features, perm=[0, 2, 1]) # a 3-D tensor. each 2D tensor is a sample \n",
    "                         \n",
    "    # convert dense tensor to sparse tensor\n",
    "    if issparse:\n",
    "        features= dense_tensor_to_sparse(features)\n",
    "    \n",
    "    # needed for tf.assign()\n",
    "    features= tf.reshape(features, [param_dict[\"data_batch_size\"], param_dict[\"probe_length\"], 2])\n",
    "    return features\n",
    "\n",
    "\n",
    "#neural net classes\n",
    "# layer.py module\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)\n",
    "\n",
    "def dot_2D(x, y, sparse=False):\n",
    "    if sparse:\n",
    "        res= tf.sparse_tensor_dense_matmul(x,y)\n",
    "    else:\n",
    "        res= tf.matmul(x,y)\n",
    "    return res # res is a 2D tensor\n",
    "\n",
    "def dot(x, y, sparse=False, is_adj= False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\" \n",
    "    if sparse:\n",
    "        if is_adj:\n",
    "            # x is the adjacency(sparse) and y is the data (dense)\n",
    "            orig_size= tf.shape(y)\n",
    "            y= tf.transpose(y, [1,0,2])\n",
    "            y= tf.reshape(y, [tf.shape(y)[0], -1])\n",
    "            res= tf.sparse_tensor_dense_matmul(x,y) # res is a dense matrix\n",
    "            res= tf.reshape(res, [tf.shape(res)[0],-1,orig_size[2]])\n",
    "            res= tf.transpose(res,[1,0,2])\n",
    "        else:\n",
    "            # y is the w and x is the data which is sparse\n",
    "            orig_size= tf.shape(x)\n",
    "            x= tf.sparse_reshape(x, [-1,tf.shape(x)[2]])\n",
    "            res= tf.sparse_tensor_dense_matmul(x,y)\n",
    "            res= tf.reshape(res, [-1,orig_size[1],tf.shape(res)[1]])\n",
    "    else:\n",
    "        if is_adj:\n",
    "            # y is the data and x is the adjacency\n",
    "            #x= tf.tile(tf.expand_dims(x,0),[tf.shape(y)[0],1,1])\n",
    "            #res = tf.matmul(x, y)\n",
    "            orig_size= tf.shape(y)\n",
    "            y= tf.transpose(y, [1,0,2])\n",
    "            y= tf.reshape(y, [tf.shape(y)[0], -1])\n",
    "            res= tf.matmul(x,y)\n",
    "            res= tf.reshape(res, [tf.shape(res)[0],-1,orig_size[2]])\n",
    "            res= tf.transpose(res,[1,0,2])\n",
    "        else:\n",
    "            # y is the w and x is the data\n",
    "            #y= tf.tile(tf.expand_dims(y,0),[tf.shape(x)[0],1,1])\n",
    "            #res = tf.matmul(x, y)\n",
    "            orig_size= tf.shape(x)\n",
    "            x= tf.reshape(x, [-1,tf.shape(x)[2]])\n",
    "            res= tf.matmul(x,y)\n",
    "            res= tf.reshape(res, [-1,orig_size[1],tf.shape(res)[1]])\n",
    "    return res # res is 3D and dense\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs): #implements function called operator\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "\n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, params, placeholders_val, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        # modify this part later\n",
    "        if dropout:\n",
    "            self.dropout = placeholders_val['dropout']\n",
    "        else:\n",
    "            self.dropout = 0. # there is no need to initialize it as a tensor scalar\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless #check later!\n",
    "        self.bias = bias\n",
    "        self.params= params\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = params['num_features_nonzero'] # num_features_nonzero is a tensor of unknown shape\n",
    "        # fed through the placeholder\n",
    "        \n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout) #By default, each element is kept or dropped independently(without noise shape)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)], # you have to implemet dot for 3D x\n",
    "                              sparse=self.sparse_inputs, is_adj= False)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]\n",
    "            support = dot(self.support[i], pre_sup, sparse=False, is_adj= True)\n",
    "            supports.append(support)\n",
    "        #output = tf.add_n(supports)\n",
    "        output= supports[0]\n",
    "        \n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']  # assuming the output is of size batchNo*row*col and bias is of size (col)\n",
    "            # the bias will be summed up with each row in the output\n",
    "\n",
    "        output= self.act(output)\n",
    "        \n",
    "\n",
    "        if self.params['GCL_batch_norm']:\n",
    "            # you need to reshape the output if you want to use this!\n",
    "            with tf.variable_scope(self.name + '_vars'):\n",
    "                output= tf.layers.batch_normalization(inputs= output, axis= 0,training= placeholders_val['is_training'], \n",
    "                                                      trainable= True, name= 'bn', reuse=tf.AUTO_REUSE)\n",
    "        \n",
    "        return output \n",
    "    \n",
    "class GraphSage(Layer):    #not implemented \n",
    "    \"\"\"Graph convolution layer using GraphSage algorithm.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, params, placeholders_val, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphSage, self).__init__(**kwargs)\n",
    "\n",
    "        # modify this part later\n",
    "        if dropout:\n",
    "            self.dropout = placeholders_val['dropout']\n",
    "        else:\n",
    "            self.dropout = 0. # there is no need to initialize it as a tensor scalar\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless #check later!\n",
    "        self.bias = bias\n",
    "        self.params= params\n",
    "        self.output_dim= output_dim\n",
    "    \n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = params['num_features_nonzero'] # num_features_nonzero is a tensor of unknown shape\n",
    "        # fed through the placeholder\n",
    "        \n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([2*input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout) #By default, each element is kept or dropped independently(without noise shape)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        neighbour_mean = dot(self.support[0], x, sparse=False, is_adj= True)\n",
    "        # concatenate neighbour mean with node feature itself\n",
    "        x= tf.concat([neighbour_mean, x], axis= 2)\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)], # you have to implemet dot for 3D x\n",
    "                              sparse=self.sparse_inputs, is_adj= False)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]\n",
    "        output= pre_sup\n",
    "        \n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']  # assuming the output is of size batchNo*row*col and bias is of size (col)\n",
    "            # the bias will be summed up with each row in the output\n",
    "\n",
    "        output= self.act(output)\n",
    "\n",
    "        if self.params['GCL_batch_norm']:\n",
    "            output= tf.reshape(output, [self.params['data_batch_size'], \n",
    "                                        self.params['probe_length'],\n",
    "                                        self.output_dim])\n",
    "            with tf.variable_scope(self.name + '_vars'):\n",
    "                output= tf.layers.batch_normalization(inputs= output, axis= 0,training= placeholders_val['is_training'], \n",
    "                                                      trainable= True, name= 'bn', reuse=tf.AUTO_REUSE)\n",
    "        \n",
    "        return output \n",
    "    \n",
    "class Conv_1D(Layer):\n",
    "    \"\"\"1D convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, params, placeholders_val, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(Conv_1D, self).__init__(**kwargs)\n",
    "\n",
    "        # modify this part later\n",
    "        if dropout:\n",
    "            self.dropout = placeholders_val['dropout']\n",
    "        else:\n",
    "            self.dropout = 0. # there is no need to initialize it as a tensor scalar\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless #check later!\n",
    "        self.bias = bias\n",
    "        self.params= params\n",
    "        self.output_dim= output_dim\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = params['num_features_nonzero'] # num_features_nonzero is a tensor of unknown shape\n",
    "        # fed through the placeholder\n",
    "        \n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout) #By default, each element is kept or dropped independently(without noise shape)\n",
    "\n",
    "        # convolve\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)], # you have to implemet dot for 3D x\n",
    "                              sparse=self.sparse_inputs, is_adj= False)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]\n",
    "        output= pre_sup\n",
    "        \n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']  # assuming the output is of size batchNo*row*col and bias is of size (col)\n",
    "            # the bias will be summed up with each row in the output\n",
    "\n",
    "        output= self.act(output)\n",
    "        \n",
    "        if self.params['CL_batch_norm']:\n",
    "            output= tf.reshape(output, [self.params['data_batch_size'], \n",
    "                            self.params['probe_length'],\n",
    "                            self.output_dim])\n",
    "            with tf.variable_scope(self.name + '_vars'):\n",
    "                output= tf.layers.batch_normalization(inputs= output, axis= 0,training= placeholders_val['is_training'], \n",
    "                                                      trainable= True, name= 'bn', reuse=tf.AUTO_REUSE)\n",
    "        return output \n",
    "\n",
    "    \n",
    "    \n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, params,  placeholders_val, dropout=0., sparse_inputs=False,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, is_2D= False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders_val['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.params= params\n",
    "        self.is_2D= is_2D\n",
    "        self.output_dim= output_dim\n",
    "        \n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = params['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        if not self.is_2D:\n",
    "            x= tf.reshape(x, [-1,tf.shape(x)[1]*tf.shape(x)[2]]) # later you need to add a flag to check for the input shape\n",
    "        output = dot_2D(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "        \n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "        \n",
    "        output= self.act(output)\n",
    "        \n",
    "        if self.params['FC_batch_norm']:\n",
    "            output= tf.reshape(output, [self.params['data_batch_size'],\n",
    "                                        self.output_dim])\n",
    "            with tf.variable_scope(self.name + '_vars'):\n",
    "                output= tf.layers.batch_normalization(inputs= output, axis= 0,training= placeholders_val['is_training'], \n",
    "                                                      trainable= True, name= 'bn', reuse=tf.AUTO_REUSE)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# model.py module\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "        self.params= {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        self.activations_eval = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.inputs_eval = None\n",
    "        self.outputs = None\n",
    "        self.outputs_eval = None\n",
    "        \n",
    "        self.inter_out= None #to check whether the dropout works for training and validation \n",
    "        self.inter_out_eval= None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.loss_eval = 0\n",
    "        \n",
    "        self.accuracy = 0\n",
    "        self.accuracy_eval = 0\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.inputs = self.placeholders['features']\n",
    "        self.inputs_norm= self.inputs \n",
    "        \n",
    "        if self.params['data_batch_norm']:\n",
    "            self.inputs_norm= batch_normalize(self.inputs)\n",
    "            \n",
    "        self.activations.append(self.inputs_norm)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if self.params['residual_mode']:\n",
    "                if i==2:\n",
    "                    residual= tf.concat([self.activations[-2], self.activations[-1]], axis=2)\n",
    "                    hidden = layer(residual)\n",
    "                else:\n",
    "                    hidden = layer(self.activations[-1])\n",
    "            elif self.params['GraphSage_final_norm']:\n",
    "                if i==2:\n",
    "                    norm = tf.nn.l2_normalize(self.activations[-1], 2)\n",
    "                    hidden= layer(norm)\n",
    "                else:\n",
    "                    hidden = layer(self.activations[-1])\n",
    "            else:\n",
    "                hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "    \n",
    "    def parameter_count(self):\n",
    "        return len(self.vars.keys())\n",
    "        \n",
    "    def build_eval(self):\n",
    "        # Build sequential layer model\n",
    "        self.inputs_eval = self.placeholders['features_eval']\n",
    "        self.inputs_norm_eval= self.inputs_eval \n",
    "        \n",
    "        if self.params['data_batch_norm']:\n",
    "            self.inputs_norm_eval= batch_normalize(self.inputs_eval)\n",
    "           \n",
    "        self.activations_eval.append(self.inputs_norm_eval)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if self.params['residual_mode']:\n",
    "                if i==2:\n",
    "                    residual= tf.concat([self.activations_eval[-2], self.activations_eval[-1]], axis=2)\n",
    "                    hidden = layer(residual)\n",
    "                else:\n",
    "                    hidden = layer(self.activations_eval[-1])\n",
    "            elif self.params['GraphSage_final_norm']:\n",
    "                if i==2:\n",
    "                    norm = tf.nn.l2_normalize(self.activations_eval[-1], 2)\n",
    "                    hidden= layer(norm)\n",
    "                else:\n",
    "                    hidden = layer(self.activations_eval[-1])\n",
    "            else:\n",
    "                hidden = layer(self.activations_eval[-1])\n",
    "            self.activations_eval.append(hidden)\n",
    "        self.outputs_eval = self.activations_eval[-1]\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss_eval()\n",
    "        self._accuracy_eval()\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    \n",
    "    def _loss_eval(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy_eval(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def update_field(features_alter, labels_alter):\n",
    "        pass\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"./model-save/tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None, save_path= '/model-save/tmp/'):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        #save_path = \"./visual-save-load/tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(save_path))\n",
    "        print(\"Model restored from file: %s\" % save_path)\n",
    "           \n",
    "\n",
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, params, input_dim, placeholders_val, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        #self.input_dim = self.inputs.get_shape().as_list()[2]  # To be supported in future Tensorflow versions\n",
    "        #self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.output_dim= tf.shape(placeholders['labels'])[1]\n",
    "        self.placeholders = placeholders\n",
    "        self.params= params\n",
    "        self.output_average= 0.\n",
    "        self.placeholders_val= placeholders_val\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\n",
    "\n",
    "        self.build()\n",
    "        self.build_eval()\n",
    "        \n",
    "\n",
    "    #def update_field(self, dic={}):\n",
    "    #    self.placeholders= dic\n",
    "    #    self.inputs= self.placeholders['features']\n",
    "        \n",
    "    def _loss(self):  # should be implemented\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            pass\n",
    "            #self.loss += self.params[\"l2_weight_decay\"] * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'])#,\n",
    "                                                  #self.placeholders['labels_mask'])\n",
    "    \n",
    "    def _loss_eval(self):  # should be implemented\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            pass\n",
    "            #self.loss += self.params[\"l2_weight_decay\"] * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss_eval += masked_softmax_cross_entropy(self.outputs_eval, self.placeholders['labels_eval'])#,\n",
    "                                                  #self.placeholders['labels_mask'])\n",
    "    \n",
    "    def output_avg(self):\n",
    "        self.out_avg, self.out_std = tf.nn.moments(tf.reshape(self.outputs,[-1]), axes=[0])\n",
    "\n",
    "    def _accuracy(self): # should be implemented\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels']) #,\n",
    "                                        #self.placeholders['labels_mask'])\n",
    "    def _accuracy_eval(self): # should be implemented\n",
    "        self.accuracy_eval = masked_accuracy(self.outputs_eval, self.placeholders['labels_eval']) #,\n",
    "                                        #self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "        in_dim = self.input_dim\n",
    "        layer_constructors = {'GC': GraphSage,\n",
    "                              'CONV1D': Conv_1D,\n",
    "                              'FC': Dense,\n",
    "                              'FC_SPEC': Dense}\n",
    "        \n",
    "        for i, [layer_type, out_dim] in enumerate(self.params['Arch']):\n",
    "            assert layer_type.upper() in list(layer_constructors.keys())\n",
    "            layer_const = layer_constructors[layer_type.upper()]\n",
    "            \n",
    "            # cancatenation of the CL and GL features\n",
    "            if self.params['residual_mode']:\n",
    "                if i==2:\n",
    "                    in_dim= 2*in_dim\n",
    "                    \n",
    "            kwargs = dict(input_dim= in_dim,\n",
    "                          output_dim=out_dim,\n",
    "                          placeholders=self.placeholders,\n",
    "                          placeholders_val= self.placeholders_val,\n",
    "                          params= self.params,\n",
    "                          act=tf.nn.relu, # not sure about activation\n",
    "                          dropout=True, # not sure about dropout implementation\n",
    "                          sparse_inputs=False,\n",
    "                          bias= True,\n",
    "                          logging=self.logging)\n",
    "            \n",
    "            \n",
    "            #Special accomodations for FC_spec\n",
    "#             if layer_type.upper() == 'FC_SPEC':\n",
    "#                 kwargs['input_dim'] = kwargs['input_dim'] * self.params[\"probe_length\"]\n",
    "#                 kwargs['is_2D'] = False\n",
    "#             if layer_type.upper() == 'FC':\n",
    "#                 #if not(self.params['Arch'][i-1][0] == 'FC'):\n",
    "#                 #    kwargs['input_dim'] = kwargs['input_dim'] * self.params[\"probe_length\"]\n",
    "#                 #    kwargs['is_2D'] = False\n",
    "#                 #else:\n",
    "#                 kwargs['is_2D'] = True # FOR ELSE \n",
    "                \n",
    "#                 if i == (len(self.params['Arch'])-1):\n",
    "#                     kwargs['act'] = lambda x: x\n",
    "             \n",
    "            \n",
    "             #Special accomodations for FC\n",
    "            if layer_type.upper() == 'FC':\n",
    "                if not(self.params['Arch'][i-1][0] == 'FC'):\n",
    "                    kwargs['input_dim'] = kwargs['input_dim'] * self.params[\"probe_length\"]\n",
    "                    kwargs['is_2D'] = False\n",
    "                else:\n",
    "                    kwargs['is_2D'] = True\n",
    "                \n",
    "                if i == (len(self.params['Arch'])-1):\n",
    "                    kwargs['act'] = lambda x: x\n",
    "                        \n",
    "            self.layers.append(layer_const(**kwargs))\n",
    "            in_dim = out_dim\n",
    "            \n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)\n",
    "\n",
    "\n",
    "def batch_normalize(orig_values):\n",
    "    \"\"\"normalize layer across batch\"\"\"\n",
    "    [mean, var] = tf.nn.moments(orig_values, 0)\n",
    "    norm_values = tf.nn.batch_normalization(orig_values, mean, var,\n",
    "                                            offset=None,\n",
    "                                            scale=None,\n",
    "                                            variance_epsilon=1e-3)\n",
    "    return norm_values\n",
    "\n",
    "# placeholder for validation\n",
    "def construct_feed_dict(dropout, is_training, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['dropout']: dropout})\n",
    "    feed_dict.update({placeholders['is_training']: is_training})\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(var_list):\n",
    "    param_counts = []\n",
    "    for variable in var_list:\n",
    "        variable_parameters = 1\n",
    "        shape = variable.get_shape()\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        param_counts.append(variable_parameters)\n",
    "    return np.sum(param_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### start load_data\n",
      "data class_column: 1957\n",
      "data num_metadata: 24\n",
      "data num_examples: 50000\n",
      "data number of datafiles: 1\n",
      "data example datafile: ./data/train/file0.tfrecord\n",
      "data batch_size: 256\n",
      "data nreaders: 1\n",
      "WARNING:tensorflow:From /s/damavand/f/homes/sadafgh/sadaf_test/software/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/training/input.py:187: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /s/damavand/f/homes/sadafgh/sadaf_test/software/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "data orig input_size: 1956\n",
      "data final input_size: 1956\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    # main body \n",
    "    # loading the data (network and node features)\n",
    "    feat_b, label_b, meta_b, input_size, nummeta, numexamps = load_data.get_batch(param_dict, True)\n",
    "\n",
    "    print(\"data final input_size: \" + str(input_size), file= open('printout','a'))\n",
    "    print(\"data nummeta: \" + str(nummeta), file= open('printout','a'))\n",
    "\n",
    "    # loading the interaction network and probe gene order then bulding adjacency\n",
    "    saved_list_names= [param_dict[\"network_file_name\"], \n",
    "                       'probe_gene_order_'+param_dict[\"net_type\"], 'main_gene_order']\n",
    "    objects= []\n",
    "    for i, name in enumerate(saved_list_names):\n",
    "        infile = open(param_dict[\"network_dir\"]+name+'.pkl',\"rb\")\n",
    "        objects.append(pkl.load(infile))\n",
    "    infile.close()\n",
    "\n",
    "    network_dict, probe_gene_order, main_gene_order= tuple(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### start load_data\n",
      "data class_column: 1957\n",
      "data num_metadata: 24\n",
      "data num_examples: 15000\n",
      "data number of datafiles: 1\n",
      "data example datafile: ./data/test/file0.tfrecord\n",
      "data batch_size: 256\n",
      "data nreaders: 1\n",
      "data orig input_size: 1956\n",
      "data final input_size: 1956\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    param_dict['train_max_steps']= 100000 #5\n",
    "    param_dict[\"save_freq\"]= int(50000/param_dict['data_batch_size']) # 1 # 391 # int(50000/param_dict['data_batch_size'])\n",
    "\n",
    "    # setup for validation dataset \n",
    "    identifier= 'eval'\n",
    "    param_dict_eval= param_dict.copy()\n",
    "    param_dict_eval[\"data_dir\"]= './data/'+identifier+'/'\n",
    "    param_dict_eval['eval_max_steps']= int(5000/param_dict_eval['data_batch_size']) #1\n",
    "    param_dict_eval['dropout']= 0.\n",
    "    param_dict_eval['is_training']= False\n",
    "    #param_dict_eval['data_batch_size']= 500\n",
    "\n",
    "    # building adj matrix (sparse)\n",
    "    adj_new, adj_prev= build_adj(network_dict, param_dict['adj_sparse'])\n",
    "\n",
    "    # preprocess the first batch of features\n",
    "    features_new= preprocess_features(feat_b, probe_gene_order, main_gene_order, param_dict['data_sparse'])\n",
    "    true_labels_one_hot = tf.one_hot(label_b, depth=param_dict['out_label_count'],\n",
    "                                             on_value=1., off_value=0., dtype= tf.float32) #  a tensor with shape (sample_size, number of classes)\n",
    "\n",
    "    # loading the validation data\n",
    "    feat_eval_b, label_eval_b, meta_eval_b, input_eval_size, nummeta_eval, numexamps_eval = load_data.get_batch(param_dict_eval, False)\n",
    "    # preprocess the first batch of features\n",
    "    features_new_eval= preprocess_features(feat_eval_b, probe_gene_order, main_gene_order, param_dict_eval['data_sparse'])\n",
    "    true_labels_one_hot_eval = tf.one_hot(label_eval_b, depth=param_dict_eval['out_label_count'],\n",
    "                                             on_value=1., off_value=0., dtype= tf.float32) #  a tensor with shape (sample_size, number of classes)\n",
    "\n",
    "    # holder dict will be extended later\n",
    "    holder= {'features': features_new, 'labels': true_labels_one_hot, 'support': [adj_new], \n",
    "             'features_eval': features_new_eval, 'labels_eval': true_labels_one_hot_eval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-26421510e6cf>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-9-7a7ff100fce8>:17: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Restoring parameters from ./model-save/tmp/gcn.ckpt\n",
      "Model restored from file: ./model-save/tmp/\n",
      "Epoch: 0001 val_loss=  1.61343 val_accuracy=  0.52734\n",
      "Epoch: 0002 val_loss=  1.65511 val_accuracy=  0.51562\n",
      "Epoch: 0003 val_loss=  1.49273 val_accuracy=  0.57031\n",
      "Epoch: 0004 val_loss=  1.52894 val_accuracy=  0.56641\n",
      "Epoch: 0005 val_loss=  1.75918 val_accuracy=  0.47266\n",
      "Epoch: 0006 val_loss=  1.63992 val_accuracy=  0.55859\n",
      "Epoch: 0007 val_loss=  1.66304 val_accuracy=  0.54297\n",
      "Epoch: 0008 val_loss=  1.65712 val_accuracy=  0.50000\n",
      "Epoch: 0009 val_loss=  1.65382 val_accuracy=  0.52344\n",
      "Epoch: 0010 val_loss=  1.47999 val_accuracy=  0.53516\n",
      "Epoch: 0011 val_loss=  1.47463 val_accuracy=  0.57422\n",
      "Epoch: 0012 val_loss=  1.38711 val_accuracy=  0.56250\n",
      "Epoch: 0013 val_loss=  1.59906 val_accuracy=  0.52344\n",
      "Epoch: 0014 val_loss=  1.65591 val_accuracy=  0.51562\n",
      "Epoch: 0015 val_loss=  1.76348 val_accuracy=  0.51562\n",
      "Epoch: 0016 val_loss=  1.84108 val_accuracy=  0.49219\n",
      "Epoch: 0017 val_loss=  1.42227 val_accuracy=  0.60938\n",
      "Epoch: 0018 val_loss=  1.62589 val_accuracy=  0.53516\n",
      "Epoch: 0019 val_loss=  1.63412 val_accuracy=  0.51172\n",
      "Epoch: 0020 val_loss=  1.31280 val_accuracy=  0.62500\n",
      "Epoch: 0021 val_loss=  1.58228 val_accuracy=  0.54688\n",
      "Epoch: 0022 val_loss=  1.64897 val_accuracy=  0.53906\n",
      "Epoch: 0023 val_loss=  1.78923 val_accuracy=  0.51953\n",
      "Epoch: 0024 val_loss=  1.62192 val_accuracy=  0.57031\n",
      "Epoch: 0025 val_loss=  1.59542 val_accuracy=  0.55859\n",
      "Epoch: 0026 val_loss=  1.48524 val_accuracy=  0.57031\n",
      "Epoch: 0027 val_loss=  1.52314 val_accuracy=  0.60156\n",
      "Epoch: 0028 val_loss=  1.68505 val_accuracy=  0.50391\n",
      "Epoch: 0029 val_loss=  1.44342 val_accuracy=  0.56250\n",
      "Epoch: 0030 val_loss=  1.67433 val_accuracy=  0.52344\n",
      "Epoch: 0031 val_loss=  1.56203 val_accuracy=  0.53125\n",
      "Epoch: 0032 val_loss=  1.78847 val_accuracy=  0.50781\n",
      "Epoch: 0033 val_loss=  1.64567 val_accuracy=  0.53906\n",
      "Epoch: 0034 val_loss=  1.73906 val_accuracy=  0.51953\n",
      "Epoch: 0035 val_loss=  1.71215 val_accuracy=  0.53516\n",
      "Epoch: 0036 val_loss=  1.60221 val_accuracy=  0.52344\n",
      "Epoch: 0037 val_loss=  1.31828 val_accuracy=  0.60156\n",
      "Epoch: 0038 val_loss=  1.50802 val_accuracy=  0.57422\n",
      "Epoch: 0039 val_loss=  1.59662 val_accuracy=  0.56250\n",
      "Epoch: 0040 val_loss=  1.44538 val_accuracy=  0.57422\n",
      "Epoch: 0041 val_loss=  1.73771 val_accuracy=  0.51562\n",
      "Epoch: 0042 val_loss=  1.42471 val_accuracy=  0.59766\n",
      "Epoch: 0043 val_loss=  1.59133 val_accuracy=  0.50000\n",
      "Epoch: 0044 val_loss=  1.44924 val_accuracy=  0.60156\n",
      "Epoch: 0045 val_loss=  1.65925 val_accuracy=  0.53516\n",
      "Epoch: 0046 val_loss=  1.58817 val_accuracy=  0.51562\n",
      "Epoch: 0047 val_loss=  1.57303 val_accuracy=  0.56641\n",
      "Epoch: 0048 val_loss=  1.58578 val_accuracy=  0.51562\n",
      "Epoch: 0049 val_loss=  1.76988 val_accuracy=  0.53125\n",
      "Epoch: 0050 val_loss=  1.57314 val_accuracy=  0.54688\n",
      "Epoch: 0051 val_loss=  1.68866 val_accuracy=  0.48047\n",
      "Epoch: 0052 val_loss=  1.48621 val_accuracy=  0.54688\n",
      "Epoch: 0053 val_loss=  1.43159 val_accuracy=  0.55859\n",
      "Epoch: 0054 val_loss=  1.64938 val_accuracy=  0.51562\n",
      "Epoch: 0055 val_loss=  1.46026 val_accuracy=  0.57031\n",
      "Epoch: 0056 val_loss=  1.63842 val_accuracy=  0.57031\n",
      "Epoch: 0057 val_loss=  1.71104 val_accuracy=  0.50781\n",
      "Epoch: 0058 val_loss=  1.45511 val_accuracy=  0.60547\n",
      "Epoch: 0059 val_loss=  1.68094 val_accuracy=  0.51953\n",
      "Epoch: 0060 val_loss=  1.56900 val_accuracy=  0.53906\n",
      "Epoch: 0061 val_loss=  1.62638 val_accuracy=  0.55078\n",
      "Epoch: 0062 val_loss=  1.49083 val_accuracy=  0.55859\n",
      "Epoch: 0063 val_loss=  1.69293 val_accuracy=  0.53125\n",
      "Epoch: 0064 val_loss=  1.63185 val_accuracy=  0.52734\n",
      "Epoch: 0065 val_loss=  1.63098 val_accuracy=  0.55859\n",
      "Epoch: 0066 val_loss=  1.77026 val_accuracy=  0.51562\n",
      "Epoch: 0067 val_loss=  1.51112 val_accuracy=  0.55469\n",
      "Epoch: 0068 val_loss=  1.71407 val_accuracy=  0.50000\n",
      "Epoch: 0069 val_loss=  1.40644 val_accuracy=  0.57812\n",
      "Epoch: 0070 val_loss=  1.51806 val_accuracy=  0.57031\n",
      "Epoch: 0071 val_loss=  1.39968 val_accuracy=  0.58203\n",
      "Epoch: 0072 val_loss=  1.77831 val_accuracy=  0.45703\n",
      "Epoch: 0073 val_loss=  1.60167 val_accuracy=  0.54297\n",
      "Epoch: 0074 val_loss=  1.76102 val_accuracy=  0.51172\n",
      "Epoch: 0075 val_loss=  1.71249 val_accuracy=  0.52734\n",
      "Epoch: 0076 val_loss=  1.38884 val_accuracy=  0.60156\n",
      "Epoch: 0077 val_loss=  1.68277 val_accuracy=  0.53516\n",
      "Epoch: 0078 val_loss=  1.55276 val_accuracy=  0.55469\n",
      "Epoch: 0079 val_loss=  1.53096 val_accuracy=  0.57031\n",
      "Epoch: 0080 val_loss=  1.51762 val_accuracy=  0.56641\n",
      "Epoch: 0081 val_loss=  1.66016 val_accuracy=  0.53125\n",
      "Epoch: 0082 val_loss=  1.66193 val_accuracy=  0.57422\n",
      "Epoch: 0083 val_loss=  1.67865 val_accuracy=  0.53906\n",
      "Epoch: 0084 val_loss=  1.56744 val_accuracy=  0.55469\n",
      "Epoch: 0085 val_loss=  1.52900 val_accuracy=  0.59375\n",
      "Epoch: 0086 val_loss=  1.39276 val_accuracy=  0.61328\n",
      "Epoch: 0087 val_loss=  1.70479 val_accuracy=  0.50000\n",
      "Epoch: 0088 val_loss=  1.55308 val_accuracy=  0.54297\n",
      "Epoch: 0089 val_loss=  1.68478 val_accuracy=  0.51562\n",
      "Epoch: 0090 val_loss=  1.66269 val_accuracy=  0.53516\n",
      "Epoch: 0091 val_loss=  1.64321 val_accuracy=  0.52344\n",
      "Epoch: 0092 val_loss=  1.75318 val_accuracy=  0.50000\n",
      "Epoch: 0093 val_loss=  1.68734 val_accuracy=  0.53516\n",
      "Epoch: 0094 val_loss=  1.64820 val_accuracy=  0.52344\n",
      "Epoch: 0095 val_loss=  1.48531 val_accuracy=  0.56250\n",
      "Epoch: 0096 val_loss=  1.46049 val_accuracy=  0.57031\n",
      "Epoch: 0097 val_loss=  1.57107 val_accuracy=  0.56641\n",
      "Epoch: 0098 val_loss=  1.53242 val_accuracy=  0.58984\n",
      "Epoch: 0099 val_loss=  1.57278 val_accuracy=  0.55078\n",
      "Epoch: 0100 val_loss=  1.58181 val_accuracy=  0.58203\n",
      "Epoch: 0101 val_loss=  1.56388 val_accuracy=  0.52734\n",
      "Epoch: 0102 val_loss=  1.46669 val_accuracy=  0.56641\n",
      "Epoch: 0103 val_loss=  1.58536 val_accuracy=  0.52734\n",
      "Epoch: 0104 val_loss=  1.56658 val_accuracy=  0.57031\n",
      "Epoch: 0105 val_loss=  1.53570 val_accuracy=  0.55078\n",
      "Epoch: 0106 val_loss=  1.65470 val_accuracy=  0.52344\n",
      "Epoch: 0107 val_loss=  1.69902 val_accuracy=  0.51172\n",
      "Epoch: 0108 val_loss=  1.63545 val_accuracy=  0.54297\n",
      "Epoch: 0109 val_loss=  1.62060 val_accuracy=  0.50781\n",
      "Epoch: 0110 val_loss=  1.65128 val_accuracy=  0.49219\n",
      "Epoch: 0111 val_loss=  1.39908 val_accuracy=  0.56641\n",
      "Epoch: 0112 val_loss=  1.55110 val_accuracy=  0.52734\n",
      "Epoch: 0113 val_loss=  1.71373 val_accuracy=  0.48828\n",
      "Epoch: 0114 val_loss=  1.40915 val_accuracy=  0.61328\n",
      "Epoch: 0115 val_loss=  1.69239 val_accuracy=  0.54688\n",
      "Epoch: 0116 val_loss=  1.52682 val_accuracy=  0.58984\n",
      "Epoch: 0117 val_loss=  1.58772 val_accuracy=  0.54688\n",
      "Epoch: 0118 val_loss=  1.72750 val_accuracy=  0.50391\n",
      "Epoch: 0119 val_loss=  1.62225 val_accuracy=  0.52344\n",
      "Epoch: 0120 val_loss=  1.48486 val_accuracy=  0.58203\n",
      "Epoch: 0121 val_loss=  1.51369 val_accuracy=  0.55859\n",
      "Epoch: 0122 val_loss=  1.73290 val_accuracy=  0.48438\n",
      "Epoch: 0123 val_loss=  1.60883 val_accuracy=  0.54688\n",
      "Epoch: 0124 val_loss=  1.70661 val_accuracy=  0.53906\n",
      "Epoch: 0125 val_loss=  1.73254 val_accuracy=  0.50391\n",
      "Epoch: 0126 val_loss=  1.51185 val_accuracy=  0.55859\n",
      "Epoch: 0127 val_loss=  1.65398 val_accuracy=  0.50000\n",
      "Epoch: 0128 val_loss=  1.37467 val_accuracy=  0.60547\n",
      "Epoch: 0129 val_loss=  1.47650 val_accuracy=  0.55469\n",
      "Epoch: 0130 val_loss=  1.51600 val_accuracy=  0.54688\n",
      "Epoch: 0131 val_loss=  1.67288 val_accuracy=  0.51172\n",
      "Epoch: 0132 val_loss=  1.65346 val_accuracy=  0.50781\n",
      "Epoch: 0133 val_loss=  1.87087 val_accuracy=  0.48047\n",
      "Epoch: 0134 val_loss=  1.59129 val_accuracy=  0.58203\n",
      "Epoch: 0135 val_loss=  1.49947 val_accuracy=  0.58984\n",
      "Epoch: 0136 val_loss=  1.68818 val_accuracy=  0.49219\n",
      "Epoch: 0137 val_loss=  1.37621 val_accuracy=  0.62500\n",
      "Epoch: 0138 val_loss=  1.59531 val_accuracy=  0.54688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0139 val_loss=  1.64918 val_accuracy=  0.53906\n",
      "Epoch: 0140 val_loss=  1.66180 val_accuracy=  0.53125\n",
      "Epoch: 0141 val_loss=  1.71026 val_accuracy=  0.53906\n",
      "Epoch: 0142 val_loss=  1.51161 val_accuracy=  0.56641\n",
      "Epoch: 0143 val_loss=  1.52943 val_accuracy=  0.55859\n",
      "Epoch: 0144 val_loss=  1.54667 val_accuracy=  0.59375\n",
      "Epoch: 0145 val_loss=  1.63837 val_accuracy=  0.53125\n",
      "Epoch: 0146 val_loss=  1.47616 val_accuracy=  0.58594\n",
      "Epoch: 0147 val_loss=  1.71130 val_accuracy=  0.49219\n",
      "Epoch: 0148 val_loss=  1.61204 val_accuracy=  0.51562\n",
      "Epoch: 0149 val_loss=  1.78699 val_accuracy=  0.51953\n",
      "Epoch: 0150 val_loss=  1.60601 val_accuracy=  0.55078\n",
      "Epoch: 0151 val_loss=  1.73551 val_accuracy=  0.52344\n",
      "Epoch: 0152 val_loss=  1.73396 val_accuracy=  0.52344\n",
      "Epoch: 0153 val_loss=  1.53685 val_accuracy=  0.54688\n",
      "Epoch: 0154 val_loss=  1.38079 val_accuracy=  0.60156\n",
      "Epoch: 0155 val_loss=  1.54027 val_accuracy=  0.57031\n",
      "Epoch: 0156 val_loss=  1.60539 val_accuracy=  0.56250\n",
      "Epoch: 0157 val_loss=  1.49416 val_accuracy=  0.58203\n",
      "Epoch: 0158 val_loss=  1.68217 val_accuracy=  0.53125\n",
      "Epoch: 0159 val_loss=  1.39656 val_accuracy=  0.58984\n",
      "Epoch: 0160 val_loss=  1.61603 val_accuracy=  0.49219\n",
      "Epoch: 0161 val_loss=  1.41577 val_accuracy=  0.60156\n",
      "Epoch: 0162 val_loss=  1.64177 val_accuracy=  0.52344\n",
      "Epoch: 0163 val_loss=  1.65127 val_accuracy=  0.52734\n",
      "Epoch: 0164 val_loss=  1.51417 val_accuracy=  0.57812\n",
      "Epoch: 0165 val_loss=  1.60359 val_accuracy=  0.51953\n",
      "Epoch: 0166 val_loss=  1.80463 val_accuracy=  0.51562\n",
      "Epoch: 0167 val_loss=  1.51177 val_accuracy=  0.58594\n",
      "Epoch: 0168 val_loss=  1.73600 val_accuracy=  0.46875\n",
      "Epoch: 0169 val_loss=  1.51635 val_accuracy=  0.53125\n",
      "Epoch: 0170 val_loss=  1.40702 val_accuracy=  0.57812\n",
      "Epoch: 0171 val_loss=  1.64648 val_accuracy=  0.50391\n",
      "Epoch: 0172 val_loss=  1.46553 val_accuracy=  0.54297\n",
      "Epoch: 0173 val_loss=  1.56972 val_accuracy=  0.60156\n",
      "Epoch: 0174 val_loss=  1.70407 val_accuracy=  0.50391\n",
      "Epoch: 0175 val_loss=  1.51205 val_accuracy=  0.59375\n",
      "Epoch: 0176 val_loss=  1.66911 val_accuracy=  0.53516\n",
      "Epoch: 0177 val_loss=  1.60261 val_accuracy=  0.50781\n",
      "Epoch: 0178 val_loss=  1.64690 val_accuracy=  0.52344\n",
      "Epoch: 0179 val_loss=  1.45015 val_accuracy=  0.58984\n",
      "Epoch: 0180 val_loss=  1.66015 val_accuracy=  0.55469\n",
      "Epoch: 0181 val_loss=  1.65167 val_accuracy=  0.52344\n",
      "Epoch: 0182 val_loss=  1.63901 val_accuracy=  0.55469\n",
      "Epoch: 0183 val_loss=  1.69922 val_accuracy=  0.53906\n",
      "Epoch: 0184 val_loss=  1.58417 val_accuracy=  0.52734\n",
      "Epoch: 0185 val_loss=  1.70577 val_accuracy=  0.53125\n",
      "Epoch: 0186 val_loss=  1.40681 val_accuracy=  0.56641\n",
      "Epoch: 0187 val_loss=  1.49331 val_accuracy=  0.57031\n",
      "Epoch: 0188 val_loss=  1.41678 val_accuracy=  0.56641\n",
      "Epoch: 0189 val_loss=  1.68533 val_accuracy=  0.50781\n",
      "Epoch: 0190 val_loss=  1.63647 val_accuracy=  0.51953\n",
      "Epoch: 0191 val_loss=  1.74782 val_accuracy=  0.51172\n",
      "Epoch: 0192 val_loss=  1.74886 val_accuracy=  0.51562\n",
      "Epoch: 0193 val_loss=  1.40590 val_accuracy=  0.60938\n",
      "Epoch: 0194 val_loss=  1.74049 val_accuracy=  0.51562\n",
      "Epoch: 0195 val_loss=  1.52664 val_accuracy=  0.57812\n",
      "Epoch: 0196 val_loss=  1.43665 val_accuracy=  0.58984\n",
      "Epoch: 0197 val_loss=  1.51189 val_accuracy=  0.54688\n",
      "Epoch: 0198 val_loss=  1.71519 val_accuracy=  0.53516\n",
      "Epoch: 0199 val_loss=  1.70090 val_accuracy=  0.55078\n",
      "Epoch: 0200 val_loss=  1.64568 val_accuracy=  0.54688\n",
      "Epoch: 0201 val_loss=  1.57186 val_accuracy=  0.57422\n",
      "Epoch: 0202 val_loss=  1.56191 val_accuracy=  0.56641\n",
      "Epoch: 0203 val_loss=  1.39168 val_accuracy=  0.60938\n",
      "Epoch: 0204 val_loss=  1.66524 val_accuracy=  0.53125\n",
      "Epoch: 0205 val_loss=  1.55273 val_accuracy=  0.53906\n",
      "Epoch: 0206 val_loss=  1.72840 val_accuracy=  0.51562\n",
      "Epoch: 0207 val_loss=  1.54299 val_accuracy=  0.54297\n",
      "Epoch: 0208 val_loss=  1.70857 val_accuracy=  0.51172\n",
      "Epoch: 0209 val_loss=  1.73711 val_accuracy=  0.49219\n",
      "Epoch: 0210 val_loss=  1.75671 val_accuracy=  0.53125\n",
      "Epoch: 0211 val_loss=  1.58936 val_accuracy=  0.55078\n",
      "Epoch: 0212 val_loss=  1.50978 val_accuracy=  0.57031\n",
      "Epoch: 0213 val_loss=  1.49396 val_accuracy=  0.53906\n",
      "Epoch: 0214 val_loss=  1.44517 val_accuracy=  0.61719\n",
      "Epoch: 0215 val_loss=  1.58136 val_accuracy=  0.55078\n",
      "Epoch: 0216 val_loss=  1.52948 val_accuracy=  0.54688\n",
      "Epoch: 0217 val_loss=  1.72472 val_accuracy=  0.52344\n",
      "Epoch: 0218 val_loss=  1.43481 val_accuracy=  0.57031\n",
      "Epoch: 0219 val_loss=  1.63696 val_accuracy=  0.51172\n",
      "Epoch: 0220 val_loss=  1.44063 val_accuracy=  0.58203\n",
      "Epoch: 0221 val_loss=  1.58102 val_accuracy=  0.56250\n",
      "Epoch: 0222 val_loss=  1.59126 val_accuracy=  0.50000\n",
      "Epoch: 0223 val_loss=  1.64665 val_accuracy=  0.53906\n",
      "Epoch: 0224 val_loss=  1.66336 val_accuracy=  0.49219\n",
      "Epoch: 0225 val_loss=  1.62771 val_accuracy=  0.55078\n",
      "Epoch: 0226 val_loss=  1.66435 val_accuracy=  0.49609\n",
      "Epoch: 0227 val_loss=  1.69156 val_accuracy=  0.49219\n",
      "Epoch: 0228 val_loss=  1.38117 val_accuracy=  0.55469\n",
      "Epoch: 0229 val_loss=  1.49785 val_accuracy=  0.54297\n",
      "Epoch: 0230 val_loss=  1.73170 val_accuracy=  0.48828\n",
      "Epoch: 0231 val_loss=  1.40706 val_accuracy=  0.58203\n",
      "Epoch: 0232 val_loss=  1.64419 val_accuracy=  0.59375\n",
      "Epoch: 0233 val_loss=  1.68649 val_accuracy=  0.53125\n",
      "Epoch: 0234 val_loss=  1.48052 val_accuracy=  0.58594\n",
      "Epoch: 0235 val_loss=  1.69369 val_accuracy=  0.49219\n",
      "Epoch: 0236 val_loss=  1.63039 val_accuracy=  0.53516\n",
      "Epoch: 0237 val_loss=  1.56268 val_accuracy=  0.55859\n",
      "Epoch: 0238 val_loss=  1.44830 val_accuracy=  0.56641\n",
      "Epoch: 0239 val_loss=  1.77520 val_accuracy=  0.50391\n",
      "Epoch: 0240 val_loss=  1.61887 val_accuracy=  0.55859\n",
      "Epoch: 0241 val_loss=  1.63461 val_accuracy=  0.54297\n",
      "Epoch: 0242 val_loss=  1.77260 val_accuracy=  0.50781\n",
      "Epoch: 0243 val_loss=  1.53159 val_accuracy=  0.53125\n",
      "Epoch: 0244 val_loss=  1.62022 val_accuracy=  0.53125\n",
      "Epoch: 0245 val_loss=  1.38260 val_accuracy=  0.59375\n",
      "Epoch: 0246 val_loss=  1.50516 val_accuracy=  0.56250\n",
      "Epoch: 0247 val_loss=  1.45460 val_accuracy=  0.55469\n",
      "Epoch: 0248 val_loss=  1.68898 val_accuracy=  0.48828\n",
      "Epoch: 0249 val_loss=  1.68235 val_accuracy=  0.51562\n",
      "Epoch: 0250 val_loss=  1.84903 val_accuracy=  0.48047\n",
      "Epoch: 0251 val_loss=  1.62768 val_accuracy=  0.56250\n",
      "Epoch: 0252 val_loss=  1.43153 val_accuracy=  0.58984\n",
      "Epoch: 0253 val_loss=  1.65677 val_accuracy=  0.52734\n",
      "Epoch: 0254 val_loss=  1.50917 val_accuracy=  0.56250\n",
      "Epoch: 0255 val_loss=  1.58347 val_accuracy=  0.53516\n",
      "Epoch: 0256 val_loss=  1.50179 val_accuracy=  0.57031\n",
      "Epoch: 0257 val_loss=  1.69279 val_accuracy=  0.51562\n",
      "Epoch: 0258 val_loss=  1.70601 val_accuracy=  0.56641\n",
      "Epoch: 0259 val_loss=  1.52490 val_accuracy=  0.55469\n",
      "Epoch: 0260 val_loss=  1.61831 val_accuracy=  0.55859\n",
      "Epoch: 0261 val_loss=  1.48470 val_accuracy=  0.60156\n",
      "Epoch: 0262 val_loss=  1.53625 val_accuracy=  0.57422\n",
      "Epoch: 0263 val_loss=  1.58956 val_accuracy=  0.53125\n",
      "Epoch: 0264 val_loss=  1.62004 val_accuracy=  0.53125\n",
      "Epoch: 0265 val_loss=  1.66632 val_accuracy=  0.52344\n",
      "Epoch: 0266 val_loss=  1.71444 val_accuracy=  0.53516\n",
      "Epoch: 0267 val_loss=  1.65930 val_accuracy=  0.53516\n",
      "Epoch: 0268 val_loss=  1.75395 val_accuracy=  0.50781\n",
      "Epoch: 0269 val_loss=  1.69470 val_accuracy=  0.53516\n",
      "Epoch: 0270 val_loss=  1.59839 val_accuracy=  0.53906\n",
      "Epoch: 0271 val_loss=  1.43282 val_accuracy=  0.57422\n",
      "Epoch: 0272 val_loss=  1.45321 val_accuracy=  0.57422\n",
      "Epoch: 0273 val_loss=  1.58346 val_accuracy=  0.58203\n",
      "Epoch: 0274 val_loss=  1.56702 val_accuracy=  0.54688\n",
      "Epoch: 0275 val_loss=  1.60234 val_accuracy=  0.55078\n",
      "Epoch: 0276 val_loss=  1.43661 val_accuracy=  0.59375\n",
      "Epoch: 0277 val_loss=  1.66320 val_accuracy=  0.47656\n",
      "Epoch: 0278 val_loss=  1.42826 val_accuracy=  0.58594\n",
      "Epoch: 0279 val_loss=  1.55194 val_accuracy=  0.53906\n",
      "Epoch: 0280 val_loss=  1.67164 val_accuracy=  0.53516\n",
      "Epoch: 0281 val_loss=  1.54175 val_accuracy=  0.58203\n",
      "Epoch: 0282 val_loss=  1.61093 val_accuracy=  0.49609\n",
      "Epoch: 0283 val_loss=  1.67845 val_accuracy=  0.50781\n",
      "Epoch: 0284 val_loss=  1.64486 val_accuracy=  0.56641\n",
      "Epoch: 0285 val_loss=  1.59249 val_accuracy=  0.49219\n",
      "Epoch: 0286 val_loss=  1.65856 val_accuracy=  0.51562\n",
      "Epoch: 0287 val_loss=  1.39710 val_accuracy=  0.56250\n",
      "Epoch: 0288 val_loss=  1.60709 val_accuracy=  0.50000\n",
      "Epoch: 0289 val_loss=  1.56597 val_accuracy=  0.53906\n",
      "Epoch: 0290 val_loss=  1.53130 val_accuracy=  0.60938\n",
      "Epoch: 0291 val_loss=  1.62399 val_accuracy=  0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0292 val_loss=  1.57541 val_accuracy=  0.57031\n",
      "Epoch: 0293 val_loss=  1.62727 val_accuracy=  0.53516\n",
      "Epoch: 0294 val_loss=  1.63073 val_accuracy=  0.52734\n",
      "Epoch: 0295 val_loss=  1.63411 val_accuracy=  0.51172\n",
      "Epoch: 0296 val_loss=  1.48906 val_accuracy=  0.57812\n",
      "Epoch: 0297 val_loss=  1.57348 val_accuracy=  0.55469\n",
      "Epoch: 0298 val_loss=  1.74700 val_accuracy=  0.48828\n",
      "Epoch: 0299 val_loss=  1.63292 val_accuracy=  0.55469\n",
      "Epoch: 0300 val_loss=  1.64273 val_accuracy=  0.55078\n",
      "Epoch: 0301 val_loss=  1.63121 val_accuracy=  0.50391\n",
      "Epoch: 0302 val_loss=  1.70680 val_accuracy=  0.50781\n",
      "Epoch: 0303 val_loss=  1.46070 val_accuracy=  0.54297\n",
      "Epoch: 0304 val_loss=  1.47387 val_accuracy=  0.57031\n",
      "Epoch: 0305 val_loss=  1.40854 val_accuracy=  0.55078\n",
      "Epoch: 0306 val_loss=  1.56250 val_accuracy=  0.53125\n",
      "Epoch: 0307 val_loss=  1.67122 val_accuracy=  0.51953\n",
      "Epoch: 0308 val_loss=  1.75111 val_accuracy=  0.51953\n",
      "Epoch: 0309 val_loss=  1.85437 val_accuracy=  0.48438\n",
      "Epoch: 0310 val_loss=  1.42881 val_accuracy=  0.61719\n",
      "Epoch: 0311 val_loss=  1.62476 val_accuracy=  0.53125\n",
      "Epoch: 0312 val_loss=  1.64284 val_accuracy=  0.51953\n",
      "Epoch: 0313 val_loss=  1.32459 val_accuracy=  0.62891\n",
      "Epoch: 0314 val_loss=  1.59085 val_accuracy=  0.54688\n",
      "Epoch: 0315 val_loss=  1.64722 val_accuracy=  0.53906\n",
      "Epoch: 0316 val_loss=  1.79450 val_accuracy=  0.52344\n",
      "Epoch: 0317 val_loss=  1.61677 val_accuracy=  0.55469\n",
      "Epoch: 0318 val_loss=  1.59874 val_accuracy=  0.55078\n",
      "Epoch: 0319 val_loss=  1.48752 val_accuracy=  0.57031\n",
      "Epoch: 0320 val_loss=  1.48939 val_accuracy=  0.59375\n",
      "Epoch: 0321 val_loss=  1.70172 val_accuracy=  0.50781\n",
      "Epoch: 0322 val_loss=  1.47470 val_accuracy=  0.55078\n",
      "Epoch: 0323 val_loss=  1.66404 val_accuracy=  0.53516\n",
      "Epoch: 0324 val_loss=  1.54905 val_accuracy=  0.53516\n",
      "Epoch: 0325 val_loss=  1.78135 val_accuracy=  0.51953\n",
      "Epoch: 0326 val_loss=  1.66874 val_accuracy=  0.52734\n",
      "Epoch: 0327 val_loss=  1.75522 val_accuracy=  0.52734\n",
      "Epoch: 0328 val_loss=  1.67057 val_accuracy=  0.53516\n",
      "Epoch: 0329 val_loss=  1.60471 val_accuracy=  0.53906\n",
      "Epoch: 0330 val_loss=  1.34701 val_accuracy=  0.58203\n",
      "Epoch: 0331 val_loss=  1.51821 val_accuracy=  0.58203\n",
      "Epoch: 0332 val_loss=  1.55247 val_accuracy=  0.56641\n",
      "Epoch: 0333 val_loss=  1.44597 val_accuracy=  0.57031\n",
      "Epoch: 0334 val_loss=  1.76847 val_accuracy=  0.50781\n",
      "Epoch: 0335 val_loss=  1.44844 val_accuracy=  0.58984\n",
      "Epoch: 0336 val_loss=  1.58486 val_accuracy=  0.50781\n",
      "Epoch: 0337 val_loss=  1.42625 val_accuracy=  0.59766\n",
      "Epoch: 0338 val_loss=  1.67031 val_accuracy=  0.53516\n",
      "Epoch: 0339 val_loss=  1.59424 val_accuracy=  0.51172\n",
      "Epoch: 0340 val_loss=  1.56108 val_accuracy=  0.57812\n",
      "Epoch: 0341 val_loss=  1.58546 val_accuracy=  0.52344\n",
      "Epoch: 0342 val_loss=  1.76561 val_accuracy=  0.52344\n",
      "Epoch: 0343 val_loss=  1.59350 val_accuracy=  0.53125\n",
      "Epoch: 0344 val_loss=  1.69832 val_accuracy=  0.47656\n",
      "Epoch: 0345 val_loss=  1.45939 val_accuracy=  0.55469\n",
      "Epoch: 0346 val_loss=  1.42672 val_accuracy=  0.56250\n",
      "Epoch: 0347 val_loss=  1.68247 val_accuracy=  0.50000\n",
      "Epoch: 0348 val_loss=  1.43547 val_accuracy=  0.58203\n",
      "Epoch: 0349 val_loss=  1.67699 val_accuracy=  0.56250\n",
      "Epoch: 0350 val_loss=  1.66484 val_accuracy=  0.50781\n",
      "Epoch: 0351 val_loss=  1.50830 val_accuracy=  0.58984\n",
      "Epoch: 0352 val_loss=  1.65585 val_accuracy=  0.51562\n",
      "Epoch: 0353 val_loss=  1.57743 val_accuracy=  0.52344\n",
      "Epoch: 0354 val_loss=  1.62106 val_accuracy=  0.55469\n",
      "Epoch: 0355 val_loss=  1.48471 val_accuracy=  0.55469\n",
      "Epoch: 0356 val_loss=  1.66313 val_accuracy=  0.54688\n",
      "Epoch: 0357 val_loss=  1.66538 val_accuracy=  0.52344\n",
      "Epoch: 0358 val_loss=  1.61361 val_accuracy=  0.55469\n",
      "Epoch: 0359 val_loss=  1.77347 val_accuracy=  0.51562\n",
      "Epoch: 0360 val_loss=  1.49586 val_accuracy=  0.55469\n",
      "Epoch: 0361 val_loss=  1.69536 val_accuracy=  0.50391\n",
      "Epoch: 0362 val_loss=  1.40869 val_accuracy=  0.58203\n",
      "Epoch: 0363 val_loss=  1.54335 val_accuracy=  0.56250\n",
      "Epoch: 0364 val_loss=  1.37478 val_accuracy=  0.58984\n",
      "Epoch: 0365 val_loss=  1.77654 val_accuracy=  0.46875\n",
      "Epoch: 0366 val_loss=  1.63901 val_accuracy=  0.52734\n",
      "Epoch: 0367 val_loss=  1.74539 val_accuracy=  0.51562\n",
      "Epoch: 0368 val_loss=  1.69947 val_accuracy=  0.53125\n",
      "Epoch: 0369 val_loss=  1.40576 val_accuracy=  0.59766\n",
      "Epoch: 0370 val_loss=  1.65935 val_accuracy=  0.53516\n",
      "Epoch: 0371 val_loss=  1.54987 val_accuracy=  0.55859\n",
      "Epoch: 0372 val_loss=  1.54829 val_accuracy=  0.55469\n",
      "Epoch: 0373 val_loss=  1.49377 val_accuracy=  0.58594\n",
      "Epoch: 0374 val_loss=  1.69415 val_accuracy=  0.51562\n",
      "Epoch: 0375 val_loss=  1.67867 val_accuracy=  0.57422\n",
      "Epoch: 0376 val_loss=  1.65000 val_accuracy=  0.53516\n",
      "Epoch: 0377 val_loss=  1.56877 val_accuracy=  0.55469\n",
      "Epoch: 0378 val_loss=  1.50270 val_accuracy=  0.59375\n",
      "Epoch: 0379 val_loss=  1.41514 val_accuracy=  0.60156\n",
      "Epoch: 0380 val_loss=  1.69142 val_accuracy=  0.51172\n",
      "Epoch: 0381 val_loss=  1.57468 val_accuracy=  0.54297\n",
      "Epoch: 0382 val_loss=  1.69326 val_accuracy=  0.51562\n",
      "Epoch: 0383 val_loss=  1.66134 val_accuracy=  0.53906\n",
      "Epoch: 0384 val_loss=  1.67006 val_accuracy=  0.51562\n",
      "Epoch: 0385 val_loss=  1.72611 val_accuracy=  0.50781\n",
      "Epoch: 0386 val_loss=  1.69174 val_accuracy=  0.54297\n",
      "Epoch: 0387 val_loss=  1.63772 val_accuracy=  0.53125\n",
      "Epoch: 0388 val_loss=  1.47717 val_accuracy=  0.55078\n",
      "Epoch: 0389 val_loss=  1.45996 val_accuracy=  0.58203\n",
      "Epoch: 0390 val_loss=  1.60548 val_accuracy=  0.55859\n",
      "Epoch: 0391 val_loss=  1.53045 val_accuracy=  0.58203\n",
      "Epoch: 0392 val_loss=  1.56983 val_accuracy=  0.55859\n",
      "Epoch: 0393 val_loss=  1.59202 val_accuracy=  0.56641\n",
      "Epoch: 0394 val_loss=  1.54637 val_accuracy=  0.53906\n",
      "Epoch: 0395 val_loss=  1.46314 val_accuracy=  0.56250\n",
      "Epoch: 0396 val_loss=  1.59593 val_accuracy=  0.53516\n",
      "Epoch: 0397 val_loss=  1.59560 val_accuracy=  0.55859\n",
      "Epoch: 0398 val_loss=  1.51166 val_accuracy=  0.56641\n",
      "Epoch: 0399 val_loss=  1.67207 val_accuracy=  0.51172\n",
      "Epoch: 0400 val_loss=  1.68556 val_accuracy=  0.50781\n",
      "Epoch: 0401 val_loss=  1.61449 val_accuracy=  0.56641\n",
      "Epoch: 0402 val_loss=  1.64100 val_accuracy=  0.50000\n",
      "Epoch: 0403 val_loss=  1.64465 val_accuracy=  0.48438\n",
      "Epoch: 0404 val_loss=  1.39658 val_accuracy=  0.56641\n",
      "Epoch: 0405 val_loss=  1.53967 val_accuracy=  0.53125\n",
      "Epoch: 0406 val_loss=  1.69369 val_accuracy=  0.49219\n",
      "Epoch: 0407 val_loss=  1.44065 val_accuracy=  0.60547\n",
      "Epoch: 0408 val_loss=  1.68199 val_accuracy=  0.55469\n",
      "Epoch: 0409 val_loss=  1.54671 val_accuracy=  0.56641\n",
      "Epoch: 0410 val_loss=  1.56783 val_accuracy=  0.55469\n",
      "Epoch: 0411 val_loss=  1.73010 val_accuracy=  0.50781\n",
      "Epoch: 0412 val_loss=  1.61811 val_accuracy=  0.51953\n",
      "Epoch: 0413 val_loss=  1.49681 val_accuracy=  0.58203\n",
      "Epoch: 0414 val_loss=  1.56091 val_accuracy=  0.56250\n",
      "Epoch: 0415 val_loss=  1.68797 val_accuracy=  0.48438\n",
      "Epoch: 0416 val_loss=  1.61077 val_accuracy=  0.55078\n",
      "Epoch: 0417 val_loss=  1.69507 val_accuracy=  0.54688\n",
      "Epoch: 0418 val_loss=  1.73603 val_accuracy=  0.50781\n",
      "Epoch: 0419 val_loss=  1.52246 val_accuracy=  0.54688\n",
      "Epoch: 0420 val_loss=  1.63766 val_accuracy=  0.51172\n",
      "Epoch: 0421 val_loss=  1.40818 val_accuracy=  0.60156\n",
      "Epoch: 0422 val_loss=  1.44247 val_accuracy=  0.56641\n",
      "Epoch: 0423 val_loss=  1.55849 val_accuracy=  0.53516\n",
      "Epoch: 0424 val_loss=  1.64388 val_accuracy=  0.51172\n",
      "Epoch: 0425 val_loss=  1.68326 val_accuracy=  0.50000\n",
      "Epoch: 0426 val_loss=  1.86706 val_accuracy=  0.47656\n",
      "Epoch: 0427 val_loss=  1.58745 val_accuracy=  0.58594\n",
      "Epoch: 0428 val_loss=  1.50873 val_accuracy=  0.58203\n",
      "Epoch: 0429 val_loss=  1.66199 val_accuracy=  0.51562\n",
      "Epoch: 0430 val_loss=  1.37394 val_accuracy=  0.61719\n",
      "Epoch: 0431 val_loss=  1.58121 val_accuracy=  0.54688\n",
      "Epoch: 0432 val_loss=  1.67050 val_accuracy=  0.53906\n",
      "Epoch: 0433 val_loss=  1.66937 val_accuracy=  0.53125\n",
      "Epoch: 0434 val_loss=  1.68010 val_accuracy=  0.53125\n",
      "Epoch: 0435 val_loss=  1.53156 val_accuracy=  0.57031\n",
      "Epoch: 0436 val_loss=  1.50788 val_accuracy=  0.55859\n",
      "Epoch: 0437 val_loss=  1.55076 val_accuracy=  0.59375\n",
      "Epoch: 0438 val_loss=  1.71402 val_accuracy=  0.51953\n",
      "Epoch: 0439 val_loss=  1.41646 val_accuracy=  0.57812\n",
      "Epoch: 0440 val_loss=  1.72729 val_accuracy=  0.49219\n",
      "Epoch: 0441 val_loss=  1.57025 val_accuracy=  0.53906\n",
      "Epoch: 0442 val_loss=  1.83782 val_accuracy=  0.50391\n",
      "Epoch: 0443 val_loss=  1.57334 val_accuracy=  0.54688\n",
      "Epoch: 0444 val_loss=  1.74687 val_accuracy=  0.52344\n",
      "Epoch: 0445 val_loss=  1.72600 val_accuracy=  0.52344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0446 val_loss=  1.54094 val_accuracy=  0.53906\n",
      "Epoch: 0447 val_loss=  1.34433 val_accuracy=  0.60547\n",
      "Epoch: 0448 val_loss=  1.52443 val_accuracy=  0.58594\n",
      "Epoch: 0449 val_loss=  1.60681 val_accuracy=  0.55859\n",
      "Epoch: 0450 val_loss=  1.51316 val_accuracy=  0.57031\n",
      "Epoch: 0451 val_loss=  1.68520 val_accuracy=  0.53125\n",
      "Epoch: 0452 val_loss=  1.36841 val_accuracy=  0.59375\n",
      "Epoch: 0453 val_loss=  1.63169 val_accuracy=  0.48828\n",
      "Epoch: 0454 val_loss=  1.44054 val_accuracy=  0.58984\n",
      "Epoch: 0455 val_loss=  1.63952 val_accuracy=  0.52344\n",
      "Epoch: 0456 val_loss=  1.60617 val_accuracy=  0.53516\n",
      "Epoch: 0457 val_loss=  1.53599 val_accuracy=  0.58203\n",
      "Epoch: 0458 val_loss=  1.61563 val_accuracy=  0.51562\n",
      "Epoch: 0459 val_loss=  1.78741 val_accuracy=  0.50781\n",
      "Epoch: 0460 val_loss=  1.53517 val_accuracy=  0.58594\n",
      "Epoch: 0461 val_loss=  1.68619 val_accuracy=  0.49219\n",
      "Epoch: 0462 val_loss=  1.53512 val_accuracy=  0.52734\n",
      "Epoch: 0463 val_loss=  1.42768 val_accuracy=  0.55859\n",
      "Epoch: 0464 val_loss=  1.63357 val_accuracy=  0.51953\n",
      "Epoch: 0465 val_loss=  1.46696 val_accuracy=  0.53906\n",
      "Epoch: 0466 val_loss=  1.58476 val_accuracy=  0.58594\n",
      "Epoch: 0467 val_loss=  1.70814 val_accuracy=  0.51172\n",
      "Epoch: 0468 val_loss=  1.49858 val_accuracy=  0.59375\n",
      "Epoch: 0469 val_loss=  1.66832 val_accuracy=  0.54297\n",
      "Epoch: 0470 val_loss=  1.61099 val_accuracy=  0.50000\n",
      "Epoch: 0471 val_loss=  1.63636 val_accuracy=  0.52344\n",
      "Epoch: 0472 val_loss=  1.44706 val_accuracy=  0.58594\n",
      "Epoch: 0473 val_loss=  1.67306 val_accuracy=  0.54688\n",
      "Epoch: 0474 val_loss=  1.59990 val_accuracy=  0.53125\n",
      "Epoch: 0475 val_loss=  1.66013 val_accuracy=  0.55078\n",
      "Epoch: 0476 val_loss=  1.71355 val_accuracy=  0.51953\n",
      "Epoch: 0477 val_loss=  1.59020 val_accuracy=  0.51562\n",
      "Epoch: 0478 val_loss=  1.67954 val_accuracy=  0.53125\n",
      "Epoch: 0479 val_loss=  1.40954 val_accuracy=  0.56250\n",
      "Epoch: 0480 val_loss=  1.50134 val_accuracy=  0.56641\n",
      "Epoch: 0481 val_loss=  1.42710 val_accuracy=  0.58203\n",
      "Epoch: 0482 val_loss=  1.73237 val_accuracy=  0.49219\n",
      "Epoch: 0483 val_loss=  1.60729 val_accuracy=  0.53906\n",
      "Epoch: 0484 val_loss=  1.75823 val_accuracy=  0.51172\n",
      "Epoch: 0485 val_loss=  1.72562 val_accuracy=  0.51562\n",
      "Epoch: 0486 val_loss=  1.36338 val_accuracy=  0.61328\n",
      "Epoch: 0487 val_loss=  1.77123 val_accuracy=  0.51172\n",
      "Epoch: 0488 val_loss=  1.51960 val_accuracy=  0.58203\n",
      "Epoch: 0489 val_loss=  1.44223 val_accuracy=  0.58594\n",
      "Epoch: 0490 val_loss=  1.53304 val_accuracy=  0.55469\n",
      "Epoch: 0491 val_loss=  1.69734 val_accuracy=  0.51953\n",
      "Epoch: 0492 val_loss=  1.68769 val_accuracy=  0.56641\n",
      "Epoch: 0493 val_loss=  1.64913 val_accuracy=  0.53906\n",
      "Epoch: 0494 val_loss=  1.56643 val_accuracy=  0.55859\n",
      "Epoch: 0495 val_loss=  1.57297 val_accuracy=  0.57812\n",
      "Epoch: 0496 val_loss=  1.39721 val_accuracy=  0.60938\n",
      "Epoch: 0497 val_loss=  1.68411 val_accuracy=  0.52734\n",
      "Epoch: 0498 val_loss=  1.51056 val_accuracy=  0.55078\n",
      "Epoch: 0499 val_loss=  1.73799 val_accuracy=  0.50391\n",
      "Epoch: 0500 val_loss=  1.56800 val_accuracy=  0.53516\n",
      "Epoch: 0501 val_loss=  1.67835 val_accuracy=  0.52344\n",
      "Epoch: 0502 val_loss=  1.76062 val_accuracy=  0.48828\n",
      "Epoch: 0503 val_loss=  1.73747 val_accuracy=  0.53906\n",
      "Epoch: 0504 val_loss=  1.62512 val_accuracy=  0.53906\n",
      "Epoch: 0505 val_loss=  1.49495 val_accuracy=  0.57031\n",
      "Epoch: 0506 val_loss=  1.48265 val_accuracy=  0.54688\n",
      "Epoch: 0507 val_loss=  1.45699 val_accuracy=  0.59766\n",
      "Epoch: 0508 val_loss=  1.57693 val_accuracy=  0.55469\n",
      "Epoch: 0509 val_loss=  1.55286 val_accuracy=  0.55469\n",
      "Epoch: 0510 val_loss=  1.68082 val_accuracy=  0.54297\n",
      "Epoch: 0511 val_loss=  1.45770 val_accuracy=  0.56250\n",
      "Epoch: 0512 val_loss=  1.59888 val_accuracy=  0.51562\n",
      "Epoch: 0513 val_loss=  1.44734 val_accuracy=  0.58203\n",
      "Epoch: 0514 val_loss=  1.59618 val_accuracy=  0.55469\n",
      "Epoch: 0515 val_loss=  1.59789 val_accuracy=  0.51953\n",
      "Epoch: 0516 val_loss=  1.63835 val_accuracy=  0.52344\n",
      "Epoch: 0517 val_loss=  1.65725 val_accuracy=  0.50391\n",
      "Epoch: 0518 val_loss=  1.63630 val_accuracy=  0.54688\n",
      "Epoch: 0519 val_loss=  1.70113 val_accuracy=  0.49219\n",
      "Epoch: 0520 val_loss=  1.64999 val_accuracy=  0.48828\n",
      "Epoch: 0521 val_loss=  1.39629 val_accuracy=  0.55859\n",
      "Epoch: 0522 val_loss=  1.49616 val_accuracy=  0.54297\n",
      "Epoch: 0523 val_loss=  1.74373 val_accuracy=  0.48438\n",
      "Epoch: 0524 val_loss=  1.38813 val_accuracy=  0.58594\n",
      "Epoch: 0525 val_loss=  1.64655 val_accuracy=  0.57812\n",
      "Epoch: 0526 val_loss=  1.66104 val_accuracy=  0.53125\n",
      "Epoch: 0527 val_loss=  1.51185 val_accuracy=  0.57812\n",
      "Epoch: 0528 val_loss=  1.70654 val_accuracy=  0.49609\n",
      "Epoch: 0529 val_loss=  1.61104 val_accuracy=  0.53906\n",
      "Epoch: 0530 val_loss=  1.55329 val_accuracy=  0.56641\n",
      "Epoch: 0531 val_loss=  1.47002 val_accuracy=  0.55859\n",
      "Epoch: 0532 val_loss=  1.75980 val_accuracy=  0.49219\n",
      "Epoch: 0533 val_loss=  1.59021 val_accuracy=  0.57422\n",
      "Epoch: 0534 val_loss=  1.66700 val_accuracy=  0.54688\n",
      "Epoch: 0535 val_loss=  1.73479 val_accuracy=  0.51562\n",
      "Epoch: 0536 val_loss=  1.53305 val_accuracy=  0.51953\n",
      "Epoch: 0537 val_loss=  1.62619 val_accuracy=  0.53125\n",
      "Epoch: 0538 val_loss=  1.40491 val_accuracy=  0.58203\n",
      "Epoch: 0539 val_loss=  1.49111 val_accuracy=  0.57031\n",
      "Epoch: 0540 val_loss=  1.44772 val_accuracy=  0.55859\n",
      "Epoch: 0541 val_loss=  1.73691 val_accuracy=  0.48438\n",
      "Epoch: 0542 val_loss=  1.65510 val_accuracy=  0.51172\n",
      "Epoch: 0543 val_loss=  1.85632 val_accuracy=  0.46875\n",
      "Epoch: 0544 val_loss=  1.61652 val_accuracy=  0.56250\n",
      "Epoch: 0545 val_loss=  1.42042 val_accuracy=  0.61328\n",
      "Epoch: 0546 val_loss=  1.66647 val_accuracy=  0.51953\n",
      "Epoch: 0547 val_loss=  1.51897 val_accuracy=  0.56250\n",
      "Epoch: 0548 val_loss=  1.54800 val_accuracy=  0.55078\n",
      "Epoch: 0549 val_loss=  1.49703 val_accuracy=  0.57422\n",
      "Epoch: 0550 val_loss=  1.70578 val_accuracy=  0.51953\n",
      "Epoch: 0551 val_loss=  1.72777 val_accuracy=  0.54688\n",
      "Epoch: 0552 val_loss=  1.48955 val_accuracy=  0.56641\n",
      "Epoch: 0553 val_loss=  1.61422 val_accuracy=  0.55469\n",
      "Epoch: 0554 val_loss=  1.49866 val_accuracy=  0.60547\n",
      "Epoch: 0555 val_loss=  1.56342 val_accuracy=  0.55859\n",
      "Epoch: 0556 val_loss=  1.55947 val_accuracy=  0.53516\n",
      "Epoch: 0557 val_loss=  1.64101 val_accuracy=  0.53125\n",
      "Epoch: 0558 val_loss=  1.66166 val_accuracy=  0.52344\n",
      "Epoch: 0559 val_loss=  1.70061 val_accuracy=  0.53516\n",
      "Epoch: 0560 val_loss=  1.65971 val_accuracy=  0.54297\n",
      "Epoch: 0561 val_loss=  1.74949 val_accuracy=  0.50000\n",
      "Epoch: 0562 val_loss=  1.71557 val_accuracy=  0.52344\n",
      "Epoch: 0563 val_loss=  1.59542 val_accuracy=  0.53906\n",
      "Epoch: 0564 val_loss=  1.42270 val_accuracy=  0.58984\n",
      "Epoch: 0565 val_loss=  1.49455 val_accuracy=  0.55859\n",
      "Epoch: 0566 val_loss=  1.56012 val_accuracy=  0.57812\n",
      "Epoch: 0567 val_loss=  1.55210 val_accuracy=  0.53906\n",
      "Epoch: 0568 val_loss=  1.63600 val_accuracy=  0.54297\n",
      "Epoch: 0569 val_loss=  1.42193 val_accuracy=  0.59766\n",
      "Epoch: 0570 val_loss=  1.66211 val_accuracy=  0.46875\n",
      "Epoch: 0571 val_loss=  1.43685 val_accuracy=  0.58594\n",
      "Epoch: 0572 val_loss=  1.55914 val_accuracy=  0.54688\n",
      "Epoch: 0573 val_loss=  1.67623 val_accuracy=  0.51953\n",
      "Epoch: 0574 val_loss=  1.51709 val_accuracy=  0.60156\n",
      "Epoch: 0575 val_loss=  1.62503 val_accuracy=  0.50391\n",
      "Epoch: 0576 val_loss=  1.66538 val_accuracy=  0.51953\n",
      "Epoch: 0577 val_loss=  1.62931 val_accuracy=  0.57812\n",
      "Epoch: 0578 val_loss=  1.62826 val_accuracy=  0.48828\n",
      "Epoch: 0579 val_loss=  1.64805 val_accuracy=  0.51953\n",
      "Epoch: 0580 val_loss=  1.39075 val_accuracy=  0.57422\n",
      "Epoch: 0581 val_loss=  1.61299 val_accuracy=  0.48438\n",
      "Epoch: 0582 val_loss=  1.54750 val_accuracy=  0.54297\n",
      "Epoch: 0583 val_loss=  1.56544 val_accuracy=  0.59766\n",
      "Epoch: 0584 val_loss=  1.62093 val_accuracy=  0.52734\n",
      "Epoch: 0585 val_loss=  1.57090 val_accuracy=  0.57031\n",
      "Epoch: 0586 val_loss=  1.61508 val_accuracy=  0.53516\n",
      "Epoch: 0587 val_loss=  1.65045 val_accuracy=  0.51562\n",
      "Epoch: 0588 val_loss=  1.63290 val_accuracy=  0.51953\n",
      "Epoch: 0589 val_loss=  1.45582 val_accuracy=  0.58203\n",
      "Epoch: 0590 val_loss=  1.60981 val_accuracy=  0.55469\n",
      "Epoch: 0591 val_loss=  1.73522 val_accuracy=  0.49609\n",
      "Epoch: 0592 val_loss=  1.58390 val_accuracy=  0.57422\n",
      "Epoch: 0593 val_loss=  1.68044 val_accuracy=  0.54297\n",
      "Epoch: 0594 val_loss=  1.61590 val_accuracy=  0.51172\n",
      "Epoch: 0595 val_loss=  1.68759 val_accuracy=  0.51953\n",
      "Epoch: 0596 val_loss=  1.45234 val_accuracy=  0.55469\n",
      "Epoch: 0597 val_loss=  1.49580 val_accuracy=  0.56250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0598 val_loss=  1.44431 val_accuracy=  0.55078\n",
      "Epoch: 0599 val_loss=  1.57448 val_accuracy=  0.51953\n",
      "Epoch: 0600 val_loss=  1.66656 val_accuracy=  0.51172\n",
      "Epoch: 0601 val_loss=  1.73422 val_accuracy=  0.52734\n",
      "Epoch: 0602 val_loss=  1.83091 val_accuracy=  0.49219\n",
      "Epoch: 0603 val_loss=  1.41065 val_accuracy=  0.61328\n",
      "Epoch: 0604 val_loss=  1.67533 val_accuracy=  0.52734\n",
      "Epoch: 0605 val_loss=  1.58741 val_accuracy=  0.53906\n",
      "Epoch: 0606 val_loss=  1.38895 val_accuracy=  0.60938\n",
      "Epoch: 0607 val_loss=  1.54547 val_accuracy=  0.53516\n",
      "Epoch: 0608 val_loss=  1.67908 val_accuracy=  0.53125\n",
      "Epoch: 0609 val_loss=  1.75870 val_accuracy=  0.53125\n",
      "Epoch: 0610 val_loss=  1.62318 val_accuracy=  0.54688\n",
      "Epoch: 0611 val_loss=  1.62169 val_accuracy=  0.54688\n",
      "Epoch: 0612 val_loss=  1.45957 val_accuracy=  0.57812\n",
      "Epoch: 0613 val_loss=  1.49705 val_accuracy=  0.59766\n",
      "Epoch: 0614 val_loss=  1.67650 val_accuracy=  0.51953\n",
      "Epoch: 0615 val_loss=  1.49710 val_accuracy=  0.53516\n",
      "Epoch: 0616 val_loss=  1.63928 val_accuracy=  0.53516\n",
      "Epoch: 0617 val_loss=  1.56955 val_accuracy=  0.52734\n",
      "Epoch: 0618 val_loss=  1.80555 val_accuracy=  0.50000\n",
      "Epoch: 0619 val_loss=  1.66257 val_accuracy=  0.53516\n",
      "Epoch: 0620 val_loss=  1.72965 val_accuracy=  0.52344\n",
      "Epoch: 0621 val_loss=  1.68307 val_accuracy=  0.53125\n",
      "Epoch: 0622 val_loss=  1.57461 val_accuracy=  0.54297\n",
      "Epoch: 0623 val_loss=  1.35661 val_accuracy=  0.58594\n",
      "Epoch: 0624 val_loss=  1.50237 val_accuracy=  0.58984\n",
      "Epoch: 0625 val_loss=  1.55534 val_accuracy=  0.56250\n",
      "Epoch: 0626 val_loss=  1.46948 val_accuracy=  0.57422\n",
      "Epoch: 0627 val_loss=  1.72936 val_accuracy=  0.53125\n",
      "Epoch: 0628 val_loss=  1.45982 val_accuracy=  0.57812\n",
      "Epoch: 0629 val_loss=  1.60587 val_accuracy=  0.49609\n",
      "Epoch: 0630 val_loss=  1.43959 val_accuracy=  0.60156\n",
      "Epoch: 0631 val_loss=  1.67032 val_accuracy=  0.50781\n",
      "Epoch: 0632 val_loss=  1.56125 val_accuracy=  0.52344\n",
      "Epoch: 0633 val_loss=  1.56634 val_accuracy=  0.58203\n",
      "Epoch: 0634 val_loss=  1.58712 val_accuracy=  0.51172\n",
      "Epoch: 0635 val_loss=  1.79281 val_accuracy=  0.50391\n",
      "Epoch: 0636 val_loss=  1.63799 val_accuracy=  0.52344\n",
      "Epoch: 0637 val_loss=  1.65471 val_accuracy=  0.49219\n",
      "Epoch: 0638 val_loss=  1.43578 val_accuracy=  0.55859\n",
      "Epoch: 0639 val_loss=  1.43890 val_accuracy=  0.55469\n",
      "Epoch: 0640 val_loss=  1.72643 val_accuracy=  0.49609\n",
      "Epoch: 0641 val_loss=  1.40315 val_accuracy=  0.58984\n",
      "Epoch: 0642 val_loss=  1.69093 val_accuracy=  0.55469\n",
      "Epoch: 0643 val_loss=  1.63675 val_accuracy=  0.51172\n",
      "Epoch: 0644 val_loss=  1.50336 val_accuracy=  0.58594\n",
      "Epoch: 0645 val_loss=  1.69115 val_accuracy=  0.50781\n",
      "Epoch: 0646 val_loss=  1.56413 val_accuracy=  0.53516\n",
      "Epoch: 0647 val_loss=  1.65171 val_accuracy=  0.55078\n",
      "Epoch: 0648 val_loss=  1.43276 val_accuracy=  0.57031\n",
      "Epoch: 0649 val_loss=  1.70070 val_accuracy=  0.53125\n",
      "Epoch: 0650 val_loss=  1.65634 val_accuracy=  0.53516\n",
      "Epoch: 0651 val_loss=  1.61442 val_accuracy=  0.53906\n",
      "Epoch: 0652 val_loss=  1.81607 val_accuracy=  0.50781\n",
      "Epoch: 0653 val_loss=  1.47174 val_accuracy=  0.55078\n",
      "Epoch: 0654 val_loss=  1.68577 val_accuracy=  0.51172\n",
      "Epoch: 0655 val_loss=  1.38927 val_accuracy=  0.58203\n",
      "Epoch: 0656 val_loss=  1.56345 val_accuracy=  0.54297\n",
      "Epoch: 0657 val_loss=  1.41356 val_accuracy=  0.56641\n",
      "Epoch: 0658 val_loss=  1.70641 val_accuracy=  0.49219\n",
      "Epoch: 0659 val_loss=  1.67073 val_accuracy=  0.51172\n",
      "Epoch: 0660 val_loss=  1.74692 val_accuracy=  0.51172\n",
      "Epoch: 0661 val_loss=  1.69795 val_accuracy=  0.52734\n",
      "Epoch: 0662 val_loss=  1.41993 val_accuracy=  0.58594\n",
      "Epoch: 0663 val_loss=  1.64583 val_accuracy=  0.53906\n",
      "Epoch: 0664 val_loss=  1.52991 val_accuracy=  0.55078\n",
      "Epoch: 0665 val_loss=  1.56560 val_accuracy=  0.55078\n",
      "Epoch: 0666 val_loss=  1.51163 val_accuracy=  0.56641\n",
      "Epoch: 0667 val_loss=  1.66554 val_accuracy=  0.52734\n",
      "Epoch: 0668 val_loss=  1.68739 val_accuracy=  0.57812\n",
      "Epoch: 0669 val_loss=  1.63047 val_accuracy=  0.53906\n",
      "Epoch: 0670 val_loss=  1.59016 val_accuracy=  0.55078\n",
      "Epoch: 0671 val_loss=  1.49720 val_accuracy=  0.60547\n",
      "Epoch: 0672 val_loss=  1.47035 val_accuracy=  0.57812\n",
      "Epoch: 0673 val_loss=  1.65930 val_accuracy=  0.50000\n",
      "Epoch: 0674 val_loss=  1.60482 val_accuracy=  0.52344\n",
      "Epoch: 0675 val_loss=  1.66745 val_accuracy=  0.52734\n",
      "Epoch: 0676 val_loss=  1.66850 val_accuracy=  0.53516\n",
      "Epoch: 0677 val_loss=  1.66979 val_accuracy=  0.51953\n",
      "Epoch: 0678 val_loss=  1.75427 val_accuracy=  0.50781\n",
      "Epoch: 0679 val_loss=  1.66291 val_accuracy=  0.54297\n",
      "Epoch: 0680 val_loss=  1.63946 val_accuracy=  0.51953\n",
      "Epoch: 0681 val_loss=  1.48093 val_accuracy=  0.54297\n",
      "Epoch: 0682 val_loss=  1.47546 val_accuracy=  0.57031\n",
      "Epoch: 0683 val_loss=  1.55621 val_accuracy=  0.56250\n",
      "Epoch: 0684 val_loss=  1.56666 val_accuracy=  0.57031\n",
      "Epoch: 0685 val_loss=  1.56269 val_accuracy=  0.54688\n",
      "Epoch: 0686 val_loss=  1.57653 val_accuracy=  0.57422\n",
      "Epoch: 0687 val_loss=  1.56926 val_accuracy=  0.53125\n",
      "Epoch: 0688 val_loss=  1.45863 val_accuracy=  0.57422\n",
      "Epoch: 0689 val_loss=  1.56792 val_accuracy=  0.53906\n",
      "Epoch: 0690 val_loss=  1.62955 val_accuracy=  0.53516\n",
      "Epoch: 0691 val_loss=  1.53601 val_accuracy=  0.56641\n",
      "Epoch: 0692 val_loss=  1.64648 val_accuracy=  0.50781\n",
      "Epoch: 0693 val_loss=  1.66011 val_accuracy=  0.51562\n",
      "Epoch: 0694 val_loss=  1.62584 val_accuracy=  0.56250\n",
      "Epoch: 0695 val_loss=  1.61621 val_accuracy=  0.51953\n",
      "Epoch: 0696 val_loss=  1.65477 val_accuracy=  0.50391\n",
      "Epoch: 0697 val_loss=  1.40216 val_accuracy=  0.55859\n",
      "Epoch: 0698 val_loss=  1.53136 val_accuracy=  0.53906\n",
      "Epoch: 0699 val_loss=  1.75275 val_accuracy=  0.47656\n",
      "Epoch: 0700 val_loss=  1.42291 val_accuracy=  0.62500\n",
      "Epoch: 0701 val_loss=  1.66564 val_accuracy=  0.55469\n",
      "Epoch: 0702 val_loss=  1.56222 val_accuracy=  0.57422\n",
      "Epoch: 0703 val_loss=  1.56549 val_accuracy=  0.55078\n",
      "Epoch: 0704 val_loss=  1.69332 val_accuracy=  0.50000\n",
      "Epoch: 0705 val_loss=  1.63082 val_accuracy=  0.52344\n",
      "Epoch: 0706 val_loss=  1.50107 val_accuracy=  0.57422\n",
      "Epoch: 0707 val_loss=  1.55296 val_accuracy=  0.55469\n",
      "Epoch: 0708 val_loss=  1.70107 val_accuracy=  0.49219\n",
      "Epoch: 0709 val_loss=  1.65491 val_accuracy=  0.55078\n",
      "Epoch: 0710 val_loss=  1.65257 val_accuracy=  0.53906\n",
      "Epoch: 0711 val_loss=  1.73741 val_accuracy=  0.50781\n",
      "Epoch: 0712 val_loss=  1.55599 val_accuracy=  0.53906\n",
      "Epoch: 0713 val_loss=  1.58796 val_accuracy=  0.51172\n",
      "Epoch: 0714 val_loss=  1.46113 val_accuracy=  0.59375\n",
      "Epoch: 0715 val_loss=  1.40609 val_accuracy=  0.57031\n",
      "Epoch: 0716 val_loss=  1.57528 val_accuracy=  0.54688\n",
      "Epoch: 0717 val_loss=  1.66634 val_accuracy=  0.50781\n",
      "Epoch: 0718 val_loss=  1.67988 val_accuracy=  0.50391\n",
      "Epoch: 0719 val_loss=  1.87574 val_accuracy=  0.49609\n",
      "Epoch: 0720 val_loss=  1.52890 val_accuracy=  0.59375\n",
      "Epoch: 0721 val_loss=  1.55908 val_accuracy=  0.56641\n",
      "Epoch: 0722 val_loss=  1.61222 val_accuracy=  0.51953\n",
      "Epoch: 0723 val_loss=  1.40690 val_accuracy=  0.60547\n",
      "Epoch: 0724 val_loss=  1.57121 val_accuracy=  0.55078\n",
      "Epoch: 0725 val_loss=  1.66525 val_accuracy=  0.54688\n",
      "Epoch: 0726 val_loss=  1.68631 val_accuracy=  0.53516\n",
      "Epoch: 0727 val_loss=  1.66149 val_accuracy=  0.52734\n",
      "Epoch: 0728 val_loss=  1.53989 val_accuracy=  0.56641\n",
      "Epoch: 0729 val_loss=  1.49635 val_accuracy=  0.57031\n",
      "Epoch: 0730 val_loss=  1.53452 val_accuracy=  0.60547\n",
      "Epoch: 0731 val_loss=  1.75809 val_accuracy=  0.50000\n",
      "Epoch: 0732 val_loss=  1.40038 val_accuracy=  0.57031\n",
      "Epoch: 0733 val_loss=  1.73079 val_accuracy=  0.48828\n",
      "Epoch: 0734 val_loss=  1.58098 val_accuracy=  0.53516\n",
      "Epoch: 0735 val_loss=  1.79881 val_accuracy=  0.50781\n",
      "Epoch: 0736 val_loss=  1.59543 val_accuracy=  0.54688\n",
      "Epoch: 0737 val_loss=  1.75658 val_accuracy=  0.53125\n",
      "Epoch: 0738 val_loss=  1.71575 val_accuracy=  0.52344\n",
      "Epoch: 0739 val_loss=  1.52088 val_accuracy=  0.53125\n",
      "Epoch: 0740 val_loss=  1.37845 val_accuracy=  0.58984\n",
      "Epoch: 0741 val_loss=  1.50849 val_accuracy=  0.58203\n",
      "Epoch: 0742 val_loss=  1.58632 val_accuracy=  0.55859\n",
      "Epoch: 0743 val_loss=  1.50260 val_accuracy=  0.57422\n",
      "Epoch: 0744 val_loss=  1.69128 val_accuracy=  0.52344\n",
      "Epoch: 0745 val_loss=  1.38857 val_accuracy=  0.59766\n",
      "Epoch: 0746 val_loss=  1.62410 val_accuracy=  0.49609\n",
      "Epoch: 0747 val_loss=  1.45287 val_accuracy=  0.58984\n",
      "Epoch: 0748 val_loss=  1.64516 val_accuracy=  0.53516\n",
      "Epoch: 0749 val_loss=  1.58737 val_accuracy=  0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0750 val_loss=  1.55018 val_accuracy=  0.57031\n",
      "Epoch: 0751 val_loss=  1.61414 val_accuracy=  0.51562\n",
      "Epoch: 0752 val_loss=  1.78776 val_accuracy=  0.52344\n",
      "Epoch: 0753 val_loss=  1.54054 val_accuracy=  0.56250\n",
      "Epoch: 0754 val_loss=  1.72908 val_accuracy=  0.45703\n",
      "Epoch: 0755 val_loss=  1.48487 val_accuracy=  0.53906\n",
      "Epoch: 0756 val_loss=  1.43065 val_accuracy=  0.56641\n",
      "Epoch: 0757 val_loss=  1.60454 val_accuracy=  0.52734\n",
      "Epoch: 0758 val_loss=  1.51927 val_accuracy=  0.52734\n",
      "Epoch: 0759 val_loss=  1.55987 val_accuracy=  0.59766\n",
      "Epoch: 0760 val_loss=  1.70297 val_accuracy=  0.49609\n",
      "Epoch: 0761 val_loss=  1.51697 val_accuracy=  0.59375\n",
      "Epoch: 0762 val_loss=  1.68153 val_accuracy=  0.53516\n",
      "Epoch: 0763 val_loss=  1.61212 val_accuracy=  0.51562\n",
      "Epoch: 0764 val_loss=  1.58273 val_accuracy=  0.54297\n",
      "Epoch: 0765 val_loss=  1.47225 val_accuracy=  0.57422\n",
      "Epoch: 0766 val_loss=  1.67768 val_accuracy=  0.54297\n",
      "Epoch: 0767 val_loss=  1.59171 val_accuracy=  0.54297\n",
      "Epoch: 0768 val_loss=  1.66745 val_accuracy=  0.55078\n",
      "Epoch: 0769 val_loss=  1.79012 val_accuracy=  0.51172\n",
      "Epoch: 0770 val_loss=  1.54224 val_accuracy=  0.54688\n",
      "Epoch: 0771 val_loss=  1.69518 val_accuracy=  0.51172\n",
      "Epoch: 0772 val_loss=  1.40396 val_accuracy=  0.58203\n",
      "Epoch: 0773 val_loss=  1.49285 val_accuracy=  0.57031\n",
      "Epoch: 0774 val_loss=  1.44677 val_accuracy=  0.58594\n",
      "Epoch: 0775 val_loss=  1.73287 val_accuracy=  0.48047\n",
      "Epoch: 0776 val_loss=  1.60711 val_accuracy=  0.55078\n",
      "Epoch: 0777 val_loss=  1.77222 val_accuracy=  0.51172\n",
      "Epoch: 0778 val_loss=  1.70317 val_accuracy=  0.51172\n",
      "Epoch: 0779 val_loss=  1.39929 val_accuracy=  0.60547\n",
      "Epoch: 0780 val_loss=  1.74182 val_accuracy=  0.51953\n",
      "Epoch: 0781 val_loss=  1.52689 val_accuracy=  0.57812\n",
      "Epoch: 0782 val_loss=  1.45856 val_accuracy=  0.58594\n",
      "Epoch: 0783 val_loss=  1.54062 val_accuracy=  0.55859\n",
      "Epoch: 0784 val_loss=  1.67876 val_accuracy=  0.51953\n",
      "Epoch: 0785 val_loss=  1.66812 val_accuracy=  0.57812\n",
      "Epoch: 0786 val_loss=  1.68983 val_accuracy=  0.53125\n",
      "Epoch: 0787 val_loss=  1.58517 val_accuracy=  0.57812\n",
      "Epoch: 0788 val_loss=  1.55375 val_accuracy=  0.57422\n",
      "Epoch: 0789 val_loss=  1.37808 val_accuracy=  0.60547\n",
      "Epoch: 0790 val_loss=  1.68166 val_accuracy=  0.51953\n",
      "Epoch: 0791 val_loss=  1.54986 val_accuracy=  0.54688\n",
      "Epoch: 0792 val_loss=  1.69705 val_accuracy=  0.51172\n",
      "Epoch: 0793 val_loss=  1.59066 val_accuracy=  0.52734\n",
      "Epoch: 0794 val_loss=  1.67174 val_accuracy=  0.52344\n",
      "Epoch: 0795 val_loss=  1.74620 val_accuracy=  0.50000\n",
      "Epoch: 0796 val_loss=  1.74484 val_accuracy=  0.52734\n",
      "Epoch: 0797 val_loss=  1.62728 val_accuracy=  0.53516\n",
      "Epoch: 0798 val_loss=  1.50255 val_accuracy=  0.56641\n",
      "Epoch: 0799 val_loss=  1.45934 val_accuracy=  0.55859\n",
      "Epoch: 0800 val_loss=  1.49355 val_accuracy=  0.57812\n",
      "Epoch: 0801 val_loss=  1.53751 val_accuracy=  0.57031\n",
      "Epoch: 0802 val_loss=  1.59779 val_accuracy=  0.53906\n",
      "Epoch: 0803 val_loss=  1.63792 val_accuracy=  0.54688\n",
      "Epoch: 0804 val_loss=  1.47155 val_accuracy=  0.55859\n",
      "Epoch: 0805 val_loss=  1.58888 val_accuracy=  0.53125\n",
      "Epoch: 0806 val_loss=  1.47517 val_accuracy=  0.55859\n",
      "Epoch: 0807 val_loss=  1.59232 val_accuracy=  0.55469\n",
      "Epoch: 0808 val_loss=  1.62894 val_accuracy=  0.51562\n",
      "Epoch: 0809 val_loss=  1.58441 val_accuracy=  0.53906\n",
      "Epoch: 0810 val_loss=  1.66908 val_accuracy=  0.50000\n",
      "Epoch: 0811 val_loss=  1.61929 val_accuracy=  0.54297\n",
      "Epoch: 0812 val_loss=  1.71707 val_accuracy=  0.49219\n",
      "Epoch: 0813 val_loss=  1.65400 val_accuracy=  0.48047\n",
      "Epoch: 0814 val_loss=  1.37806 val_accuracy=  0.56641\n",
      "Epoch: 0815 val_loss=  1.51797 val_accuracy=  0.54688\n",
      "Epoch: 0816 val_loss=  1.73359 val_accuracy=  0.48047\n",
      "Epoch: 0817 val_loss=  1.40911 val_accuracy=  0.57812\n",
      "Epoch: 0818 val_loss=  1.65417 val_accuracy=  0.58203\n",
      "Epoch: 0819 val_loss=  1.63522 val_accuracy=  0.53516\n",
      "Epoch: 0820 val_loss=  1.52678 val_accuracy=  0.56641\n",
      "Epoch: 0821 val_loss=  1.68629 val_accuracy=  0.49609\n",
      "Epoch: 0822 val_loss=  1.63763 val_accuracy=  0.53516\n",
      "Epoch: 0823 val_loss=  1.52016 val_accuracy=  0.57422\n",
      "Epoch: 0824 val_loss=  1.48350 val_accuracy=  0.55469\n",
      "Epoch: 0825 val_loss=  1.76619 val_accuracy=  0.50000\n",
      "Epoch: 0826 val_loss=  1.60726 val_accuracy=  0.55078\n",
      "Epoch: 0827 val_loss=  1.69123 val_accuracy=  0.55078\n",
      "Epoch: 0828 val_loss=  1.66919 val_accuracy=  0.53516\n",
      "Epoch: 0829 val_loss=  1.53707 val_accuracy=  0.52734\n",
      "Epoch: 0830 val_loss=  1.65338 val_accuracy=  0.52344\n",
      "Epoch: 0831 val_loss=  1.39475 val_accuracy=  0.59375\n",
      "Epoch: 0832 val_loss=  1.49988 val_accuracy=  0.56641\n",
      "Epoch: 0833 val_loss=  1.44177 val_accuracy=  0.56250\n",
      "Epoch: 0834 val_loss=  1.74623 val_accuracy=  0.50000\n",
      "Epoch: 0835 val_loss=  1.63299 val_accuracy=  0.51172\n",
      "Epoch: 0836 val_loss=  1.86731 val_accuracy=  0.46875\n",
      "Epoch: 0837 val_loss=  1.62267 val_accuracy=  0.57031\n",
      "Epoch: 0838 val_loss=  1.42038 val_accuracy=  0.61328\n",
      "Epoch: 0839 val_loss=  1.70461 val_accuracy=  0.51172\n",
      "Epoch: 0840 val_loss=  1.45035 val_accuracy=  0.58594\n",
      "Epoch: 0841 val_loss=  1.58603 val_accuracy=  0.54297\n",
      "Epoch: 0842 val_loss=  1.54987 val_accuracy=  0.57422\n",
      "Epoch: 0843 val_loss=  1.65563 val_accuracy=  0.53125\n",
      "Epoch: 0844 val_loss=  1.73758 val_accuracy=  0.53906\n",
      "Epoch: 0845 val_loss=  1.49842 val_accuracy=  0.56641\n",
      "Epoch: 0846 val_loss=  1.57962 val_accuracy=  0.55859\n",
      "Epoch: 0847 val_loss=  1.52780 val_accuracy=  0.60156\n",
      "Epoch: 0848 val_loss=  1.58913 val_accuracy=  0.55078\n",
      "Epoch: 0849 val_loss=  1.54250 val_accuracy=  0.54688\n",
      "Epoch: 0850 val_loss=  1.62635 val_accuracy=  0.53516\n",
      "Epoch: 0851 val_loss=  1.65439 val_accuracy=  0.51953\n",
      "Epoch: 0852 val_loss=  1.71031 val_accuracy=  0.53125\n",
      "Epoch: 0853 val_loss=  1.65943 val_accuracy=  0.54688\n",
      "Epoch: 0854 val_loss=  1.79614 val_accuracy=  0.48438\n",
      "Epoch: 0855 val_loss=  1.67216 val_accuracy=  0.53516\n",
      "Epoch: 0856 val_loss=  1.57743 val_accuracy=  0.52734\n",
      "Epoch: 0857 val_loss=  1.42818 val_accuracy=  0.59375\n",
      "Epoch: 0858 val_loss=  1.50965 val_accuracy=  0.55859\n",
      "Epoch: 0859 val_loss=  1.57360 val_accuracy=  0.57422\n",
      "Epoch: 0860 val_loss=  1.51947 val_accuracy=  0.55469\n",
      "Epoch: 0861 val_loss=  1.65022 val_accuracy=  0.54688\n",
      "Epoch: 0862 val_loss=  1.43461 val_accuracy=  0.58594\n",
      "Epoch: 0863 val_loss=  1.64408 val_accuracy=  0.47656\n",
      "Epoch: 0864 val_loss=  1.44549 val_accuracy=  0.58984\n",
      "Epoch: 0865 val_loss=  1.55204 val_accuracy=  0.55078\n",
      "Epoch: 0866 val_loss=  1.68377 val_accuracy=  0.51562\n",
      "Epoch: 0867 val_loss=  1.50581 val_accuracy=  0.59375\n",
      "Epoch: 0868 val_loss=  1.63375 val_accuracy=  0.50000\n",
      "Epoch: 0869 val_loss=  1.71480 val_accuracy=  0.51953\n",
      "Epoch: 0870 val_loss=  1.58450 val_accuracy=  0.58984\n",
      "Epoch: 0871 val_loss=  1.66869 val_accuracy=  0.47656\n",
      "Epoch: 0872 val_loss=  1.58943 val_accuracy=  0.53125\n",
      "Epoch: 0873 val_loss=  1.41858 val_accuracy=  0.55469\n",
      "Epoch: 0874 val_loss=  1.60010 val_accuracy=  0.50000\n",
      "Epoch: 0875 val_loss=  1.54555 val_accuracy=  0.54297\n",
      "Epoch: 0876 val_loss=  1.55813 val_accuracy=  0.59766\n",
      "Epoch: 0877 val_loss=  1.65719 val_accuracy=  0.51953\n",
      "Epoch: 0878 val_loss=  1.52799 val_accuracy=  0.57031\n",
      "Epoch: 0879 val_loss=  1.64957 val_accuracy=  0.53516\n",
      "Epoch: 0880 val_loss=  1.65582 val_accuracy=  0.50000\n",
      "Epoch: 0881 val_loss=  1.63956 val_accuracy=  0.52734\n",
      "Epoch: 0882 val_loss=  1.41683 val_accuracy=  0.59375\n",
      "Epoch: 0883 val_loss=  1.64141 val_accuracy=  0.53906\n",
      "Epoch: 0884 val_loss=  1.70656 val_accuracy=  0.50000\n",
      "Epoch: 0885 val_loss=  1.60137 val_accuracy=  0.58203\n",
      "Epoch: 0886 val_loss=  1.67475 val_accuracy=  0.54297\n",
      "Epoch: 0887 val_loss=  1.60849 val_accuracy=  0.51172\n",
      "Epoch: 0888 val_loss=  1.71523 val_accuracy=  0.51172\n",
      "Epoch: 0889 val_loss=  1.44637 val_accuracy=  0.56250\n",
      "Epoch: 0890 val_loss=  1.52842 val_accuracy=  0.55859\n",
      "Epoch: 0891 val_loss=  1.38756 val_accuracy=  0.57031\n",
      "Epoch: 0892 val_loss=  1.58631 val_accuracy=  0.52734\n",
      "Epoch: 0893 val_loss=  1.68299 val_accuracy=  0.49609\n",
      "Epoch: 0894 val_loss=  1.69855 val_accuracy=  0.53906\n",
      "Epoch: 0895 val_loss=  1.83492 val_accuracy=  0.50000\n",
      "Epoch: 0896 val_loss=  1.43395 val_accuracy=  0.59766\n",
      "Epoch: 0897 val_loss=  1.67916 val_accuracy=  0.52734\n",
      "Epoch: 0898 val_loss=  1.59006 val_accuracy=  0.54297\n",
      "Epoch: 0899 val_loss=  1.40027 val_accuracy=  0.60938\n",
      "Epoch: 0900 val_loss=  1.52691 val_accuracy=  0.55078\n",
      "Epoch: 0901 val_loss=  1.67634 val_accuracy=  0.54297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0902 val_loss=  1.73753 val_accuracy=  0.53125\n",
      "Epoch: 0903 val_loss=  1.62456 val_accuracy=  0.54297\n",
      "Epoch: 0904 val_loss=  1.62571 val_accuracy=  0.55078\n",
      "Epoch: 0905 val_loss=  1.49183 val_accuracy=  0.58984\n",
      "Epoch: 0906 val_loss=  1.44026 val_accuracy=  0.60156\n",
      "Epoch: 0907 val_loss=  1.70163 val_accuracy=  0.51562\n",
      "Epoch: 0908 val_loss=  1.49538 val_accuracy=  0.53906\n",
      "Epoch: 0909 val_loss=  1.66721 val_accuracy=  0.51953\n",
      "Epoch: 0910 val_loss=  1.55756 val_accuracy=  0.53125\n",
      "Epoch: 0911 val_loss=  1.78954 val_accuracy=  0.51562\n",
      "Epoch: 0912 val_loss=  1.67127 val_accuracy=  0.52734\n",
      "Epoch: 0913 val_loss=  1.74759 val_accuracy=  0.52734\n",
      "Epoch: 0914 val_loss=  1.65020 val_accuracy=  0.53906\n",
      "Epoch: 0915 val_loss=  1.57906 val_accuracy=  0.55859\n",
      "Epoch: 0916 val_loss=  1.32450 val_accuracy=  0.59375\n",
      "Epoch: 0917 val_loss=  1.54020 val_accuracy=  0.58984\n",
      "Epoch: 0918 val_loss=  1.53751 val_accuracy=  0.56641\n",
      "Epoch: 0919 val_loss=  1.51310 val_accuracy=  0.56250\n",
      "Epoch: 0920 val_loss=  1.70662 val_accuracy=  0.52734\n",
      "Epoch: 0921 val_loss=  1.43042 val_accuracy=  0.57812\n",
      "Epoch: 0922 val_loss=  1.64777 val_accuracy=  0.48828\n",
      "Epoch: 0923 val_loss=  1.43447 val_accuracy=  0.59375\n",
      "Epoch: 0924 val_loss=  1.67151 val_accuracy=  0.51172\n",
      "Epoch: 0925 val_loss=  1.55435 val_accuracy=  0.52344\n",
      "Epoch: 0926 val_loss=  1.56493 val_accuracy=  0.57422\n",
      "Epoch: 0927 val_loss=  1.59222 val_accuracy=  0.51562\n",
      "Epoch: 0928 val_loss=  1.78325 val_accuracy=  0.51953\n",
      "Epoch: 0929 val_loss=  1.65644 val_accuracy=  0.50781\n",
      "Epoch: 0930 val_loss=  1.63115 val_accuracy=  0.49219\n",
      "Epoch: 0931 val_loss=  1.45177 val_accuracy=  0.55469\n",
      "Epoch: 0932 val_loss=  1.48747 val_accuracy=  0.53516\n",
      "Epoch: 0933 val_loss=  1.69189 val_accuracy=  0.49219\n",
      "Epoch: 0934 val_loss=  1.41336 val_accuracy=  0.58594\n",
      "Epoch: 0935 val_loss=  1.67721 val_accuracy=  0.57031\n",
      "Epoch: 0936 val_loss=  1.60661 val_accuracy=  0.52344\n",
      "Epoch: 0937 val_loss=  1.50531 val_accuracy=  0.58594\n",
      "Epoch: 0938 val_loss=  1.70647 val_accuracy=  0.50781\n",
      "Epoch: 0939 val_loss=  1.59215 val_accuracy=  0.53125\n",
      "Epoch: 0940 val_loss=  1.65374 val_accuracy=  0.54688\n",
      "Epoch: 0941 val_loss=  1.44254 val_accuracy=  0.56250\n",
      "Epoch: 0942 val_loss=  1.72124 val_accuracy=  0.52734\n",
      "Epoch: 0943 val_loss=  1.60166 val_accuracy=  0.54297\n",
      "Epoch: 0944 val_loss=  1.64609 val_accuracy=  0.53125\n",
      "Epoch: 0945 val_loss=  1.81018 val_accuracy=  0.50000\n",
      "Epoch: 0946 val_loss=  1.45684 val_accuracy=  0.55859\n",
      "Epoch: 0947 val_loss=  1.67440 val_accuracy=  0.52344\n",
      "Epoch: 0948 val_loss=  1.38426 val_accuracy=  0.58203\n",
      "Epoch: 0949 val_loss=  1.57258 val_accuracy=  0.54297\n",
      "Epoch: 0950 val_loss=  1.42031 val_accuracy=  0.57812\n",
      "Epoch: 0951 val_loss=  1.68200 val_accuracy=  0.49219\n",
      "Epoch: 0952 val_loss=  1.68423 val_accuracy=  0.50391\n",
      "Epoch: 0953 val_loss=  1.74699 val_accuracy=  0.51562\n",
      "Epoch: 0954 val_loss=  1.69365 val_accuracy=  0.53125\n",
      "Epoch: 0955 val_loss=  1.38881 val_accuracy=  0.60547\n",
      "Epoch: 0956 val_loss=  1.69442 val_accuracy=  0.52344\n",
      "Epoch: 0957 val_loss=  1.52274 val_accuracy=  0.55859\n",
      "Epoch: 0958 val_loss=  1.59146 val_accuracy=  0.54688\n",
      "Epoch: 0959 val_loss=  1.46669 val_accuracy=  0.57031\n",
      "Epoch: 0960 val_loss=  1.66195 val_accuracy=  0.53906\n",
      "Epoch: 0961 val_loss=  1.71458 val_accuracy=  0.55469\n",
      "Epoch: 0962 val_loss=  1.61138 val_accuracy=  0.55078\n",
      "Epoch: 0963 val_loss=  1.58117 val_accuracy=  0.55078\n",
      "Epoch: 0964 val_loss=  1.50700 val_accuracy=  0.59375\n",
      "Epoch: 0965 val_loss=  1.52090 val_accuracy=  0.57812\n",
      "Epoch: 0966 val_loss=  1.61896 val_accuracy=  0.51953\n",
      "Epoch: 0967 val_loss=  1.62365 val_accuracy=  0.51562\n",
      "Epoch: 0968 val_loss=  1.68715 val_accuracy=  0.51953\n",
      "Epoch: 0969 val_loss=  1.63724 val_accuracy=  0.54688\n",
      "Epoch: 0970 val_loss=  1.67967 val_accuracy=  0.51953\n",
      "Epoch: 0971 val_loss=  1.75183 val_accuracy=  0.50781\n",
      "Epoch: 0972 val_loss=  1.66568 val_accuracy=  0.53516\n",
      "Epoch: 0973 val_loss=  1.65770 val_accuracy=  0.53125\n",
      "Epoch: 0974 val_loss=  1.44704 val_accuracy=  0.56250\n",
      "Epoch: 0975 val_loss=  1.47861 val_accuracy=  0.55469\n",
      "Epoch: 0976 val_loss=  1.57889 val_accuracy=  0.56641\n",
      "Epoch: 0977 val_loss=  1.55803 val_accuracy=  0.57031\n",
      "Epoch: 0978 val_loss=  1.56910 val_accuracy=  0.55859\n",
      "Epoch: 0979 val_loss=  1.56022 val_accuracy=  0.57812\n",
      "Epoch: 0980 val_loss=  1.58661 val_accuracy=  0.52344\n",
      "Epoch: 0981 val_loss=  1.46832 val_accuracy=  0.58203\n",
      "Epoch: 0982 val_loss=  1.54336 val_accuracy=  0.53516\n",
      "Epoch: 0983 val_loss=  1.63601 val_accuracy=  0.52344\n",
      "Epoch: 0984 val_loss=  1.51444 val_accuracy=  0.58203\n",
      "Epoch: 0985 val_loss=  1.63059 val_accuracy=  0.51562\n",
      "Epoch: 0986 val_loss=  1.68820 val_accuracy=  0.52344\n",
      "Epoch: 0987 val_loss=  1.63927 val_accuracy=  0.55859\n",
      "Epoch: 0988 val_loss=  1.58934 val_accuracy=  0.51953\n",
      "Epoch: 0989 val_loss=  1.67057 val_accuracy=  0.50000\n",
      "Epoch: 0990 val_loss=  1.35727 val_accuracy=  0.57031\n",
      "Epoch: 0991 val_loss=  1.54443 val_accuracy=  0.53516\n",
      "Epoch: 0992 val_loss=  1.73868 val_accuracy=  0.47266\n",
      "Epoch: 0993 val_loss=  1.42742 val_accuracy=  0.62109\n",
      "Epoch: 0994 val_loss=  1.65777 val_accuracy=  0.55078\n",
      "Epoch: 0995 val_loss=  1.58768 val_accuracy=  0.57031\n",
      "Epoch: 0996 val_loss=  1.54816 val_accuracy=  0.55078\n",
      "Epoch: 0997 val_loss=  1.69554 val_accuracy=  0.49609\n",
      "Epoch: 0998 val_loss=  1.61982 val_accuracy=  0.51562\n",
      "Epoch: 0999 val_loss=  1.49525 val_accuracy=  0.57422\n",
      "Epoch: 1000 val_loss=  1.55182 val_accuracy=  0.55469\n",
      "Epoch: 1001 val_loss=  1.70131 val_accuracy=  0.46875\n",
      "Epoch: 1002 val_loss=  1.62852 val_accuracy=  0.56641\n",
      "Epoch: 1003 val_loss=  1.68501 val_accuracy=  0.53125\n",
      "Epoch: 1004 val_loss=  1.72909 val_accuracy=  0.50391\n",
      "Epoch: 1005 val_loss=  1.56089 val_accuracy=  0.54297\n",
      "Epoch: 1006 val_loss=  1.58667 val_accuracy=  0.51953\n",
      "Epoch: 1007 val_loss=  1.44365 val_accuracy=  0.60156\n",
      "Epoch: 1008 val_loss=  1.40134 val_accuracy=  0.56250\n",
      "Epoch: 1009 val_loss=  1.61087 val_accuracy=  0.53516\n",
      "Epoch: 1010 val_loss=  1.64543 val_accuracy=  0.51953\n",
      "Epoch: 1011 val_loss=  1.69695 val_accuracy=  0.50391\n",
      "Epoch: 1012 val_loss=  1.87156 val_accuracy=  0.49609\n",
      "Epoch: 1013 val_loss=  1.46277 val_accuracy=  0.60547\n",
      "Epoch: 1014 val_loss=  1.57145 val_accuracy=  0.55859\n",
      "Epoch: 1015 val_loss=  1.60779 val_accuracy=  0.52344\n",
      "Epoch: 1016 val_loss=  1.39703 val_accuracy=  0.60547\n",
      "Epoch: 1017 val_loss=  1.58260 val_accuracy=  0.54688\n",
      "Epoch: 1018 val_loss=  1.65761 val_accuracy=  0.54688\n",
      "Epoch: 1019 val_loss=  1.70514 val_accuracy=  0.52734\n",
      "Epoch: 1020 val_loss=  1.68532 val_accuracy=  0.52734\n",
      "Epoch: 1021 val_loss=  1.57658 val_accuracy=  0.57422\n",
      "Epoch: 1022 val_loss=  1.49919 val_accuracy=  0.56250\n",
      "Epoch: 1023 val_loss=  1.49029 val_accuracy=  0.60156\n",
      "Epoch: 1024 val_loss=  1.75394 val_accuracy=  0.50781\n",
      "Epoch: 1025 val_loss=  1.38132 val_accuracy=  0.57031\n",
      "Epoch: 1026 val_loss=  1.73541 val_accuracy=  0.49219\n",
      "Epoch: 1027 val_loss=  1.59445 val_accuracy=  0.53906\n",
      "Epoch: 1028 val_loss=  1.78161 val_accuracy=  0.50781\n",
      "Epoch: 1029 val_loss=  1.60022 val_accuracy=  0.55469\n",
      "Epoch: 1030 val_loss=  1.73604 val_accuracy=  0.53906\n",
      "Epoch: 1031 val_loss=  1.73199 val_accuracy=  0.51562\n",
      "Epoch: 1032 val_loss=  1.51749 val_accuracy=  0.53516\n",
      "Epoch: 1033 val_loss=  1.40958 val_accuracy=  0.58203\n",
      "Epoch: 1034 val_loss=  1.51215 val_accuracy=  0.57422\n",
      "Epoch: 1035 val_loss=  1.55493 val_accuracy=  0.56641\n",
      "Epoch: 1036 val_loss=  1.51031 val_accuracy=  0.57031\n",
      "Epoch: 1037 val_loss=  1.74060 val_accuracy=  0.50781\n",
      "Epoch: 1038 val_loss=  1.37962 val_accuracy=  0.60938\n",
      "Epoch: 1039 val_loss=  1.60379 val_accuracy=  0.48828\n",
      "Epoch: 1040 val_loss=  1.44864 val_accuracy=  0.60938\n",
      "Epoch: 1041 val_loss=  1.65105 val_accuracy=  0.51562\n",
      "Epoch: 1042 val_loss=  1.60151 val_accuracy=  0.53125\n",
      "Epoch: 1043 val_loss=  1.55065 val_accuracy=  0.57812\n",
      "Epoch: 1044 val_loss=  1.63394 val_accuracy=  0.51562\n",
      "Epoch: 1045 val_loss=  1.74993 val_accuracy=  0.52344\n",
      "Epoch: 1046 val_loss=  1.56063 val_accuracy=  0.57031\n",
      "Epoch: 1047 val_loss=  1.73084 val_accuracy=  0.44141\n",
      "Epoch: 1048 val_loss=  1.45966 val_accuracy=  0.55859\n",
      "Epoch: 1049 val_loss=  1.42612 val_accuracy=  0.55078\n",
      "Epoch: 1050 val_loss=  1.62091 val_accuracy=  0.52344\n",
      "Epoch: 1051 val_loss=  1.52528 val_accuracy=  0.53516\n",
      "Epoch: 1052 val_loss=  1.55786 val_accuracy=  0.60156\n",
      "Epoch: 1053 val_loss=  1.69640 val_accuracy=  0.49219\n",
      "Epoch: 1054 val_loss=  1.50479 val_accuracy=  0.60938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1055 val_loss=  1.68309 val_accuracy=  0.52344\n",
      "Epoch: 1056 val_loss=  1.62709 val_accuracy=  0.50391\n",
      "Epoch: 1057 val_loss=  1.55978 val_accuracy=  0.55859\n",
      "Epoch: 1058 val_loss=  1.48705 val_accuracy=  0.57812\n",
      "Epoch: 1059 val_loss=  1.67428 val_accuracy=  0.53125\n",
      "Epoch: 1060 val_loss=  1.62006 val_accuracy=  0.53906\n",
      "Epoch: 1061 val_loss=  1.63101 val_accuracy=  0.55469\n",
      "Epoch: 1062 val_loss=  1.81296 val_accuracy=  0.50000\n",
      "Epoch: 1063 val_loss=  1.50328 val_accuracy=  0.56641\n",
      "Epoch: 1064 val_loss=  1.71932 val_accuracy=  0.50391\n",
      "Epoch: 1065 val_loss=  1.40736 val_accuracy=  0.57422\n",
      "Epoch: 1066 val_loss=  1.48960 val_accuracy=  0.57422\n",
      "Epoch: 1067 val_loss=  1.42461 val_accuracy=  0.57812\n",
      "Epoch: 1068 val_loss=  1.77569 val_accuracy=  0.46484\n",
      "Epoch: 1069 val_loss=  1.59576 val_accuracy=  0.54688\n",
      "Epoch: 1070 val_loss=  1.76142 val_accuracy=  0.51562\n",
      "Epoch: 1071 val_loss=  1.70012 val_accuracy=  0.52344\n",
      "Epoch: 1072 val_loss=  1.41230 val_accuracy=  0.60156\n",
      "Epoch: 1073 val_loss=  1.74148 val_accuracy=  0.51562\n",
      "Epoch: 1074 val_loss=  1.51447 val_accuracy=  0.57031\n",
      "Epoch: 1075 val_loss=  1.49828 val_accuracy=  0.57031\n",
      "Epoch: 1076 val_loss=  1.54435 val_accuracy=  0.55859\n",
      "Epoch: 1077 val_loss=  1.64979 val_accuracy=  0.51953\n",
      "Epoch: 1078 val_loss=  1.62798 val_accuracy=  0.58984\n",
      "Epoch: 1079 val_loss=  1.72868 val_accuracy=  0.52344\n",
      "Epoch: 1080 val_loss=  1.56920 val_accuracy=  0.57812\n",
      "Epoch: 1081 val_loss=  1.56338 val_accuracy=  0.57812\n",
      "Epoch: 1082 val_loss=  1.37867 val_accuracy=  0.59766\n",
      "Epoch: 1083 val_loss=  1.69455 val_accuracy=  0.53125\n",
      "Epoch: 1084 val_loss=  1.53783 val_accuracy=  0.53906\n",
      "Epoch: 1085 val_loss=  1.70025 val_accuracy=  0.50781\n",
      "Epoch: 1086 val_loss=  1.62286 val_accuracy=  0.52344\n",
      "Epoch: 1087 val_loss=  1.62717 val_accuracy=  0.53516\n",
      "Epoch: 1088 val_loss=  1.78241 val_accuracy=  0.48828\n",
      "Epoch: 1089 val_loss=  1.70982 val_accuracy=  0.53516\n",
      "Epoch: 1090 val_loss=  1.65633 val_accuracy=  0.51953\n",
      "Epoch: 1091 val_loss=  1.48843 val_accuracy=  0.56250\n",
      "Epoch: 1092 val_loss=  1.47401 val_accuracy=  0.55469\n",
      "Epoch: 1093 val_loss=  1.48965 val_accuracy=  0.57422\n",
      "Epoch: 1094 val_loss=  1.54305 val_accuracy=  0.57812\n",
      "Epoch: 1095 val_loss=  1.59407 val_accuracy=  0.54297\n",
      "Epoch: 1096 val_loss=  1.61147 val_accuracy=  0.55859\n",
      "Epoch: 1097 val_loss=  1.49634 val_accuracy=  0.56250\n",
      "Epoch: 1098 val_loss=  1.56607 val_accuracy=  0.53516\n",
      "Epoch: 1099 val_loss=  1.48579 val_accuracy=  0.55078\n",
      "Epoch: 1100 val_loss=  1.62392 val_accuracy=  0.54688\n",
      "Epoch: 1101 val_loss=  1.58388 val_accuracy=  0.53906\n",
      "Epoch: 1102 val_loss=  1.59859 val_accuracy=  0.52344\n",
      "Epoch: 1103 val_loss=  1.67590 val_accuracy=  0.50000\n",
      "Epoch: 1104 val_loss=  1.61936 val_accuracy=  0.53906\n",
      "Epoch: 1105 val_loss=  1.69329 val_accuracy=  0.49609\n",
      "Epoch: 1106 val_loss=  1.64856 val_accuracy=  0.49219\n",
      "Epoch: 1107 val_loss=  1.40043 val_accuracy=  0.56641\n",
      "Epoch: 1108 val_loss=  1.50908 val_accuracy=  0.54297\n",
      "Epoch: 1109 val_loss=  1.74963 val_accuracy=  0.48047\n",
      "Epoch: 1110 val_loss=  1.45887 val_accuracy=  0.57422\n",
      "Epoch: 1111 val_loss=  1.63785 val_accuracy=  0.57812\n",
      "Epoch: 1112 val_loss=  1.57396 val_accuracy=  0.56250\n",
      "Epoch: 1113 val_loss=  1.55330 val_accuracy=  0.56250\n",
      "Epoch: 1114 val_loss=  1.69072 val_accuracy=  0.50000\n",
      "Epoch: 1115 val_loss=  1.64126 val_accuracy=  0.52734\n",
      "Epoch: 1116 val_loss=  1.48665 val_accuracy=  0.57812\n",
      "Epoch: 1117 val_loss=  1.50611 val_accuracy=  0.54688\n",
      "Epoch: 1118 val_loss=  1.76630 val_accuracy=  0.49219\n",
      "Epoch: 1119 val_loss=  1.58267 val_accuracy=  0.57031\n",
      "Epoch: 1120 val_loss=  1.71708 val_accuracy=  0.54297\n",
      "Epoch: 1121 val_loss=  1.67503 val_accuracy=  0.52344\n",
      "Epoch: 1122 val_loss=  1.56188 val_accuracy=  0.53516\n",
      "Epoch: 1123 val_loss=  1.64459 val_accuracy=  0.52344\n",
      "Epoch: 1124 val_loss=  1.34755 val_accuracy=  0.60156\n",
      "Epoch: 1125 val_loss=  1.50738 val_accuracy=  0.55469\n",
      "Epoch: 1126 val_loss=  1.43506 val_accuracy=  0.56250\n",
      "Epoch: 1127 val_loss=  1.75218 val_accuracy=  0.50000\n",
      "Epoch: 1128 val_loss=  1.64258 val_accuracy=  0.50781\n",
      "Epoch: 1129 val_loss=  1.86852 val_accuracy=  0.47656\n",
      "Epoch: 1130 val_loss=  1.59961 val_accuracy=  0.57422\n",
      "Epoch: 1131 val_loss=  1.44229 val_accuracy=  0.59375\n",
      "Epoch: 1132 val_loss=  1.72228 val_accuracy=  0.50000\n",
      "Epoch: 1133 val_loss=  1.44861 val_accuracy=  0.57812\n",
      "Epoch: 1134 val_loss=  1.57197 val_accuracy=  0.55078\n",
      "Epoch: 1135 val_loss=  1.55226 val_accuracy=  0.57031\n",
      "Epoch: 1136 val_loss=  1.67304 val_accuracy=  0.52734\n",
      "Epoch: 1137 val_loss=  1.72154 val_accuracy=  0.54688\n",
      "Epoch: 1138 val_loss=  1.49736 val_accuracy=  0.57422\n",
      "Epoch: 1139 val_loss=  1.58931 val_accuracy=  0.55078\n",
      "Epoch: 1140 val_loss=  1.50135 val_accuracy=  0.58984\n",
      "Epoch: 1141 val_loss=  1.62717 val_accuracy=  0.53906\n",
      "Epoch: 1142 val_loss=  1.51804 val_accuracy=  0.56250\n",
      "Epoch: 1143 val_loss=  1.62718 val_accuracy=  0.53125\n",
      "Epoch: 1144 val_loss=  1.64190 val_accuracy=  0.52344\n",
      "Epoch: 1145 val_loss=  1.72478 val_accuracy=  0.53125\n",
      "Epoch: 1146 val_loss=  1.68111 val_accuracy=  0.53516\n",
      "Epoch: 1147 val_loss=  1.76618 val_accuracy=  0.50391\n",
      "Epoch: 1148 val_loss=  1.70093 val_accuracy=  0.52344\n",
      "Epoch: 1149 val_loss=  1.55533 val_accuracy=  0.52734\n",
      "Epoch: 1150 val_loss=  1.41519 val_accuracy=  0.58984\n",
      "Epoch: 1151 val_loss=  1.53230 val_accuracy=  0.57031\n",
      "Epoch: 1152 val_loss=  1.54576 val_accuracy=  0.58203\n",
      "Epoch: 1153 val_loss=  1.52894 val_accuracy=  0.56250\n",
      "Epoch: 1154 val_loss=  1.64555 val_accuracy=  0.53125\n",
      "Epoch: 1155 val_loss=  1.44620 val_accuracy=  0.59766\n",
      "Epoch: 1156 val_loss=  1.63397 val_accuracy=  0.47266\n",
      "Epoch: 1157 val_loss=  1.42737 val_accuracy=  0.59375\n",
      "Epoch: 1158 val_loss=  1.57407 val_accuracy=  0.53516\n",
      "Epoch: 1159 val_loss=  1.67415 val_accuracy=  0.51953\n",
      "Epoch: 1160 val_loss=  1.53191 val_accuracy=  0.58203\n",
      "Epoch: 1161 val_loss=  1.62758 val_accuracy=  0.51953\n",
      "Epoch: 1162 val_loss=  1.74066 val_accuracy=  0.51172\n",
      "Epoch: 1163 val_loss=  1.55042 val_accuracy=  0.60156\n",
      "Epoch: 1164 val_loss=  1.67478 val_accuracy=  0.47656\n",
      "Epoch: 1165 val_loss=  1.58296 val_accuracy=  0.52344\n",
      "Epoch: 1166 val_loss=  1.39679 val_accuracy=  0.58203\n",
      "Epoch: 1167 val_loss=  1.62137 val_accuracy=  0.48047\n",
      "Epoch: 1168 val_loss=  1.54663 val_accuracy=  0.53516\n",
      "Epoch: 1169 val_loss=  1.57415 val_accuracy=  0.59375\n",
      "Epoch: 1170 val_loss=  1.63992 val_accuracy=  0.52734\n",
      "Epoch: 1171 val_loss=  1.53421 val_accuracy=  0.57422\n",
      "Epoch: 1172 val_loss=  1.68814 val_accuracy=  0.53516\n",
      "Epoch: 1173 val_loss=  1.61839 val_accuracy=  0.50391\n",
      "Epoch: 1174 val_loss=  1.65942 val_accuracy=  0.52344\n",
      "Epoch: 1175 val_loss=  1.40699 val_accuracy=  0.59375\n",
      "Epoch: 1176 val_loss=  1.65343 val_accuracy=  0.53906\n",
      "Epoch: 1177 val_loss=  1.69308 val_accuracy=  0.50391\n",
      "Epoch: 1178 val_loss=  1.57332 val_accuracy=  0.58984\n",
      "Epoch: 1179 val_loss=  1.71353 val_accuracy=  0.53516\n",
      "Epoch: 1180 val_loss=  1.60540 val_accuracy=  0.51562\n",
      "Epoch: 1181 val_loss=  1.69869 val_accuracy=  0.51562\n",
      "Epoch: 1182 val_loss=  1.46465 val_accuracy=  0.55078\n",
      "Epoch: 1183 val_loss=  1.51771 val_accuracy=  0.56641\n",
      "Epoch: 1184 val_loss=  1.38181 val_accuracy=  0.57422\n",
      "Epoch: 1185 val_loss=  1.60969 val_accuracy=  0.51953\n",
      "Epoch: 1186 val_loss=  1.67933 val_accuracy=  0.49609\n",
      "Epoch: 1187 val_loss=  1.70795 val_accuracy=  0.53516\n",
      "Epoch: 1188 val_loss=  1.83473 val_accuracy=  0.49609\n",
      "Epoch: 1189 val_loss=  1.40547 val_accuracy=  0.60156\n",
      "Epoch: 1190 val_loss=  1.71139 val_accuracy=  0.52734\n",
      "Epoch: 1191 val_loss=  1.58369 val_accuracy=  0.55859\n",
      "Epoch: 1192 val_loss=  1.39355 val_accuracy=  0.60938\n",
      "Epoch: 1193 val_loss=  1.53636 val_accuracy=  0.53516\n",
      "Epoch: 1194 val_loss=  1.68333 val_accuracy=  0.54297\n",
      "Epoch: 1195 val_loss=  1.71345 val_accuracy=  0.55078\n",
      "Epoch: 1196 val_loss=  1.65044 val_accuracy=  0.53516\n",
      "Epoch: 1197 val_loss=  1.60324 val_accuracy=  0.55078\n",
      "Epoch: 1198 val_loss=  1.50473 val_accuracy=  0.58203\n",
      "Epoch: 1199 val_loss=  1.44599 val_accuracy=  0.60547\n",
      "Epoch: 1200 val_loss=  1.69202 val_accuracy=  0.52344\n",
      "Epoch: 1201 val_loss=  1.49684 val_accuracy=  0.55078\n",
      "Epoch: 1202 val_loss=  1.63955 val_accuracy=  0.52344\n",
      "Epoch: 1203 val_loss=  1.59190 val_accuracy=  0.52344\n",
      "Epoch: 1204 val_loss=  1.75126 val_accuracy=  0.51562\n",
      "Epoch: 1205 val_loss=  1.69017 val_accuracy=  0.51172\n",
      "Epoch: 1206 val_loss=  1.76215 val_accuracy=  0.51953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1207 val_loss=  1.63409 val_accuracy=  0.53906\n",
      "Epoch: 1208 val_loss=  1.54931 val_accuracy=  0.55859\n",
      "Epoch: 1209 val_loss=  1.36886 val_accuracy=  0.56641\n",
      "Epoch: 1210 val_loss=  1.51107 val_accuracy=  0.60938\n",
      "Epoch: 1211 val_loss=  1.56468 val_accuracy=  0.54688\n",
      "Epoch: 1212 val_loss=  1.51596 val_accuracy=  0.56250\n",
      "Epoch: 1213 val_loss=  1.67213 val_accuracy=  0.53125\n",
      "Epoch: 1214 val_loss=  1.48035 val_accuracy=  0.56641\n",
      "Epoch: 1215 val_loss=  1.60395 val_accuracy=  0.50391\n",
      "Epoch: 1216 val_loss=  1.43029 val_accuracy=  0.59375\n",
      "Epoch: 1217 val_loss=  1.67842 val_accuracy=  0.52344\n",
      "Epoch: 1218 val_loss=  1.56456 val_accuracy=  0.51172\n",
      "Epoch: 1219 val_loss=  1.56623 val_accuracy=  0.56641\n",
      "Epoch: 1220 val_loss=  1.62125 val_accuracy=  0.51172\n",
      "Epoch: 1221 val_loss=  1.74542 val_accuracy=  0.52344\n",
      "Epoch: 1222 val_loss=  1.67932 val_accuracy=  0.51172\n",
      "Epoch: 1223 val_loss=  1.61420 val_accuracy=  0.50000\n",
      "Epoch: 1224 val_loss=  1.43741 val_accuracy=  0.55469\n",
      "Epoch: 1225 val_loss=  1.51094 val_accuracy=  0.53516\n",
      "Epoch: 1226 val_loss=  1.65776 val_accuracy=  0.50391\n",
      "Epoch: 1227 val_loss=  1.41150 val_accuracy=  0.57812\n",
      "Epoch: 1228 val_loss=  1.68427 val_accuracy=  0.57031\n",
      "Epoch: 1229 val_loss=  1.64614 val_accuracy=  0.54688\n",
      "Epoch: 1230 val_loss=  1.46932 val_accuracy=  0.58984\n",
      "Epoch: 1231 val_loss=  1.74240 val_accuracy=  0.48047\n",
      "Epoch: 1232 val_loss=  1.63118 val_accuracy=  0.52734\n",
      "Epoch: 1233 val_loss=  1.58282 val_accuracy=  0.55469\n",
      "Epoch: 1234 val_loss=  1.42240 val_accuracy=  0.56641\n",
      "Epoch: 1235 val_loss=  1.72643 val_accuracy=  0.52734\n",
      "Epoch: 1236 val_loss=  1.63334 val_accuracy=  0.53906\n",
      "Epoch: 1237 val_loss=  1.64850 val_accuracy=  0.53516\n",
      "Epoch: 1238 val_loss=  1.80292 val_accuracy=  0.50391\n",
      "Epoch: 1239 val_loss=  1.43581 val_accuracy=  0.56641\n",
      "Epoch: 1240 val_loss=  1.66989 val_accuracy=  0.49609\n",
      "Epoch: 1241 val_loss=  1.40088 val_accuracy=  0.58203\n",
      "Epoch: 1242 val_loss=  1.52460 val_accuracy=  0.56641\n",
      "Epoch: 1243 val_loss=  1.44874 val_accuracy=  0.57031\n",
      "Epoch: 1244 val_loss=  1.66745 val_accuracy=  0.50391\n",
      "Epoch: 1245 val_loss=  1.69993 val_accuracy=  0.50781\n",
      "Epoch: 1246 val_loss=  1.78555 val_accuracy=  0.50000\n",
      "Epoch: 1247 val_loss=  1.65394 val_accuracy=  0.54297\n",
      "Epoch: 1248 val_loss=  1.43973 val_accuracy=  0.59766\n",
      "Epoch: 1249 val_loss=  1.63512 val_accuracy=  0.53125\n",
      "Epoch: 1250 val_loss=  1.51691 val_accuracy=  0.55859\n",
      "Epoch: 1251 val_loss=  1.62185 val_accuracy=  0.54297\n",
      "Epoch: 1252 val_loss=  1.44267 val_accuracy=  0.57812\n",
      "Epoch: 1253 val_loss=  1.65543 val_accuracy=  0.54297\n",
      "Epoch: 1254 val_loss=  1.73237 val_accuracy=  0.54688\n",
      "Epoch: 1255 val_loss=  1.57670 val_accuracy=  0.55859\n",
      "Epoch: 1256 val_loss=  1.60288 val_accuracy=  0.54688\n",
      "Epoch: 1257 val_loss=  1.52777 val_accuracy=  0.58984\n",
      "Epoch: 1258 val_loss=  1.49410 val_accuracy=  0.56641\n",
      "Epoch: 1259 val_loss=  1.63160 val_accuracy=  0.52344\n",
      "Epoch: 1260 val_loss=  1.61532 val_accuracy=  0.50781\n",
      "Epoch: 1261 val_loss=  1.70118 val_accuracy=  0.51953\n",
      "Epoch: 1262 val_loss=  1.64168 val_accuracy=  0.53906\n",
      "Epoch: 1263 val_loss=  1.66584 val_accuracy=  0.52344\n",
      "Epoch: 1264 val_loss=  1.77192 val_accuracy=  0.51172\n",
      "Epoch: 1265 val_loss=  1.65208 val_accuracy=  0.53125\n",
      "Epoch: 1266 val_loss=  1.67090 val_accuracy=  0.51953\n",
      "Epoch: 1267 val_loss=  1.42875 val_accuracy=  0.57031\n",
      "Epoch: 1268 val_loss=  1.48291 val_accuracy=  0.55859\n",
      "Epoch: 1269 val_loss=  1.59051 val_accuracy=  0.56250\n",
      "Epoch: 1270 val_loss=  1.55855 val_accuracy=  0.57031\n",
      "Epoch: 1271 val_loss=  1.55198 val_accuracy=  0.55078\n",
      "Epoch: 1272 val_loss=  1.54316 val_accuracy=  0.58594\n",
      "Epoch: 1273 val_loss=  1.60407 val_accuracy=  0.51172\n",
      "Epoch: 1274 val_loss=  1.44581 val_accuracy=  0.59375\n",
      "Epoch: 1275 val_loss=  1.55495 val_accuracy=  0.53906\n",
      "Epoch: 1276 val_loss=  1.66837 val_accuracy=  0.52734\n",
      "Epoch: 1277 val_loss=  1.50722 val_accuracy=  0.58203\n",
      "Epoch: 1278 val_loss=  1.65199 val_accuracy=  0.50391\n",
      "Epoch: 1279 val_loss=  1.67700 val_accuracy=  0.52344\n",
      "Epoch: 1280 val_loss=  1.60966 val_accuracy=  0.57422\n",
      "Epoch: 1281 val_loss=  1.63040 val_accuracy=  0.50391\n",
      "Epoch: 1282 val_loss=  1.67094 val_accuracy=  0.50000\n",
      "Epoch: 1283 val_loss=  1.37099 val_accuracy=  0.57422\n",
      "Epoch: 1284 val_loss=  1.54649 val_accuracy=  0.52734\n",
      "Epoch: 1285 val_loss=  1.70024 val_accuracy=  0.48828\n",
      "Epoch: 1286 val_loss=  1.44167 val_accuracy=  0.60938\n",
      "Epoch: 1287 val_loss=  1.65181 val_accuracy=  0.55469\n",
      "Epoch: 1288 val_loss=  1.62836 val_accuracy=  0.55859\n",
      "Epoch: 1289 val_loss=  1.51870 val_accuracy=  0.55859\n",
      "Epoch: 1290 val_loss=  1.68697 val_accuracy=  0.51953\n",
      "Epoch: 1291 val_loss=  1.62550 val_accuracy=  0.51172\n",
      "Epoch: 1292 val_loss=  1.50140 val_accuracy=  0.57031\n",
      "Epoch: 1293 val_loss=  1.56251 val_accuracy=  0.54688\n",
      "Epoch: 1294 val_loss=  1.68119 val_accuracy=  0.47266\n",
      "Epoch: 1295 val_loss=  1.65393 val_accuracy=  0.56250\n",
      "Epoch: 1296 val_loss=  1.65231 val_accuracy=  0.54688\n",
      "Epoch: 1297 val_loss=  1.73071 val_accuracy=  0.50391\n",
      "Epoch: 1298 val_loss=  1.57571 val_accuracy=  0.53906\n",
      "Epoch: 1299 val_loss=  1.56697 val_accuracy=  0.51172\n",
      "Epoch: 1300 val_loss=  1.42731 val_accuracy=  0.59766\n",
      "Epoch: 1301 val_loss=  1.42164 val_accuracy=  0.57031\n",
      "Epoch: 1302 val_loss=  1.59441 val_accuracy=  0.52734\n",
      "Epoch: 1303 val_loss=  1.64850 val_accuracy=  0.51562\n",
      "Epoch: 1304 val_loss=  1.72889 val_accuracy=  0.50781\n",
      "Epoch: 1305 val_loss=  1.84450 val_accuracy=  0.50391\n",
      "Epoch: 1306 val_loss=  1.46300 val_accuracy=  0.59766\n",
      "Epoch: 1307 val_loss=  1.58975 val_accuracy=  0.55078\n",
      "Epoch: 1308 val_loss=  1.60401 val_accuracy=  0.52734\n",
      "Epoch: 1309 val_loss=  1.39748 val_accuracy=  0.60547\n",
      "Epoch: 1310 val_loss=  1.56143 val_accuracy=  0.55078\n",
      "Epoch: 1311 val_loss=  1.65264 val_accuracy=  0.54688\n",
      "Epoch: 1312 val_loss=  1.72650 val_accuracy=  0.52344\n",
      "Epoch: 1313 val_loss=  1.66411 val_accuracy=  0.54688\n",
      "Epoch: 1314 val_loss=  1.60205 val_accuracy=  0.56250\n",
      "Epoch: 1315 val_loss=  1.48012 val_accuracy=  0.55859\n",
      "Epoch: 1316 val_loss=  1.49714 val_accuracy=  0.60547\n",
      "Epoch: 1317 val_loss=  1.73337 val_accuracy=  0.50391\n",
      "Epoch: 1318 val_loss=  1.42753 val_accuracy=  0.55859\n",
      "Epoch: 1319 val_loss=  1.70143 val_accuracy=  0.50391\n",
      "Epoch: 1320 val_loss=  1.57946 val_accuracy=  0.54688\n",
      "Epoch: 1321 val_loss=  1.79115 val_accuracy=  0.50391\n",
      "Epoch: 1322 val_loss=  1.59509 val_accuracy=  0.55078\n",
      "Epoch: 1323 val_loss=  1.73558 val_accuracy=  0.53125\n",
      "Epoch: 1324 val_loss=  1.71902 val_accuracy=  0.51953\n",
      "Epoch: 1325 val_loss=  1.58129 val_accuracy=  0.51953\n",
      "Epoch: 1326 val_loss=  1.34187 val_accuracy=  0.59375\n",
      "Epoch: 1327 val_loss=  1.53985 val_accuracy=  0.57812\n",
      "Epoch: 1328 val_loss=  1.54692 val_accuracy=  0.58203\n",
      "Epoch: 1329 val_loss=  1.50417 val_accuracy=  0.57422\n",
      "Epoch: 1330 val_loss=  1.74169 val_accuracy=  0.51172\n",
      "Epoch: 1331 val_loss=  1.42447 val_accuracy=  0.60547\n",
      "Epoch: 1332 val_loss=  1.57819 val_accuracy=  0.49609\n",
      "Epoch: 1333 val_loss=  1.42064 val_accuracy=  0.61328\n",
      "Epoch: 1334 val_loss=  1.65348 val_accuracy=  0.51953\n",
      "Epoch: 1335 val_loss=  1.57542 val_accuracy=  0.53125\n",
      "Epoch: 1336 val_loss=  1.58562 val_accuracy=  0.56641\n",
      "Epoch: 1337 val_loss=  1.60867 val_accuracy=  0.52734\n",
      "Epoch: 1338 val_loss=  1.77248 val_accuracy=  0.51953\n",
      "Epoch: 1339 val_loss=  1.55326 val_accuracy=  0.57031\n",
      "Epoch: 1340 val_loss=  1.74872 val_accuracy=  0.45312\n",
      "Epoch: 1341 val_loss=  1.44511 val_accuracy=  0.56250\n",
      "Epoch: 1342 val_loss=  1.44126 val_accuracy=  0.55078\n",
      "Epoch: 1343 val_loss=  1.59403 val_accuracy=  0.53516\n",
      "Epoch: 1344 val_loss=  1.53574 val_accuracy=  0.54297\n",
      "Epoch: 1345 val_loss=  1.60599 val_accuracy=  0.58984\n",
      "Epoch: 1346 val_loss=  1.65924 val_accuracy=  0.50391\n",
      "Epoch: 1347 val_loss=  1.50619 val_accuracy=  0.60156\n",
      "Epoch: 1348 val_loss=  1.68773 val_accuracy=  0.52344\n",
      "Epoch: 1349 val_loss=  1.60349 val_accuracy=  0.50781\n",
      "Epoch: 1350 val_loss=  1.55659 val_accuracy=  0.55859\n",
      "Epoch: 1351 val_loss=  1.51668 val_accuracy=  0.55469\n",
      "Epoch: 1352 val_loss=  1.66476 val_accuracy=  0.52734\n",
      "Epoch: 1353 val_loss=  1.63786 val_accuracy=  0.53125\n",
      "Epoch: 1354 val_loss=  1.62093 val_accuracy=  0.55859\n",
      "Epoch: 1355 val_loss=  1.82052 val_accuracy=  0.49219\n",
      "Epoch: 1356 val_loss=  1.50084 val_accuracy=  0.56641\n",
      "Epoch: 1357 val_loss=  1.70456 val_accuracy=  0.50781\n",
      "Epoch: 1358 val_loss=  1.43747 val_accuracy=  0.56641\n",
      "Epoch: 1359 val_loss=  1.45504 val_accuracy=  0.57812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1360 val_loss=  1.42438 val_accuracy=  0.58594\n",
      "Epoch: 1361 val_loss=  1.79986 val_accuracy=  0.44922\n",
      "Epoch: 1362 val_loss=  1.57761 val_accuracy=  0.55078\n",
      "Epoch: 1363 val_loss=  1.79434 val_accuracy=  0.50781\n",
      "Epoch: 1364 val_loss=  1.65551 val_accuracy=  0.54297\n",
      "Epoch: 1365 val_loss=  1.39645 val_accuracy=  0.60938\n",
      "Epoch: 1366 val_loss=  1.72484 val_accuracy=  0.52344\n",
      "Epoch: 1367 val_loss=  1.54833 val_accuracy=  0.57422\n",
      "Epoch: 1368 val_loss=  1.51669 val_accuracy=  0.56641\n",
      "Epoch: 1369 val_loss=  1.53839 val_accuracy=  0.57031\n",
      "Epoch: 1370 val_loss=  1.63503 val_accuracy=  0.52344\n",
      "Epoch: 1371 val_loss=  1.65882 val_accuracy=  0.58594\n",
      "Epoch: 1372 val_loss=  1.67508 val_accuracy=  0.53906\n",
      "Epoch: 1373 val_loss=  1.59873 val_accuracy=  0.56641\n",
      "Epoch: 1374 val_loss=  1.54638 val_accuracy=  0.58594\n",
      "Epoch: 1375 val_loss=  1.39959 val_accuracy=  0.61328\n",
      "Epoch: 1376 val_loss=  1.69207 val_accuracy=  0.51172\n",
      "Epoch: 1377 val_loss=  1.51502 val_accuracy=  0.55469\n",
      "Epoch: 1378 val_loss=  1.71818 val_accuracy=  0.51953\n",
      "Epoch: 1379 val_loss=  1.64400 val_accuracy=  0.53516\n",
      "Epoch: 1380 val_loss=  1.61859 val_accuracy=  0.53516\n",
      "Epoch: 1381 val_loss=  1.76479 val_accuracy=  0.48828\n",
      "Epoch: 1382 val_loss=  1.69622 val_accuracy=  0.55078\n",
      "Epoch: 1383 val_loss=  1.67867 val_accuracy=  0.50391\n",
      "Epoch: 1384 val_loss=  1.47990 val_accuracy=  0.56641\n",
      "Epoch: 1385 val_loss=  1.45579 val_accuracy=  0.56641\n",
      "Epoch: 1386 val_loss=  1.53420 val_accuracy=  0.57031\n",
      "Epoch: 1387 val_loss=  1.58060 val_accuracy=  0.57422\n",
      "Epoch: 1388 val_loss=  1.54662 val_accuracy=  0.54688\n",
      "Epoch: 1389 val_loss=  1.60131 val_accuracy=  0.56641\n",
      "Epoch: 1390 val_loss=  1.49558 val_accuracy=  0.54688\n",
      "Epoch: 1391 val_loss=  1.53323 val_accuracy=  0.53516\n",
      "Epoch: 1392 val_loss=  1.53503 val_accuracy=  0.53125\n",
      "Epoch: 1393 val_loss=  1.62530 val_accuracy=  0.56250\n",
      "Epoch: 1394 val_loss=  1.58352 val_accuracy=  0.53516\n",
      "Epoch: 1395 val_loss=  1.57193 val_accuracy=  0.53906\n",
      "Epoch: 1396 val_loss=  1.71569 val_accuracy=  0.50391\n",
      "Epoch: 1397 val_loss=  1.63560 val_accuracy=  0.55078\n",
      "Epoch: 1398 val_loss=  1.62433 val_accuracy=  0.51562\n",
      "Epoch: 1399 val_loss=  1.70292 val_accuracy=  0.49219\n",
      "Epoch: 1400 val_loss=  1.36384 val_accuracy=  0.57422\n",
      "Epoch: 1401 val_loss=  1.50359 val_accuracy=  0.54297\n",
      "Epoch: 1402 val_loss=  1.74506 val_accuracy=  0.47656\n",
      "Epoch: 1403 val_loss=  1.44580 val_accuracy=  0.58984\n",
      "Epoch: 1404 val_loss=  1.67950 val_accuracy=  0.57031\n",
      "Epoch: 1405 val_loss=  1.53533 val_accuracy=  0.57422\n",
      "Epoch: 1406 val_loss=  1.55989 val_accuracy=  0.55469\n",
      "Epoch: 1407 val_loss=  1.69791 val_accuracy=  0.49609\n",
      "Epoch: 1408 val_loss=  1.63736 val_accuracy=  0.52734\n",
      "Epoch: 1409 val_loss=  1.48839 val_accuracy=  0.58984\n",
      "Epoch: 1410 val_loss=  1.53339 val_accuracy=  0.53516\n",
      "Epoch: 1411 val_loss=  1.74769 val_accuracy=  0.50000\n",
      "Epoch: 1412 val_loss=  1.59416 val_accuracy=  0.56641\n",
      "Epoch: 1413 val_loss=  1.71007 val_accuracy=  0.53906\n",
      "Epoch: 1414 val_loss=  1.69859 val_accuracy=  0.51953\n",
      "Epoch: 1415 val_loss=  1.53140 val_accuracy=  0.54297\n",
      "Epoch: 1416 val_loss=  1.67298 val_accuracy=  0.50000\n",
      "Epoch: 1417 val_loss=  1.35555 val_accuracy=  0.59766\n",
      "Epoch: 1418 val_loss=  1.48077 val_accuracy=  0.55469\n",
      "Epoch: 1419 val_loss=  1.44022 val_accuracy=  0.56641\n",
      "Epoch: 1420 val_loss=  1.74693 val_accuracy=  0.49219\n",
      "Epoch: 1421 val_loss=  1.65587 val_accuracy=  0.50781\n",
      "Epoch: 1422 val_loss=  1.83946 val_accuracy=  0.49219\n",
      "Epoch: 1423 val_loss=  1.62826 val_accuracy=  0.56250\n",
      "Epoch: 1424 val_loss=  1.44861 val_accuracy=  0.59766\n",
      "Epoch: 1425 val_loss=  1.71720 val_accuracy=  0.50000\n",
      "Epoch: 1426 val_loss=  1.41159 val_accuracy=  0.60547\n",
      "Epoch: 1427 val_loss=  1.57153 val_accuracy=  0.53906\n",
      "Epoch: 1428 val_loss=  1.62671 val_accuracy=  0.55859\n",
      "Epoch: 1429 val_loss=  1.69061 val_accuracy=  0.52344\n",
      "Epoch: 1430 val_loss=  1.65185 val_accuracy=  0.56250\n",
      "Epoch: 1431 val_loss=  1.52023 val_accuracy=  0.57031\n",
      "Epoch: 1432 val_loss=  1.58625 val_accuracy=  0.55078\n",
      "Epoch: 1433 val_loss=  1.49059 val_accuracy=  0.60938\n",
      "Epoch: 1434 val_loss=  1.63531 val_accuracy=  0.53516\n",
      "Epoch: 1435 val_loss=  1.50632 val_accuracy=  0.57031\n",
      "Epoch: 1436 val_loss=  1.64719 val_accuracy=  0.53125\n",
      "Epoch: 1437 val_loss=  1.63358 val_accuracy=  0.51953\n",
      "Epoch: 1438 val_loss=  1.69769 val_accuracy=  0.53125\n",
      "Epoch: 1439 val_loss=  1.68762 val_accuracy=  0.53516\n",
      "Epoch: 1440 val_loss=  1.78503 val_accuracy=  0.50391\n",
      "Epoch: 1441 val_loss=  1.72265 val_accuracy=  0.52734\n",
      "Epoch: 1442 val_loss=  1.54865 val_accuracy=  0.53516\n",
      "Epoch: 1443 val_loss=  1.38979 val_accuracy=  0.59375\n",
      "Epoch: 1444 val_loss=  1.55054 val_accuracy=  0.56250\n",
      "Epoch: 1445 val_loss=  1.57962 val_accuracy=  0.57812\n",
      "Epoch: 1446 val_loss=  1.47873 val_accuracy=  0.57422\n",
      "Epoch: 1447 val_loss=  1.70092 val_accuracy=  0.52344\n",
      "Epoch: 1448 val_loss=  1.42585 val_accuracy=  0.59375\n",
      "Epoch: 1449 val_loss=  1.63424 val_accuracy=  0.48828\n",
      "Epoch: 1450 val_loss=  1.38049 val_accuracy=  0.61719\n",
      "Epoch: 1451 val_loss=  1.64350 val_accuracy=  0.51562\n",
      "Epoch: 1452 val_loss=  1.65878 val_accuracy=  0.51953\n",
      "Epoch: 1453 val_loss=  1.51857 val_accuracy=  0.58594\n",
      "Epoch: 1454 val_loss=  1.64002 val_accuracy=  0.50391\n",
      "Epoch: 1455 val_loss=  1.74138 val_accuracy=  0.51953\n",
      "Epoch: 1456 val_loss=  1.55012 val_accuracy=  0.57031\n",
      "Epoch: 1457 val_loss=  1.68428 val_accuracy=  0.47656\n",
      "Epoch: 1458 val_loss=  1.58143 val_accuracy=  0.51953\n",
      "Epoch: 1459 val_loss=  1.40194 val_accuracy=  0.57812\n",
      "Epoch: 1460 val_loss=  1.63666 val_accuracy=  0.48438\n",
      "Epoch: 1461 val_loss=  1.51044 val_accuracy=  0.54297\n",
      "Epoch: 1462 val_loss=  1.55342 val_accuracy=  0.60156\n",
      "Epoch: 1463 val_loss=  1.66500 val_accuracy=  0.52344\n",
      "Epoch: 1464 val_loss=  1.57320 val_accuracy=  0.56641\n",
      "Epoch: 1465 val_loss=  1.66880 val_accuracy=  0.53125\n",
      "Epoch: 1466 val_loss=  1.58172 val_accuracy=  0.51953\n",
      "Epoch: 1467 val_loss=  1.65012 val_accuracy=  0.53125\n",
      "Epoch: 1468 val_loss=  1.43412 val_accuracy=  0.59375\n",
      "Epoch: 1469 val_loss=  1.63001 val_accuracy=  0.53906\n",
      "Epoch: 1470 val_loss=  1.70187 val_accuracy=  0.50781\n",
      "Epoch: 1471 val_loss=  1.56558 val_accuracy=  0.58984\n",
      "Epoch: 1472 val_loss=  1.73241 val_accuracy=  0.53125\n",
      "Epoch: 1473 val_loss=  1.61005 val_accuracy=  0.52734\n",
      "Epoch: 1474 val_loss=  1.70985 val_accuracy=  0.51562\n",
      "Epoch: 1475 val_loss=  1.44034 val_accuracy=  0.55859\n",
      "Epoch: 1476 val_loss=  1.49730 val_accuracy=  0.57422\n",
      "Epoch: 1477 val_loss=  1.38923 val_accuracy=  0.57812\n",
      "Epoch: 1478 val_loss=  1.65418 val_accuracy=  0.50781\n",
      "Epoch: 1479 val_loss=  1.66517 val_accuracy=  0.49609\n",
      "Epoch: 1480 val_loss=  1.69380 val_accuracy=  0.52734\n",
      "Epoch: 1481 val_loss=  1.81924 val_accuracy=  0.50391\n",
      "Epoch: 1482 val_loss=  1.38637 val_accuracy=  0.60938\n",
      "Epoch: 1483 val_loss=  1.75849 val_accuracy=  0.51562\n",
      "Epoch: 1484 val_loss=  1.54371 val_accuracy=  0.57422\n",
      "Epoch: 1485 val_loss=  1.40424 val_accuracy=  0.60547\n",
      "Epoch: 1486 val_loss=  1.52147 val_accuracy=  0.54688\n",
      "Epoch: 1487 val_loss=  1.70400 val_accuracy=  0.54297\n",
      "Epoch: 1488 val_loss=  1.71877 val_accuracy=  0.54688\n",
      "Epoch: 1489 val_loss=  1.62893 val_accuracy=  0.54297\n",
      "Epoch: 1490 val_loss=  1.58199 val_accuracy=  0.56250\n",
      "Epoch: 1491 val_loss=  1.55758 val_accuracy=  0.56250\n",
      "Epoch: 1492 val_loss=  1.40172 val_accuracy=  0.61328\n",
      "Epoch: 1493 val_loss=  1.69345 val_accuracy=  0.52344\n",
      "Epoch: 1494 val_loss=  1.53688 val_accuracy=  0.54688\n",
      "Epoch: 1495 val_loss=  1.65114 val_accuracy=  0.52344\n",
      "Epoch: 1496 val_loss=  1.57718 val_accuracy=  0.52344\n",
      "Epoch: 1497 val_loss=  1.74963 val_accuracy=  0.51953\n",
      "Epoch: 1498 val_loss=  1.70367 val_accuracy=  0.50781\n",
      "Epoch: 1499 val_loss=  1.77565 val_accuracy=  0.51172\n",
      "Epoch: 1500 val_loss=  1.59964 val_accuracy=  0.55469\n",
      "Epoch: 1501 val_loss=  1.53275 val_accuracy=  0.55859\n",
      "Epoch: 1502 val_loss=  1.42625 val_accuracy=  0.55078\n",
      "Epoch: 1503 val_loss=  1.46841 val_accuracy=  0.62500\n",
      "Epoch: 1504 val_loss=  1.59859 val_accuracy=  0.53516\n",
      "Epoch: 1505 val_loss=  1.52538 val_accuracy=  0.55859\n",
      "Epoch: 1506 val_loss=  1.68250 val_accuracy=  0.53125\n",
      "Epoch: 1507 val_loss=  1.45022 val_accuracy=  0.56641\n",
      "Epoch: 1508 val_loss=  1.65313 val_accuracy=  0.50391\n",
      "Epoch: 1509 val_loss=  1.41191 val_accuracy=  0.58594\n",
      "Epoch: 1510 val_loss=  1.65284 val_accuracy=  0.53125\n",
      "Epoch: 1511 val_loss=  1.55978 val_accuracy=  0.51172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1512 val_loss=  1.59469 val_accuracy=  0.55469\n",
      "Epoch: 1513 val_loss=  1.65748 val_accuracy=  0.49219\n",
      "Epoch: 1514 val_loss=  1.70605 val_accuracy=  0.54297\n",
      "Epoch: 1515 val_loss=  1.68139 val_accuracy=  0.50781\n",
      "Epoch: 1516 val_loss=  1.62507 val_accuracy=  0.50391\n",
      "Epoch: 1517 val_loss=  1.40890 val_accuracy=  0.55469\n",
      "Epoch: 1518 val_loss=  1.51357 val_accuracy=  0.53906\n",
      "Epoch: 1519 val_loss=  1.71338 val_accuracy=  0.48828\n",
      "Epoch: 1520 val_loss=  1.38348 val_accuracy=  0.57812\n",
      "Epoch: 1521 val_loss=  1.67959 val_accuracy=  0.56641\n",
      "Epoch: 1522 val_loss=  1.64900 val_accuracy=  0.52734\n",
      "Epoch: 1523 val_loss=  1.47982 val_accuracy=  0.60547\n",
      "Epoch: 1524 val_loss=  1.75206 val_accuracy=  0.48438\n",
      "Epoch: 1525 val_loss=  1.59740 val_accuracy=  0.54297\n",
      "Epoch: 1526 val_loss=  1.58459 val_accuracy=  0.55469\n",
      "Epoch: 1527 val_loss=  1.43927 val_accuracy=  0.57422\n",
      "Epoch: 1528 val_loss=  1.72886 val_accuracy=  0.53906\n",
      "Epoch: 1529 val_loss=  1.63233 val_accuracy=  0.53906\n",
      "Epoch: 1530 val_loss=  1.64011 val_accuracy=  0.53516\n",
      "Epoch: 1531 val_loss=  1.80571 val_accuracy=  0.49609\n",
      "Epoch: 1532 val_loss=  1.46450 val_accuracy=  0.55859\n",
      "Epoch: 1533 val_loss=  1.66355 val_accuracy=  0.51172\n",
      "Epoch: 1534 val_loss=  1.39996 val_accuracy=  0.58984\n",
      "Epoch: 1535 val_loss=  1.51083 val_accuracy=  0.57422\n",
      "Epoch: 1536 val_loss=  1.44130 val_accuracy=  0.56641\n",
      "Epoch: 1537 val_loss=  1.67397 val_accuracy=  0.51172\n",
      "Epoch: 1538 val_loss=  1.68675 val_accuracy=  0.51562\n",
      "Epoch: 1539 val_loss=  1.83140 val_accuracy=  0.49609\n",
      "Epoch: 1540 val_loss=  1.62198 val_accuracy=  0.55859\n",
      "Epoch: 1541 val_loss=  1.43280 val_accuracy=  0.60156\n",
      "Epoch: 1542 val_loss=  1.61276 val_accuracy=  0.53516\n",
      "Epoch: 1543 val_loss=  1.53140 val_accuracy=  0.56250\n",
      "Epoch: 1544 val_loss=  1.60842 val_accuracy=  0.54688\n",
      "Epoch: 1545 val_loss=  1.51419 val_accuracy=  0.56641\n",
      "Epoch: 1546 val_loss=  1.61950 val_accuracy=  0.54297\n",
      "Epoch: 1547 val_loss=  1.71122 val_accuracy=  0.56250\n",
      "Epoch: 1548 val_loss=  1.56296 val_accuracy=  0.55469\n",
      "Epoch: 1549 val_loss=  1.61896 val_accuracy=  0.55078\n",
      "Epoch: 1550 val_loss=  1.54006 val_accuracy=  0.59766\n",
      "Epoch: 1551 val_loss=  1.50431 val_accuracy=  0.56641\n",
      "Epoch: 1552 val_loss=  1.59656 val_accuracy=  0.53125\n",
      "Epoch: 1553 val_loss=  1.63127 val_accuracy=  0.52734\n",
      "Epoch: 1554 val_loss=  1.68944 val_accuracy=  0.52344\n",
      "Epoch: 1555 val_loss=  1.67218 val_accuracy=  0.54297\n",
      "Epoch: 1556 val_loss=  1.65287 val_accuracy=  0.51562\n",
      "Epoch: 1557 val_loss=  1.75222 val_accuracy=  0.51953\n",
      "Epoch: 1558 val_loss=  1.66146 val_accuracy=  0.53516\n",
      "Epoch: 1559 val_loss=  1.65428 val_accuracy=  0.51953\n",
      "Epoch: 1560 val_loss=  1.44489 val_accuracy=  0.57422\n",
      "Epoch: 1561 val_loss=  1.46473 val_accuracy=  0.55859\n",
      "Epoch: 1562 val_loss=  1.57511 val_accuracy=  0.57031\n",
      "Epoch: 1563 val_loss=  1.58124 val_accuracy=  0.55078\n",
      "Epoch: 1564 val_loss=  1.58639 val_accuracy=  0.55078\n",
      "Epoch: 1565 val_loss=  1.48191 val_accuracy=  0.58984\n",
      "Epoch: 1566 val_loss=  1.63591 val_accuracy=  0.50781\n",
      "Epoch: 1567 val_loss=  1.43987 val_accuracy=  0.58594\n",
      "Epoch: 1568 val_loss=  1.54385 val_accuracy=  0.55078\n",
      "Epoch: 1569 val_loss=  1.67162 val_accuracy=  0.51953\n",
      "Epoch: 1570 val_loss=  1.53349 val_accuracy=  0.58984\n",
      "Epoch: 1571 val_loss=  1.61816 val_accuracy=  0.51562\n",
      "Epoch: 1572 val_loss=  1.68738 val_accuracy=  0.50391\n",
      "Epoch: 1573 val_loss=  1.62139 val_accuracy=  0.58203\n",
      "Epoch: 1574 val_loss=  1.61764 val_accuracy=  0.49219\n",
      "Epoch: 1575 val_loss=  1.67445 val_accuracy=  0.51172\n",
      "Epoch: 1576 val_loss=  1.34804 val_accuracy=  0.57812\n",
      "Epoch: 1577 val_loss=  1.58374 val_accuracy=  0.51953\n",
      "Epoch: 1578 val_loss=  1.66861 val_accuracy=  0.50000\n",
      "Epoch: 1579 val_loss=  1.48233 val_accuracy=  0.60938\n",
      "Epoch: 1580 val_loss=  1.60713 val_accuracy=  0.56250\n",
      "Epoch: 1581 val_loss=  1.63625 val_accuracy=  0.54688\n",
      "Epoch: 1582 val_loss=  1.57447 val_accuracy=  0.55469\n",
      "Epoch: 1583 val_loss=  1.65100 val_accuracy=  0.51172\n",
      "Epoch: 1584 val_loss=  1.62697 val_accuracy=  0.51953\n",
      "Epoch: 1585 val_loss=  1.50157 val_accuracy=  0.57031\n",
      "Epoch: 1586 val_loss=  1.54406 val_accuracy=  0.56250\n",
      "Epoch: 1587 val_loss=  1.69719 val_accuracy=  0.48047\n",
      "Epoch: 1588 val_loss=  1.66871 val_accuracy=  0.55859\n",
      "Epoch: 1589 val_loss=  1.64292 val_accuracy=  0.54297\n",
      "Epoch: 1590 val_loss=  1.68923 val_accuracy=  0.50781\n",
      "Epoch: 1591 val_loss=  1.62225 val_accuracy=  0.52734\n",
      "Epoch: 1592 val_loss=  1.52293 val_accuracy=  0.53125\n",
      "Epoch: 1593 val_loss=  1.46266 val_accuracy=  0.58203\n",
      "Epoch: 1594 val_loss=  1.39771 val_accuracy=  0.56250\n",
      "Epoch: 1595 val_loss=  1.57732 val_accuracy=  0.54297\n",
      "Epoch: 1596 val_loss=  1.65724 val_accuracy=  0.51562\n",
      "Epoch: 1597 val_loss=  1.74679 val_accuracy=  0.51562\n",
      "Epoch: 1598 val_loss=  1.83314 val_accuracy=  0.50391\n",
      "Epoch: 1599 val_loss=  1.46021 val_accuracy=  0.58594\n",
      "Epoch: 1600 val_loss=  1.60752 val_accuracy=  0.55859\n",
      "Epoch: 1601 val_loss=  1.59991 val_accuracy=  0.51953\n",
      "Epoch: 1602 val_loss=  1.35807 val_accuracy=  0.62500\n",
      "Epoch: 1603 val_loss=  1.57007 val_accuracy=  0.55078\n",
      "Epoch: 1604 val_loss=  1.64257 val_accuracy=  0.53125\n",
      "Epoch: 1605 val_loss=  1.73347 val_accuracy=  0.52734\n",
      "Epoch: 1606 val_loss=  1.64636 val_accuracy=  0.55469\n",
      "Epoch: 1607 val_loss=  1.63771 val_accuracy=  0.55078\n",
      "Epoch: 1608 val_loss=  1.47187 val_accuracy=  0.57031\n",
      "Epoch: 1609 val_loss=  1.47816 val_accuracy=  0.60547\n",
      "Epoch: 1610 val_loss=  1.75673 val_accuracy=  0.49219\n",
      "Epoch: 1611 val_loss=  1.41572 val_accuracy=  0.57422\n",
      "Epoch: 1612 val_loss=  1.71029 val_accuracy=  0.50781\n",
      "Epoch: 1613 val_loss=  1.56718 val_accuracy=  0.53906\n",
      "Epoch: 1614 val_loss=  1.79365 val_accuracy=  0.50391\n",
      "Epoch: 1615 val_loss=  1.59212 val_accuracy=  0.54688\n",
      "Epoch: 1616 val_loss=  1.77847 val_accuracy=  0.50781\n",
      "Epoch: 1617 val_loss=  1.71838 val_accuracy=  0.53125\n",
      "Epoch: 1618 val_loss=  1.57711 val_accuracy=  0.52344\n",
      "Epoch: 1619 val_loss=  1.35048 val_accuracy=  0.60156\n",
      "Epoch: 1620 val_loss=  1.50543 val_accuracy=  0.58984\n",
      "Epoch: 1621 val_loss=  1.54817 val_accuracy=  0.57422\n",
      "Epoch: 1622 val_loss=  1.51270 val_accuracy=  0.56250\n",
      "Epoch: 1623 val_loss=  1.71552 val_accuracy=  0.51953\n",
      "Epoch: 1624 val_loss=  1.43344 val_accuracy=  0.60547\n",
      "Epoch: 1625 val_loss=  1.57035 val_accuracy=  0.49219\n",
      "Epoch: 1626 val_loss=  1.43005 val_accuracy=  0.60938\n",
      "Epoch: 1627 val_loss=  1.67531 val_accuracy=  0.52344\n",
      "Epoch: 1628 val_loss=  1.57868 val_accuracy=  0.53125\n",
      "Epoch: 1629 val_loss=  1.55334 val_accuracy=  0.56641\n",
      "Epoch: 1630 val_loss=  1.63238 val_accuracy=  0.51953\n",
      "Epoch: 1631 val_loss=  1.75093 val_accuracy=  0.52734\n",
      "Epoch: 1632 val_loss=  1.56144 val_accuracy=  0.55469\n",
      "Epoch: 1633 val_loss=  1.70884 val_accuracy=  0.46875\n",
      "Epoch: 1634 val_loss=  1.47854 val_accuracy=  0.55469\n",
      "Epoch: 1635 val_loss=  1.43532 val_accuracy=  0.54688\n",
      "Epoch: 1636 val_loss=  1.61939 val_accuracy=  0.51562\n",
      "Epoch: 1637 val_loss=  1.49942 val_accuracy=  0.55078\n",
      "Epoch: 1638 val_loss=  1.62461 val_accuracy=  0.58984\n",
      "Epoch: 1639 val_loss=  1.68737 val_accuracy=  0.51172\n",
      "Epoch: 1640 val_loss=  1.50586 val_accuracy=  0.59375\n",
      "Epoch: 1641 val_loss=  1.67276 val_accuracy=  0.51562\n",
      "Epoch: 1642 val_loss=  1.55838 val_accuracy=  0.52734\n",
      "Epoch: 1643 val_loss=  1.63060 val_accuracy=  0.55078\n",
      "Epoch: 1644 val_loss=  1.47540 val_accuracy=  0.55078\n",
      "Epoch: 1645 val_loss=  1.70095 val_accuracy=  0.52344\n",
      "Epoch: 1646 val_loss=  1.61049 val_accuracy=  0.54297\n",
      "Epoch: 1647 val_loss=  1.62917 val_accuracy=  0.57031\n",
      "Epoch: 1648 val_loss=  1.80279 val_accuracy=  0.49609\n",
      "Epoch: 1649 val_loss=  1.51931 val_accuracy=  0.55078\n",
      "Epoch: 1650 val_loss=  1.67607 val_accuracy=  0.51953\n",
      "Epoch: 1651 val_loss=  1.45016 val_accuracy=  0.56641\n",
      "Epoch: 1652 val_loss=  1.47541 val_accuracy=  0.57031\n",
      "Epoch: 1653 val_loss=  1.39765 val_accuracy=  0.60547\n",
      "Epoch: 1654 val_loss=  1.80938 val_accuracy=  0.44531\n",
      "Epoch: 1655 val_loss=  1.59640 val_accuracy=  0.54688\n",
      "Epoch: 1656 val_loss=  1.73668 val_accuracy=  0.51172\n",
      "Epoch: 1657 val_loss=  1.70280 val_accuracy=  0.53516\n",
      "Epoch: 1658 val_loss=  1.38994 val_accuracy=  0.60156\n",
      "Epoch: 1659 val_loss=  1.71539 val_accuracy=  0.53125\n",
      "Epoch: 1660 val_loss=  1.55890 val_accuracy=  0.57031\n",
      "Epoch: 1661 val_loss=  1.50490 val_accuracy=  0.57812\n",
      "Epoch: 1662 val_loss=  1.55379 val_accuracy=  0.55469\n",
      "Epoch: 1663 val_loss=  1.63894 val_accuracy=  0.52734\n",
      "Epoch: 1664 val_loss=  1.68524 val_accuracy=  0.58203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1665 val_loss=  1.66193 val_accuracy=  0.53906\n",
      "Epoch: 1666 val_loss=  1.56717 val_accuracy=  0.55469\n",
      "Epoch: 1667 val_loss=  1.53867 val_accuracy=  0.58984\n",
      "Epoch: 1668 val_loss=  1.40325 val_accuracy=  0.60938\n",
      "Epoch: 1669 val_loss=  1.67634 val_accuracy=  0.51953\n",
      "Epoch: 1670 val_loss=  1.52692 val_accuracy=  0.55469\n",
      "Epoch: 1671 val_loss=  1.69392 val_accuracy=  0.51562\n",
      "Epoch: 1672 val_loss=  1.68407 val_accuracy=  0.52344\n",
      "Epoch: 1673 val_loss=  1.63639 val_accuracy=  0.52734\n",
      "Epoch: 1674 val_loss=  1.73276 val_accuracy=  0.50781\n",
      "Epoch: 1675 val_loss=  1.70360 val_accuracy=  0.53906\n",
      "Epoch: 1676 val_loss=  1.66077 val_accuracy=  0.51172\n",
      "Epoch: 1677 val_loss=  1.48536 val_accuracy=  0.55859\n",
      "Epoch: 1678 val_loss=  1.44299 val_accuracy=  0.57031\n",
      "Epoch: 1679 val_loss=  1.55170 val_accuracy=  0.57031\n",
      "Epoch: 1680 val_loss=  1.57810 val_accuracy=  0.57812\n",
      "Epoch: 1681 val_loss=  1.52090 val_accuracy=  0.55469\n",
      "Epoch: 1682 val_loss=  1.62459 val_accuracy=  0.55469\n",
      "Epoch: 1683 val_loss=  1.50968 val_accuracy=  0.54297\n",
      "Epoch: 1684 val_loss=  1.50565 val_accuracy=  0.55078\n",
      "Epoch: 1685 val_loss=  1.59453 val_accuracy=  0.51953\n",
      "Epoch: 1686 val_loss=  1.57508 val_accuracy=  0.56641\n",
      "Epoch: 1687 val_loss=  1.55788 val_accuracy=  0.53906\n",
      "Epoch: 1688 val_loss=  1.59852 val_accuracy=  0.53125\n",
      "Epoch: 1689 val_loss=  1.69306 val_accuracy=  0.50000\n",
      "Epoch: 1690 val_loss=  1.64476 val_accuracy=  0.55078\n",
      "Epoch: 1691 val_loss=  1.64812 val_accuracy=  0.49609\n",
      "Epoch: 1692 val_loss=  1.67907 val_accuracy=  0.49609\n",
      "Epoch: 1693 val_loss=  1.40410 val_accuracy=  0.55859\n",
      "Epoch: 1694 val_loss=  1.51169 val_accuracy=  0.53516\n",
      "Epoch: 1695 val_loss=  1.70004 val_accuracy=  0.49219\n",
      "Epoch: 1696 val_loss=  1.44891 val_accuracy=  0.59375\n",
      "Epoch: 1697 val_loss=  1.69595 val_accuracy=  0.55859\n",
      "Epoch: 1698 val_loss=  1.51673 val_accuracy=  0.58984\n",
      "Epoch: 1699 val_loss=  1.57406 val_accuracy=  0.55469\n",
      "Epoch: 1700 val_loss=  1.72367 val_accuracy=  0.49219\n",
      "Epoch: 1701 val_loss=  1.62405 val_accuracy=  0.52734\n",
      "Epoch: 1702 val_loss=  1.47244 val_accuracy=  0.59766\n",
      "Epoch: 1703 val_loss=  1.53264 val_accuracy=  0.54688\n",
      "Epoch: 1704 val_loss=  1.73749 val_accuracy=  0.49219\n",
      "Epoch: 1705 val_loss=  1.58147 val_accuracy=  0.56250\n",
      "Epoch: 1706 val_loss=  1.73320 val_accuracy=  0.53516\n",
      "Epoch: 1707 val_loss=  1.69549 val_accuracy=  0.52344\n",
      "Epoch: 1708 val_loss=  1.55600 val_accuracy=  0.53906\n",
      "Epoch: 1709 val_loss=  1.65096 val_accuracy=  0.51172\n",
      "Epoch: 1710 val_loss=  1.36761 val_accuracy=  0.60156\n",
      "Epoch: 1711 val_loss=  1.48813 val_accuracy=  0.57031\n",
      "Epoch: 1712 val_loss=  1.46730 val_accuracy=  0.56250\n",
      "Epoch: 1713 val_loss=  1.70953 val_accuracy=  0.50781\n",
      "Epoch: 1714 val_loss=  1.66705 val_accuracy=  0.50391\n",
      "Epoch: 1715 val_loss=  1.85150 val_accuracy=  0.48828\n",
      "Epoch: 1716 val_loss=  1.61961 val_accuracy=  0.56641\n",
      "Epoch: 1717 val_loss=  1.44258 val_accuracy=  0.60938\n",
      "Epoch: 1718 val_loss=  1.72700 val_accuracy=  0.48828\n",
      "Epoch: 1719 val_loss=  1.39466 val_accuracy=  0.61719\n",
      "Epoch: 1720 val_loss=  1.56571 val_accuracy=  0.54688\n",
      "Epoch: 1721 val_loss=  1.64592 val_accuracy=  0.54688\n",
      "Epoch: 1722 val_loss=  1.69050 val_accuracy=  0.52344\n",
      "Epoch: 1723 val_loss=  1.67878 val_accuracy=  0.55078\n",
      "Epoch: 1724 val_loss=  1.51243 val_accuracy=  0.57422\n",
      "Epoch: 1725 val_loss=  1.55808 val_accuracy=  0.55859\n",
      "Epoch: 1726 val_loss=  1.52316 val_accuracy=  0.59766\n",
      "Epoch: 1727 val_loss=  1.62200 val_accuracy=  0.53906\n",
      "Epoch: 1728 val_loss=  1.51104 val_accuracy=  0.57812\n",
      "Epoch: 1729 val_loss=  1.67040 val_accuracy=  0.51953\n",
      "Epoch: 1730 val_loss=  1.60968 val_accuracy=  0.52344\n",
      "Epoch: 1731 val_loss=  1.75085 val_accuracy=  0.52734\n",
      "Epoch: 1732 val_loss=  1.62433 val_accuracy=  0.55078\n",
      "Epoch: 1733 val_loss=  1.78218 val_accuracy=  0.50391\n",
      "Epoch: 1734 val_loss=  1.74376 val_accuracy=  0.51172\n",
      "Epoch: 1735 val_loss=  1.49814 val_accuracy=  0.54297\n",
      "Epoch: 1736 val_loss=  1.39579 val_accuracy=  0.59375\n",
      "Epoch: 1737 val_loss=  1.57577 val_accuracy=  0.55859\n",
      "Epoch: 1738 val_loss=  1.58923 val_accuracy=  0.56250\n",
      "Epoch: 1739 val_loss=  1.46283 val_accuracy=  0.58203\n",
      "Epoch: 1740 val_loss=  1.69651 val_accuracy=  0.52344\n",
      "Epoch: 1741 val_loss=  1.42199 val_accuracy=  0.59375\n",
      "Epoch: 1742 val_loss=  1.61516 val_accuracy=  0.50391\n",
      "Epoch: 1743 val_loss=  1.39311 val_accuracy=  0.60938\n",
      "Epoch: 1744 val_loss=  1.67854 val_accuracy=  0.50391\n",
      "Epoch: 1745 val_loss=  1.63933 val_accuracy=  0.53125\n",
      "Epoch: 1746 val_loss=  1.51623 val_accuracy=  0.58203\n",
      "Epoch: 1747 val_loss=  1.61027 val_accuracy=  0.50781\n",
      "Epoch: 1748 val_loss=  1.76087 val_accuracy=  0.51172\n",
      "Epoch: 1749 val_loss=  1.55057 val_accuracy=  0.57812\n",
      "Epoch: 1750 val_loss=  1.72513 val_accuracy=  0.47266\n",
      "Epoch: 1751 val_loss=  1.53352 val_accuracy=  0.53125\n",
      "Epoch: 1752 val_loss=  1.38929 val_accuracy=  0.58203\n",
      "Epoch: 1753 val_loss=  1.66204 val_accuracy=  0.48828\n",
      "Epoch: 1754 val_loss=  1.49099 val_accuracy=  0.54688\n",
      "Epoch: 1755 val_loss=  1.55740 val_accuracy=  0.59766\n",
      "Epoch: 1756 val_loss=  1.68194 val_accuracy=  0.51562\n",
      "Epoch: 1757 val_loss=  1.53937 val_accuracy=  0.58203\n",
      "Epoch: 1758 val_loss=  1.66929 val_accuracy=  0.52344\n",
      "Epoch: 1759 val_loss=  1.59740 val_accuracy=  0.51562\n",
      "Epoch: 1760 val_loss=  1.63174 val_accuracy=  0.52734\n",
      "Epoch: 1761 val_loss=  1.44683 val_accuracy=  0.58984\n",
      "Epoch: 1762 val_loss=  1.66873 val_accuracy=  0.53125\n",
      "Epoch: 1763 val_loss=  1.65361 val_accuracy=  0.51953\n",
      "Epoch: 1764 val_loss=  1.59943 val_accuracy=  0.57422\n",
      "Epoch: 1765 val_loss=  1.74143 val_accuracy=  0.53125\n",
      "Epoch: 1766 val_loss=  1.60094 val_accuracy=  0.52734\n",
      "Epoch: 1767 val_loss=  1.72636 val_accuracy=  0.51562\n",
      "Epoch: 1768 val_loss=  1.39029 val_accuracy=  0.57031\n",
      "Epoch: 1769 val_loss=  1.52448 val_accuracy=  0.55859\n",
      "Epoch: 1770 val_loss=  1.39130 val_accuracy=  0.58594\n",
      "Epoch: 1771 val_loss=  1.67628 val_accuracy=  0.51953\n",
      "Epoch: 1772 val_loss=  1.63786 val_accuracy=  0.50000\n",
      "Epoch: 1773 val_loss=  1.73079 val_accuracy=  0.51953\n",
      "Epoch: 1774 val_loss=  1.79643 val_accuracy=  0.50781\n",
      "Epoch: 1775 val_loss=  1.36570 val_accuracy=  0.61328\n",
      "Epoch: 1776 val_loss=  1.74729 val_accuracy=  0.51172\n",
      "Epoch: 1777 val_loss=  1.54272 val_accuracy=  0.58594\n",
      "Epoch: 1778 val_loss=  1.42676 val_accuracy=  0.58984\n",
      "Epoch: 1779 val_loss=  1.51277 val_accuracy=  0.56250\n",
      "Epoch: 1780 val_loss=  1.71488 val_accuracy=  0.53125\n",
      "Epoch: 1781 val_loss=  1.69672 val_accuracy=  0.55859\n",
      "Epoch: 1782 val_loss=  1.63970 val_accuracy=  0.54688\n",
      "Epoch: 1783 val_loss=  1.55495 val_accuracy=  0.57812\n",
      "Epoch: 1784 val_loss=  1.59091 val_accuracy=  0.56641\n",
      "Epoch: 1785 val_loss=  1.37201 val_accuracy=  0.61719\n",
      "Epoch: 1786 val_loss=  1.70525 val_accuracy=  0.51172\n",
      "Epoch: 1787 val_loss=  1.52716 val_accuracy=  0.56250\n",
      "Epoch: 1788 val_loss=  1.71087 val_accuracy=  0.50781\n",
      "Epoch: 1789 val_loss=  1.51046 val_accuracy=  0.55469\n",
      "Epoch: 1790 val_loss=  1.77484 val_accuracy=  0.50000\n",
      "Epoch: 1791 val_loss=  1.72440 val_accuracy=  0.49609\n",
      "Epoch: 1792 val_loss=  1.73414 val_accuracy=  0.53125\n",
      "Epoch: 1793 val_loss=  1.62098 val_accuracy=  0.55078\n",
      "Epoch: 1794 val_loss=  1.53330 val_accuracy=  0.57422\n",
      "Epoch: 1795 val_loss=  1.42742 val_accuracy=  0.55859\n",
      "Epoch: 1796 val_loss=  1.48053 val_accuracy=  0.61719\n",
      "Epoch: 1797 val_loss=  1.58352 val_accuracy=  0.54688\n",
      "Epoch: 1798 val_loss=  1.52316 val_accuracy=  0.55078\n",
      "Epoch: 1799 val_loss=  1.71787 val_accuracy=  0.53125\n",
      "Epoch: 1800 val_loss=  1.41451 val_accuracy=  0.57422\n",
      "Epoch: 1801 val_loss=  1.66065 val_accuracy=  0.51172\n",
      "Epoch: 1802 val_loss=  1.43142 val_accuracy=  0.58984\n",
      "Epoch: 1803 val_loss=  1.62083 val_accuracy=  0.53906\n",
      "Epoch: 1804 val_loss=  1.56185 val_accuracy=  0.51953\n",
      "Epoch: 1805 val_loss=  1.62202 val_accuracy=  0.54688\n",
      "Epoch: 1806 val_loss=  1.67446 val_accuracy=  0.48438\n",
      "Epoch: 1807 val_loss=  1.65559 val_accuracy=  0.54297\n",
      "Epoch: 1808 val_loss=  1.66962 val_accuracy=  0.48828\n",
      "Epoch: 1809 val_loss=  1.68086 val_accuracy=  0.50000\n",
      "Epoch: 1810 val_loss=  1.36487 val_accuracy=  0.57031\n",
      "Epoch: 1811 val_loss=  1.52190 val_accuracy=  0.53906\n",
      "Epoch: 1812 val_loss=  1.71596 val_accuracy=  0.49219\n",
      "Epoch: 1813 val_loss=  1.38117 val_accuracy=  0.58594\n",
      "Epoch: 1814 val_loss=  1.68875 val_accuracy=  0.57422\n",
      "Epoch: 1815 val_loss=  1.66984 val_accuracy=  0.53125\n",
      "Epoch: 1816 val_loss=  1.47663 val_accuracy=  0.59766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1817 val_loss=  1.72340 val_accuracy=  0.47656\n",
      "Epoch: 1818 val_loss=  1.61443 val_accuracy=  0.53516\n",
      "Epoch: 1819 val_loss=  1.58446 val_accuracy=  0.56250\n",
      "Epoch: 1820 val_loss=  1.42358 val_accuracy=  0.58203\n",
      "Epoch: 1821 val_loss=  1.76126 val_accuracy=  0.51953\n",
      "Epoch: 1822 val_loss=  1.60158 val_accuracy=  0.55078\n",
      "Epoch: 1823 val_loss=  1.66771 val_accuracy=  0.52734\n",
      "Epoch: 1824 val_loss=  1.77925 val_accuracy=  0.51172\n",
      "Epoch: 1825 val_loss=  1.52286 val_accuracy=  0.54297\n",
      "Epoch: 1826 val_loss=  1.61433 val_accuracy=  0.51172\n",
      "Epoch: 1827 val_loss=  1.40860 val_accuracy=  0.59375\n",
      "Epoch: 1828 val_loss=  1.50211 val_accuracy=  0.57031\n",
      "Epoch: 1829 val_loss=  1.42050 val_accuracy=  0.56641\n",
      "Epoch: 1830 val_loss=  1.69744 val_accuracy=  0.50000\n",
      "Epoch: 1831 val_loss=  1.70861 val_accuracy=  0.50000\n",
      "Epoch: 1832 val_loss=  1.83550 val_accuracy=  0.48438\n",
      "Epoch: 1833 val_loss=  1.60385 val_accuracy=  0.57031\n",
      "Epoch: 1834 val_loss=  1.42770 val_accuracy=  0.59766\n",
      "Epoch: 1835 val_loss=  1.67785 val_accuracy=  0.52344\n",
      "Epoch: 1836 val_loss=  1.50682 val_accuracy=  0.55859\n",
      "Epoch: 1837 val_loss=  1.57574 val_accuracy=  0.56250\n",
      "Epoch: 1838 val_loss=  1.50555 val_accuracy=  0.56641\n",
      "Epoch: 1839 val_loss=  1.69220 val_accuracy=  0.51953\n",
      "Epoch: 1840 val_loss=  1.69329 val_accuracy=  0.56641\n",
      "Epoch: 1841 val_loss=  1.54867 val_accuracy=  0.54688\n",
      "Epoch: 1842 val_loss=  1.59971 val_accuracy=  0.55859\n",
      "Epoch: 1843 val_loss=  1.53154 val_accuracy=  0.60547\n",
      "Epoch: 1844 val_loss=  1.51927 val_accuracy=  0.57031\n",
      "Epoch: 1845 val_loss=  1.58491 val_accuracy=  0.52734\n",
      "Epoch: 1846 val_loss=  1.64259 val_accuracy=  0.52734\n",
      "Epoch: 1847 val_loss=  1.66474 val_accuracy=  0.53125\n",
      "Epoch: 1848 val_loss=  1.68971 val_accuracy=  0.53125\n",
      "Epoch: 1849 val_loss=  1.65797 val_accuracy=  0.51953\n",
      "Epoch: 1850 val_loss=  1.73249 val_accuracy=  0.51562\n",
      "Epoch: 1851 val_loss=  1.68375 val_accuracy=  0.53125\n",
      "Epoch: 1852 val_loss=  1.66263 val_accuracy=  0.51562\n",
      "Epoch: 1853 val_loss=  1.42641 val_accuracy=  0.57812\n",
      "Epoch: 1854 val_loss=  1.43082 val_accuracy=  0.57422\n",
      "Epoch: 1855 val_loss=  1.59461 val_accuracy=  0.57031\n",
      "Epoch: 1856 val_loss=  1.59887 val_accuracy=  0.55078\n",
      "Epoch: 1857 val_loss=  1.59371 val_accuracy=  0.55469\n",
      "Epoch: 1858 val_loss=  1.46130 val_accuracy=  0.58984\n",
      "Epoch: 1859 val_loss=  1.63611 val_accuracy=  0.50391\n",
      "Epoch: 1860 val_loss=  1.43972 val_accuracy=  0.59375\n",
      "Epoch: 1861 val_loss=  1.53462 val_accuracy=  0.53906\n",
      "Epoch: 1862 val_loss=  1.66441 val_accuracy=  0.52344\n",
      "Epoch: 1863 val_loss=  1.53055 val_accuracy=  0.58203\n",
      "Epoch: 1864 val_loss=  1.62642 val_accuracy=  0.50000\n",
      "Epoch: 1865 val_loss=  1.68710 val_accuracy=  0.50391\n",
      "Epoch: 1866 val_loss=  1.65220 val_accuracy=  0.57031\n",
      "Epoch: 1867 val_loss=  1.59705 val_accuracy=  0.51172\n",
      "Epoch: 1868 val_loss=  1.67775 val_accuracy=  0.50000\n",
      "Epoch: 1869 val_loss=  1.37126 val_accuracy=  0.56641\n",
      "Epoch: 1870 val_loss=  1.60427 val_accuracy=  0.50781\n",
      "Epoch: 1871 val_loss=  1.60992 val_accuracy=  0.51953\n",
      "Epoch: 1872 val_loss=  1.48932 val_accuracy=  0.60547\n",
      "Epoch: 1873 val_loss=  1.62717 val_accuracy=  0.54688\n",
      "Epoch: 1874 val_loss=  1.60301 val_accuracy=  0.55859\n",
      "Epoch: 1875 val_loss=  1.61201 val_accuracy=  0.54297\n",
      "Optimization Finished!\n",
      "total validation loss:  1.5939196763356527\n",
      "total validation accuracy:  0.54379375\n"
     ]
    }
   ],
   "source": [
    "############## Validation and test ###############################\n",
    "with tf.device('/device:GPU:0'):\n",
    "    placeholders_val= {'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "                       'is_training': tf.placeholder_with_default(True, shape=())}\n",
    "    feed_dict= construct_feed_dict(param_dict['dropout'], param_dict['is_training'], placeholders_val)\n",
    "\n",
    "    # Create model\n",
    "    model = GCN(holder, param_dict, input_dim=tf.shape(features_new)[2], placeholders_val= placeholders_val, logging=True)\n",
    "    model.output_avg()\n",
    "    save_freq= param_dict[\"save_freq\"]\n",
    "    valid_freq= 391\n",
    "\n",
    "    \n",
    "\n",
    "sess= tf.Session(config=tf.ConfigProto(allow_soft_placement= True))    \n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "print(\"data feat_batch, label_batch, meta_batch: \", file= open('printout','a'))\n",
    "\n",
    "# initialize the queue threads to start to shovel data\n",
    "# initialize the variable\n",
    "\n",
    "ALL_EVAL_ACC= []\n",
    "ALL_EVAL_LOSS= []\n",
    "\n",
    "\n",
    "sess.run(tf.initializers.global_variables())\n",
    "model.load(sess, save_path='./model-save/tmp/')\n",
    "\n",
    "if identifier == 'test_unseen':\n",
    "    dev_steps= 5 #int(np.ceil(320/256)) # LCM(256, 320)= 1280\n",
    "elif identifier == 'test':\n",
    "    dev_steps= 1875 #int(np.ceil(15000/256)) # LCM(256, 15000)= 480,000\n",
    "elif identifier == 'eval':\n",
    "    dev_steps= 625 #int(np.ceil(5000/256)) # LCM(256,5000)= 160,000\n",
    "    \n",
    "for step in range(dev_steps):\n",
    "    # Train model\n",
    "    t = time.time()\n",
    "    \n",
    "\n",
    "    feed_dict.update({placeholders_val['dropout']: param_dict_eval['dropout']})\n",
    "    feed_dict.update({placeholders_val['is_training']: param_dict_eval['is_training']})\n",
    "    \n",
    "    outs_val = sess.run([model.loss_eval, model.accuracy_eval, \n",
    "                         model.inputs_eval, model.outputs_eval], feed_dict= feed_dict)\n",
    "    ALL_EVAL_LOSS.append(outs_val[0])\n",
    "    ALL_EVAL_ACC.append(outs_val[1])\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (step + 1),\"val_loss= \", \"{:.5f}\".format(outs_val[0]),\n",
    "          \"val_accuracy= \", \"{:.5f}\".format(outs_val[1]))\n",
    "    \n",
    "print(\"Optimization Finished!\")\n",
    "print('total validation loss: ', sum(ALL_EVAL_LOSS)/len(ALL_EVAL_LOSS))\n",
    "print('total validation accuracy: ', sum(ALL_EVAL_ACC)/len(ALL_EVAL_ACC))\n",
    "\n",
    "\n",
    "# We request our child threads to stop ...\n",
    "coord.request_stop()\n",
    "# ... and we wait for them to do so before releasing the main thread\n",
    "coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy over the whole validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Validation and test ###############################\n",
    "# with tf.device('/device:GPU:0'):\n",
    "#     placeholders_val= {'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "#                        'is_training': tf.placeholder_with_default(True, shape=())}\n",
    "#     feed_dict= construct_feed_dict(param_dict['dropout'], param_dict['is_training'], placeholders_val)\n",
    "\n",
    "#     # Create model\n",
    "#     model = GCN(holder, param_dict, input_dim=tf.shape(features_new)[2], placeholders_val= placeholders_val, logging=True)\n",
    "#     model.output_avg()\n",
    "#     save_freq= param_dict[\"save_freq\"]\n",
    "#     valid_freq= 391\n",
    "\n",
    "    \n",
    "\n",
    "# sess= tf.Session(config=tf.ConfigProto(allow_soft_placement= True))    \n",
    "# coord = tf.train.Coordinator()\n",
    "# threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "# print(\"data feat_batch, label_batch, meta_batch: \", file= open('printout','a'))\n",
    "\n",
    "# # initialize the queue threads to start to shovel data\n",
    "# # initialize the variable\n",
    "\n",
    "# ALL_EVAL_ACC= []\n",
    "# ALL_EVAL_LOSS= []\n",
    "\n",
    "\n",
    "# sess.run(tf.initializers.global_variables())\n",
    "# model.load(sess, save_path='./model-save/tmp/')\n",
    "\n",
    "# dev_steps= int(np.ceil(500/256))\n",
    "# for step in range(dev_steps):\n",
    "#     # Train model\n",
    "#     t = time.time()\n",
    "    \n",
    "\n",
    "#     feed_dict.update({placeholders_val['dropout']: param_dict_eval['dropout']})\n",
    "#     feed_dict.update({placeholders_val['is_training']: param_dict_eval['is_training']})\n",
    "    \n",
    "#     outs_val = sess.run([model.loss_eval, model.accuracy_eval, \n",
    "#                          model.inputs_eval, model.outputs_eval, model.inter_out_eval], feed_dict= feed_dict)\n",
    "#     ALL_EVAL_LOSS.append(outs_val[0])\n",
    "#     ALL_EVAL_ACC.append(outs_val[1])\n",
    "    \n",
    "#     # Print results\n",
    "#     print(\"Epoch:\", '%04d' % (step + 1),\"val_loss= \", \"{:.5f}\".format(outs_val[0]),\n",
    "#           \"val_accuracy= \", \"{:.5f}\".format(outs_val[1]))\n",
    "    \n",
    "# print(\"Optimization Finished!\")\n",
    "# print('total validation loss: ', sum(ALL_EVAL_LOSS)/len(ALL_EVAL_LOSS))\n",
    "# print('total validation accuracy: ', sum(ALL_EVAL_ACC)/len(ALL_EVAL_ACC))\n",
    "\n",
    "\n",
    "# # We request our child threads to stop ...\n",
    "# coord.request_stop()\n",
    "# # ... and we wait for them to do so before releasing the main thread\n",
    "# coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7",
   "language": "python",
   "name": "python3.6.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
